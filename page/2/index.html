<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>7125messi的博客 </title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="website" />



<link href="https://7125messi.github.io/index.xml" rel="alternate" type="application/rss+xml" title="7125messi的博客" />

<link rel="canonical" href="https://7125messi.github.io/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<section id="main-content" class="container main_content homepage">
  <header class="container header">
    <h1>7125messi的博客
</h1>

    <span>last update: <time datetime="2019-08-02T21:14:54&#43;08:00">2 August at 9:14pm</time>
</span>

  </header>
  
  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%89%E5%A4%A7%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%99%A8/">自然语言处理三大特征抽取器
</a>
</h2>

  <time datetime="2019-07-08">8 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
本文来自新浪微博资深算法工程师张俊林老师的讲解，是非常好的学习材料。
在上一篇介绍Bert的文章“自然语言处理中的预训练技术发展史”里，我曾大言不惭地宣称如下两个个人判断：
第一个是Bert这种两阶段的模式（预训练+Finetuning）必将成为NLP领域研究和工业应用的流行方法；
第二个是从NLP领域的特征抽取器角度来说，Transformer会逐步取代RNN成为最主流的的特征抽取器。
关于特征抽取器方面的判断，上面文章限于篇幅，只是给了一个结论，并未给出具备诱惑力的说明，看过我文章的人都知道我不是一个随便下结论的人（那位正在补充下一句：“你随便起来不是……”的同学请住口，请不要泄露国家机密，你可以继续睡觉，吵到其它同学也没有关系，哈哈），但是为什么当时我会下这个结论呢？本文可以看做是上文的一个外传，会给出比较详实的证据来支撑之前给出的结论。
如果对目前NLP里的三大特征抽取器的未来走向趋势做个宏观判断的话，我的判断是这样的：
 RNN人老珠黄，已经基本完成它的历史使命，将来会逐步退出历史舞台； CNN如果改造得当，将来还是有希望有自己在NLP领域的一席之地，如果改造成功程度超出期望，那么还有一丝可能作为割据一方的军阀，继续生存壮大，当然我认为这个希望不大，可能跟宋小宝打篮球把姚明打哭的概率相当； 而新欢Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。至于将来是否会出现新的特征抽取器，一枪将Tranformer挑落马下，继而取而代之成为新的特征抽取山大王？这种担忧其实是挺有必要的，毕竟李商隐在一千年前就告诫过我们说：“君恩如水向东流，得宠忧移失宠愁。 莫向樽前奏花落，凉风只在殿西头。”  当然这首诗看样子目前送给RNN是比较贴切的，至于未来Transformer是否会失宠？这个问题的答案基本可以是肯定的，无非这个时刻的来临是3年之后，还是1年之后出现而已。当然，我希望如果是在读这篇文章的你，或者是我，在未来的某一天，从街头拉来一位长相普通的淑女，送到韩国整容，一不小心偏离流水线整容工业的美女模板，整出一位天香国色的绝色，来把Transformer打入冷宫，那是最好不过。但是在目前的状态下，即使是打着望远镜，貌似还没有看到有这种资质的候选人出现在我们的视野之内。
我知道如果是一位严谨的研发人员，不应该在目前局势还没那么明朗的时候做出如上看似有些武断的明确结论，所以这种说法可能会引起争议。但是这确实就是我目前的真实想法，至于根据什么得出的上述判断？这种判断是否有依据？依据是否充分？相信你在看完这篇文章可以有个属于自己的结论。
可能谈到这里，有些平常吃亏吃的少所以喜欢挑刺的同学会质疑说：你凭什么说NLP的典型特征抽取器就这三种呢？你置其它知名的特征抽取器比如Recursive NN于何地?嗯，是，很多介绍NLP重要进展的文章里甚至把Recursive NN当做一项NLP里的重大进展，除了它，还有其它的比如Memory Network也享受这种部局级尊贵待遇。但是我一直都不太看好这两个技术，而且不看好很多年了，目前情形更坚定了这个看法。而且我免费奉劝你一句，没必要在这两个技术上浪费时间，至于为什么，因为跟本文主题无关，以后有机会再详细说。
上面是结论，下面，我们正式进入举证阶段。

战场侦查：NLP任务的特点及任务类型 NLP任务的特点和图像有极大的不同，上图展示了一个例子，NLP的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。如果你听到我对你说：“你欠我那一千万不用还了”和“我欠你那一千万不用还了”，你听到后分别是什么心情？两者区别了解一下；另外，句子中的长距离特征对于理解语义也非常关键，例子参考上图标红的单词，特征抽取器能否具备长距离特征捕获能力这一点对于解决NLP任务来说也是很关键的。
上面这几个特点请记清，一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性。这也是为何我在介绍RNN、CNN、Transformer等特征抽取器之前，先说明这些内容的原因。
NLP是个很宽泛的领域，包含了几十个子领域，理论上只要跟语言处理相关，都可以纳入这个范围。但是如果我们对大量NLP任务进行抽象的话，会发现绝大多数NLP任务可以归结为几大类任务。两个看似差异很大的任务，在解决任务的模型角度，可能完全是一样的。
通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。
解决这些不同的任务，从模型角度来讲什么最重要？是特征抽取器的能力。尤其是深度学习流行开来后，这一点更凸显出来。因为深度学习最大的优点是“端到端（end to end）”，当然这里不是指的从客户端到云端，意思是以前研发人员得考虑设计抽取哪些特征，而端到端时代后，这些你完全不用管，把原始输入扔给好的特征抽取器，它自己会把有用的特征抽取出来。
身为资深Bug制造者和算法工程师，你现在需要做的事情就是：选择一个好的特征抽取器，选择一个好的特征抽取器，选择一个好的特征抽取器，喂给它大量的训练数据，设定好优化目标（loss function），告诉它你想让它干嘛……..然后你觉得你啥也不用干等结果就行了是吧？那你是我见过的整个宇宙中最乐观的人…….你大量时间其实是用在调参上…….。从这个过程可以看出，如果我们有个强大的特征抽取器，那么中初级算法工程师沦为调参侠也就是个必然了，在AutoML（自动那啥）流行的年代，也许以后你想当调参侠而不得，李斯说的“吾欲与若复牵黄犬，俱出上蔡东门逐狡兔，岂可得乎！”请了解一下。所以请珍惜你半夜两点还在调整超参的日子吧，因为对于你来说有一个好消息一个坏消息，好消息是：对于你来说可能这样辛苦的日子不多了！坏消息是：对于你来说可能这样辛苦的日子不多了！！！那么怎么才能成为算法高手？你去设计一个更强大的特征抽取器呀。
下面开始分叙三大特征抽取器。

沙场老将RNN：廉颇老矣，尚能饭否 RNN模型我估计大家都熟悉，就不详细介绍了，模型结构参考上图，核心是每个输入对应隐层节点，而隐层节点之间形成了线性序列，信息由前向后在隐层之间逐步向后传递。我们下面直接进入我想讲的内容。
为何RNN能够成为解决NLP问题的主流特征抽取器
我们知道，RNN自从引入NLP界后，很快就成为吸引眼球的明星模型，在NLP各种任务中被广泛使用。但是原始的RNN也存在问题，它采取线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题。为了解决这个问题，后来引入了LSTM和GRU模型，通过增加中间状态信息直接向后传播，以此缓解梯度消失问题，获得了很好的效果，于是很快LSTM和GRU成为RNN的标准模型。其实图像领域最早由HighwayNet/Resnet等导致模型革命的skip connection的原始思路就是从LSTM的隐层传递机制借鉴来的。经过不断优化，后来NLP又从图像领域借鉴并引入了attention机制（从这两个过程可以看到不同领域的相互技术借鉴与促进作用），叠加网络把层深作深，以及引入Encoder-Decoder框架，这些技术进展极大拓展了RNN的能力以及应用效果。下图展示的模型就是非常典型的使用RNN来解决NLP任务的通用框架技术大礼包，在更新的技术出现前，你可以在NLP各种领域见到这个技术大礼包的身影。
上述内容简单介绍了RNN在NLP领域的大致技术演进过程。那么为什么RNN能够这么快在NLP流行并且占据了主导地位呢？主要原因还是因为RNN的结构天然适配解决NLP的问题，NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。所以RNN特别适合NLP这种线形序列应用场景，这是RNN为何在NLP界如此流行的根本原因。
RNN在新时代面临的两个严重问题
RNN在NLP界一直红了很多年（2014-2018？），在2018年之前，大部分各个子领域的State of Art的结果都是RNN获得的。但是最近一年来，眼看着RNN的领袖群伦的地位正在被动摇，所谓各领风骚3-5年，看来网红模型也不例外。
那这又是因为什么呢？主要有两个原因。
第一个原因在于一些后起之秀新模型的崛起，比如经过特殊改造的CNN模型，以及最近特别流行的Transformer，这些后起之秀尤其是Transformer的应用效果相比RNN来说，目前看具有明显的优势。这是个主要原因，老人如果干不过新人，又没有脱胎换骨自我革命的能力，自然要自觉或不自愿地退出历史舞台，这是自然规律。至于RNN能力偏弱的具体证据，本文后面会专门谈，这里不展开讲。当然，技术人员里的RNN保皇派们，这个群体规模应该还是相当大的，他们不会轻易放弃曾经这么热门过的流量明星的，所以也想了或者正在想一些改进方法，试图给RNN延年益寿。至于这些方法是什么，有没有作用，后面也陆续会谈。
另外一个严重阻碍RNN将来继续走红的问题是：RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力，这个乍一看好像不是太大的问题，其实问题很严重。如果你仅仅满足于通过改RNN发一篇论文，那么这确实不是大问题，但是如果工业界进行技术选型的时候，在有快得多的模型可用的前提下，是不太可能选择那么慢的模型的。一个没有实际落地应用支撑其存在价值的模型，其前景如何这个问题，估计用小脑思考也能得出答案。
那问题来了：为什么RNN并行计算能力比较差？是什么原因造成的？
我们知道，RNN之所以是RNN，能将其和其它模型区分开的最典型标志是：T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词Xt，这个不算特点，所有模型都要接收这个原始输入；关键的是另外一个输入，T时刻的隐层状态St还依赖T-1时刻的隐层状态S(t-1)的输出，这是最能体现RNN本质特征的一点，RNN的历史信息是通过这个信息传输渠道往后传输的，示意参考上图。那么为什么RNN的并行计算能力不行呢？问题就出在这里。因为T时刻的计算依赖T-1时刻的隐层计算结果，而T-1时刻的计算依赖T-2时刻的隐层计算结果……..这样就形成了所谓的序列依赖关系。就是说只能先把第1时间步的算完，才能算第2时间步的结果，这就造成了RNN在这个角度上是无法并行计算的，只能老老实实地按着时间步一个单词一个单词往后走。
而CNN和Transformer就不存在这种序列依赖问题，所以对于这两者来说并行计算能力就不是问题，每个时间步的操作可以并行一起计算。
那么能否针对性地对RNN改造一下，提升它的并行计算能力呢？如果可以的话，效果如何呢？下面我们讨论一下这个问题。
如何改造RNN使其具备并行计算能力？
上面说过，RNN不能并行计算的症结所在，在于T时刻对T-1时刻计算结果的依赖，而这体现在隐层之间的全连接网络上。既然症结在这里，那么要想解决问题，也得在这个环节下手才行。在这个环节多做点什么事情能够增加RNN的并行计算能力呢？你可以想一想。
其实留给你的选项并不多，你可以有两个大的思路来改进：一种是仍然保留任意连续时间步（T-1到T时刻）之间的隐层连接；而另外一种是部分地打断连续时间步（T-1到T时刻）之间的隐层连接 。
我们先来看第一种方法，现在我们的问题转化成了：我们仍然要保留任意连续时间步（T-1到T时刻）之间的隐层连接，但是在这个前提下，我们还要能够做到并行计算，这怎么处理呢？因为只要保留连续两个时间步的隐层连接，则意味着要计算T时刻的隐层结果，就需要T-1时刻隐层结果先算完，这不又落入了序列依赖的陷阱里了吗？嗯，确实是这样，但是为什么一定要在不同时间步的输入之间并行呢？没有人说RNN的并行计算一定发生在不同时间步上啊，你想想，隐层是不是也是包含很多神经元？那么在隐层神经元之间并行计算行吗？如果你要是还没理解这是什么意思，那请看下图。
上面的图只显示了各个时间步的隐层节点，每个时间步的隐层包含3个神经元，这是个俯视图，是从上往下看RNN的隐层节点的。另外，连续两个时间步的隐层神经元之间仍然有连接，上图没有画出来是为了看着简洁一些。这下应该明白了吧，假设隐层神经元有3个，那么我们可以形成3路并行计算（红色箭头分隔开成了三路），而每一路因为仍然存在序列依赖问题，所以每一路内仍然是串行的。大思路应该明白了是吧？但是了解RNN结构的同学会发现这样还遗留一个问题：隐层神经元之间的连接是全连接，就是说T时刻某个隐层神经元与T-1时刻所有隐层神经元都有连接，如果是这样，是无法做到在神经元之间并行计算的，你可以想想为什么，这个简单，我假设你有能力想明白。那么怎么办呢？很简单，T时刻和T-1时刻的隐层神经元之间的连接关系需要改造，从之前的全连接，改造成对应位置的神经元（就是上图被红箭头分隔到同一行的神经元之间）有连接，和其它神经元没有连接。这样就可以解决这个问题，在不同路的隐层神经元之间可以并行计算了。
第一种改造RNN并行计算能力的方法思路大致如上所述，这种方法的代表就是论文“Simple Recurrent Units for Highly Parallelizable Recurrence”中提出的SRU方法，它最本质的改进是把隐层之间的神经元依赖由全连接改成了哈达马乘积，这样T时刻隐层单元本来对T-1时刻所有隐层单元的依赖，改成了只是对T-1时刻对应单元的依赖，于是可以在隐层单元之间进行并行计算，但是收集信息仍然是按照时间序列来进行的。所以其并行性是在隐层单元之间发生的，而不是在不同时间步之间发生的。
这其实是比较巧妙的一种方法，但是它的问题在于其并行程度上限是有限的，并行程度取决于隐层神经元个数，而一般这个数值往往不会太大，再增加并行性已经不太可能。另外每一路并行线路仍然需要序列计算，这也会拖慢整体速度。SRU的测试速度为：在文本分类上和原始CNN（Kim 2014）的速度相当，论文没有说CNN是否采取了并行训练方法。 其它在复杂任务阅读理解及MT任务上只做了效果评估，没有和CNN进行速度比较，我估计这是有原因的，因为复杂任务往往需要深层网络，其它的就不妄作猜测了。
第二种改进典型的思路是：为了能够在不同时间步输入之间进行并行计算，那么只有一种做法，那就是打断隐层之间的连接，但是又不能全打断，因为这样基本就无法捕获组合特征了，所以唯一能选的策略就是部分打断，比如每隔2个时间步打断一次，但是距离稍微远点的特征如何捕获呢？只能加深层深，通过层深来建立远距离特征之间的联系。代表性模型比如上图展示的Sliced RNN。我当初看到这个模型的时候，心里忍不住发出杠铃般的笑声，情不自禁地走上前跟他打了个招呼：你好呀，CNN模型，想不到你这个糙汉子有一天也会穿上粉色裙装，装扮成RNN的样子出现在我面前啊，哈哈。了解CNN模型的同学看到我上面这句话估计会莞尔会心一笑：这不就是简化版本的CNN吗？不了解CNN的同学建议看完后面CNN部分再回头来看看是不是这个意思。
那经过这种改造的RNN速度改进如何呢？论文给出了速度对比实验，归纳起来，SRNN速度比GRU模型快5到15倍，嗯，效果不错，但是跟对比模型DC-CNN模型速度比较起来，比CNN模型仍然平均慢了大约3倍。这很正常但是又有点说不太过去，说正常是因为本来这就是把RNN改头换面成类似CNN的结构，而片段里仍然采取RNN序列模型，所以必然会拉慢速度，比CNN慢再正常不过了。说“说不过去”是指的是：既然本质上是CNN，速度又比CNN慢，那么这么改的意义在哪里？为什么不直接用CNN呢？是不是？前面那位因为吃亏吃的少所以爱抬杠的同学又会说了：也许人家效果特别好呢。嗯，从这个结构的作用机制上看，可能性不太大。你说论文实验部分证明了这一点呀，我认为实验部分对比试验做的不充分，需要补充除了DC-CNN外的其他CNN模型进行对比。当然这点纯属个人意见，别当真，因为我讲起话来的时候经常摇头晃脑，此时一般会有人惊奇地跟我反馈说：为什么你一讲话我就听到了水声？
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%89%E5%A4%A7%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%99%A8/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/feature_selector%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">Feature_selector实现高效的特征选择
</a>
</h2>

  <time datetime="2019-07-03">3 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
具体可以参考我的Azure Notebook：
https://notebooks.azure.com/messi7125/projects/feature-engineering
下面简要介绍一下功能介绍
Introduction: Feature Selector Usage 在这个笔记本中，我们将使用FeatureSelector类来选择要从数据集中删除的要素。 此类有五种方法可用于查找要删除的功能：
 具有高missing-values百分比的特征 具有高相关性的特征 对模型预测结果无贡献的特征（即zero importance） 对模型预测结果只有很小贡献的特征（即low importance） 具有单个值的特征（即数据集中该特征取值的集合只有一个元素）  requirements：
lightgbm==2.1.1 matplotlib==2.1.2 seaborn==0.8.1 numpy==1.14.5 pandas==0.23.1 scikit-learn==0.19.1  from feature_selector.selector import FeatureSelector import pandas as pd  Example Dataset 该数据集被用作[Kaggle的[Home Credit Default Risk Competition]竞赛的一部分（https://www.kaggle.com/c/home-credit-default-risk/）。 它适用于受监督的机器学习分类任务，其目标是预测客户是否违约贷款。 整个数据集可以[这里]下载，我们将使用10,000行的小样本。
特征选择器设计用于机器学习任务，但可以应用于任何数据集。基于特征重要性的方法确实需要受监督的机器学习问题。
train = pd.read_csv('../data/credit_example.csv') train_labels = train['TARGET'] train.head()  数据集中有几个分类列。 使用基于特征重要性的方法时，FeatureSelector使用独热编码处理这些。
train = train.drop(columns = ['TARGET']) train.head()  Implementation FeatureSelector有五个函数用于识别要删除的列：
 identify_missing identify_single_unique identify_collinear identify_zero_importance identify_low_importance  这些方法根据指定的标准查找要删除的功能。 标识的特征存储在FeatureSelector的ops属性（Python字典）中。 我们可以手动删除已识别的功能，或使用FeatureSelector中的remove功能来实际删除功能。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/feature_selector%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">UER Py高质量中文BERT预训练模型
</a>
</h2>

  <time datetime="2019-06-30">30 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
最近在github看到了一个项目:
https://github.com/dbiir/UER-py
貌似是腾讯出品，良心之作，特地去Colab撸了一下，确实可以，封装了很多模块，后续估计还要迭代，直至打成包就更好了。
有兴趣的同学可以去我的Colab上，玩一下。
https://colab.research.google.com/drive/1N81AYxPolDWPMdbZpYO1NnfV4T403Bxz
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/nlp%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%84%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">NLP深度学习的各类模型综述
</a>
</h2>

  <time datetime="2019-06-30">30 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
参考来源：https://www.jiqizhixin.com/articles/2019-06-21
导读：NLP，让人与机器的交互不再遥远；深度学习，让语言解析不再是智能系统的瓶颈。本文尝试回顾NLP发展的历史，解读NLP升级迭代过程中一些重要而有意思的模型，总结不同任务场景应用模型的思路，对NLP未来发展趋势发表自己的看法。笔者经验有限，如有不全或不当地方，请予指正。

1 背景 自然语言处理（英语：Natural Language Process，简称NLP）是计算机科学、信息工程以及人工智能的子领域，专注于人机语言交互，探讨如何处理和运用自然语言。自然语言处理的研究，最早可以说开始于图灵测试，经历了以规则为基础的研究方法，流行于现在基于统计学的模型和方法，从早期的传统机器学习方法，基于高维稀疏特征的训练方式，到现在主流的深度学习方法，使用基于神经网络的低维稠密向量特征训练模型。最近几年，随着深度学习以及相关技术的发展，NLP领域的研究取得一个又一个突破，研究者设计各种模型和方法，来解决NLP的各类问题。下图是Young等[1]统计了过去6年ACL、EMNLP、EACL和NAACL上发表深度学习长篇论文的比例逐年增加，而2018年下半场基本是ELMo、GPT、BERT等深度学习模型光芒四射的showtime，所以本文会将更多的笔墨用于陈述分析深度学习模型。
机器学习是计算机通过模式和推理、而不是明确指令的方式，高效执行指定任务的学习算法。贝叶斯概率模型、逻辑回归、决策树、SVM、主题模型、HMM模型等，都是常见的用于NLP研究的传统机器学习算法。而深度学习是一种基于特征学习的机器学习方法，把原始数据通过简单但非线性的模块转变成更高层次、更加抽象的特征表示，通过足够多的转换组合，非常复杂的函数也能被学习。在多年的实验中，人们发现了认知的两个重要机制：抽象和迭代，从原始信号，做底层抽象，逐渐向高层抽象迭代，在迭代中抽象出更高层的模式。如何形象地理解？在机器视觉领域会比较容易理解，深度学习通过多层神经网络依次提取出图像信息的边缘特征、简单形状特征譬如嘴巴的轮廓、更高层的形状特征譬如脸型；而在自然语言处理领域则没有那么直观的理解，我们可以通过深度学习模型学习到文本信息的语法特征和语义特征。可以说，深度学习，代表自然语言处理研究从机器学习到认知计算的进步。
要讲深度学习，得从语言模型开始讲起。自然语言处理的基础研究便是人机语言交互，以机器能够理解的算法来反映人类的语言，核心是基于统计学的语言模型。语言模型（英语：Language Model，简称LM），是一串词序列的概率分布。通过语言模型，可以量化地评估一串文字存在的可能性。对于一段长度为n的文本，文本中的每个单词都有通过上文预测该单词的过程，所有单词的概率乘积便可以用来评估文本存在的可能性。在实践中，如果文本很长，P(w_i|context(w_i))的估算会很困难，因此有了简化版：N元模型。在N元模型中，通过对当前词的前N个词进行计算来估算该词的条件概率。对于N元模型。常用的有unigram、bigram和trigram，N越大，越容易出现数据稀疏问题，估算结果越不准。为了解决N元模型估算概率时的数据稀疏问题，研究者尝试用神经网络来研究语言模型。
早在2000年，就有研究者提出用神经网络研究语言模型的想法，经典代表有2003年Bengio等[2]提出的NNLM，但效果并不显著，深度学习用于NLP的研究一直处在探索的阶段。直到2011年，Collobert等[3]用一个简单的深度学习模型在命名实体识别NER、语义角色标注SRL、词性标注POS-tagging等NLP任务取得SOTA成绩，基于深度学习的研究方法得到越来越多关注。2013年，以Word2vec、Glove为代表的词向量大火，更多的研究从词向量的角度探索如何提高语言模型的能力，研究关注词内语义和上下文语义。此外，基于深度学习的研究经历了CNN、RNN、Transormer等特征提取器，研究者尝试用各种机制优化语言模型的能力，包括预训练结合下游任务微调的方法。最近最吸睛的EMLo、GPT和BERT模型，便是这种预训练方法的优秀代表，频频刷新SOTA。

2 模型 接下来文章会以循序渐进的方式分成五个部分来介绍深度学习模型：第一部分介绍一些基础模型；第二部分介绍基于CNN的模型；第三部分介绍基于RNN的模型；第四部分介绍基于Attention机制的模型；第五部分介绍基于Transformer的模型，讲述一段语言模型升级迭代的故事。NLP有很多模型和方法，不同的任务场景有不同的模型和策略来解决某些问题。笔者认为，理解了这些模型，再去理解别的模型便不是很难的事情，甚至可以自己尝试设计模型来满足具体任务场景的需求。
Basic Embedding Model
NNLM是非常经典的神经网络语言模型，虽然相比传统机器学习并没有很显著的成效，但大家说起NLP在深度学习方向的探索，总不忘提及这位元老。分布式表示的概念很早就有研究者提出并应用于深度学习模型，Word2vec使用CBOW和Skip-gram训练模型，意外的语意组合效果才使得词向量广泛普及。FastText在语言模型上没有什么特别的突破，但模型的优化使得深度学习模型在大规模数据的训练非常快甚至秒级，而且文本分类的效果匹敌CNN/RNN之类模型，在工业化应用占有一席之地。

NNLM 2003年Bengio等[2]提出一种基于神经网络的语言模型NNLM，模型同时学习词的分布式表示，并基于词的分布式表示，学习词序列的概率函数，这样就可以用词序列的联合概率来表示句子，而且，模型还能够产生指数量级的语义相似的句子。作者认为，通过这种方式训练出来的语言模型具有很强的泛化能力，对于训练语料里没有出现过的句子，模型能够通过见过的相似词组成的相似句子来学习。
模型训练的目标函数是长度为n的词序列的联合概率，分解成两部分：一是特征映射，通过映射矩阵C，将词典V中的每个单词都能映射成一个特征向量C(i) ∈ Rm；二是计算条件概率分布，通过函数g，将输入的词向量序列(C(wt−n+1),··· ,C(wt−1))转化成一个概率分布y ∈ R|V|，g的输出是个向量，第i个元素表示词序列第n个词是Vi的概率。网络输出层采用softmax函数。

Word2vec 对于复杂的自然语言任务进行建模，概率模型应运而生成为首选的方法，但在最开始的时候，学习语言模型的联合概率函数存在致命的维数灾难问题。假如语言模型的词典大小为100000，要表示10个连续词的联合分布，模型参数可能就要有1050个。相应的，模型要具备足够的置信度，需要的样本量指数级增加。为了解决这个问题，最早是1986年Hinton等[5]提出分布式表示（Distributed Representation），基本思想是将词表示成n维连续的实数向量。分布式表示具备强大的特征表示能力，n维向量，每维有k个值，便能表示 kn个特征。
词向量是NLP深度学习研究的基石，本质上遵循这样的假设：语义相似的词趋向于出现在相似的上下文。因此在学习过程中，这些向量会努力捕捉词的邻近特征，从而学习到词汇之间的相似性。有了词向量，便能够通过计算余弦距离等方式来度量词与词之间的相似度。
Mikolov等[4]对词向量的广泛普及功不可没，他们用CBOW和Skip-gram模型训练出来的词向量具有神奇的语意组合特效，词向量的加减计算结果刚好是对应词的语意组合，譬如v(King) - v(Man) + v(Woman) = v(Queen)。这种意外的特效使得Word2vec快速流行起来。至于为什么会有这种行为呢？有意思的是Gittens等[10]做了研究，尝试给出了理论假设：词映射到低维分布式空间必须是均匀分布的，词向量才能有这个语意组合效果。
CBOW和Skip-gram是Word2vec的两种不同训练方式。CBOW指抠掉一个词，通过上下文预测该词；Skip-gram则与CBOW相反，通过一个词预测其上下文。
以CBOW为例。CBOW模型是一个简单的只包含一个隐含层的全连接神经网络，输入层采用one-hot编码方式，词典大小为V；隐含层大小是N；输出层通过softmax函数得到词典里每个词的概率分布。层与层之间分别有两个权重矩阵W ∈ RV ×N和W′ ∈ RH×V。词典里的每个单词都会学习到两个向量vc和vw，分别代表上下文向量和目标词向量。因此，给予上下文c，出现目标词w的概率是：p(w|c)，该模型需要学习的网络参数θ 。
词向量是用概率模型训练出来的产物，对训练语料库出现频次很低甚至不曾出现的词，词向量很难准确地表示。词向量是词级别的特征表示方式，单词内的形态和形状信息同样有用，研究者提出了基于字级别的表征方式，甚至更细粒度的基于Byte级别的表征方式。2017年，Mikolov等[9]提出用字级别的信息丰富词向量信息。此外，在机器翻译相关任务里，基于Byte级别的特征表示方式BPE还被用来学习不同语言之间的共享信息，主要以拉丁语系为主。2019年，Lample等[11]提出基于BERT优化的跨语言模型XLM，用BPE编码方式来提升不同语言之间共享的词汇量。
虽然通过词向量，一个单词能够很容易找到语义相似的单词，但单一词向量，不可避免一词多义问题。对于一词多义问题，最常用有效的方式便是基于预训练模型，结合上下文表示单词，代表模型有EMLo、GPT和BERT，都能有效地解决这个问题。
词，可以通过词向量进行表征，自然会有人思考分布式表示这个概念是否可以延伸到句子、文章、主题等。词袋模型Bag-of-Words忽略词序以及词语义，2014年Mikolov等[8]将分布式向量的表示扩展到句子和文章，模型训练类似word2vec。关于句子的向量表示，2015年Kiros等提出skip-thought，2018年Logeswaran等提出改进版quick-thought。

FastText 2016年，Mikolov等[7]提出一种简单轻量、用于文本分类的深度学习模型，架构跟Mikolov等[4]提出的word2vec的CBOW模型相似，但有所不同：各个词的embedding向量，补充字级别的n-gram特征向量，然后将这些向量求和平均，用基于霍夫曼树的分层softmax函数，输出对应的类别标签。FastText能够做到效果好、速度快，优化的点有两个：一是引入subword n-gram的概念解决词态变化的问题，利用字级别的n-gram信息捕获字符间的顺序关系，依次丰富单词内部更细微的语义；二是用基于霍夫曼树的分层softmax函数，将计算复杂度从O(kh)降低到O(h log2(k))，其中，k是类别个数，h是文本表示的维数。相比char-CNN之类的深度学习模型需要小时或天的训练时间，FastText只需要秒级的训练时间。
CNN-based Model
词向量通过低维分布式空间能够有效地表示词，使其成为NLP深度学习研究的基石。在词向量的基础上，需要有一种有效的特征提取器从词向量序列里提取出更高层次的特征，应用到NLP任务中去，譬如机器翻译、情感分析、问答、摘要，等。鉴于卷积神经网络CNN在机器视觉领域非常出色的特征提取能力，自然而然被研究者尝试用于自然语言处理领域。
回顾CNN应用于NLP领域的过去，好像一段民间皇子误入宫廷的斗争史。CNN用于句子建模最早可以追溯到2008年Collobert等[12]的研究，使用的是查找表映射词的表征方式，这可以看作是一种原始的词向量方法。2011年Collobert等[3]扩展了他的研究工作，提出一种基于CNN的通用框架用于各种NLP任务。Collobert[3]、Kalchbrenner[14]、Kim[13]等基于CNN的研究工作，推进了CNN应用在NLP研究的普及。
CNN擅长捕获局部特征，能够将信息量丰富的潜在语义特征用于下游任务，譬如TextCNN用于文本分类。但CNN却有长距离依赖问题，研究者尝试用动态卷积网络DCNN来改善这个不足。跟RNN的竞争过程中，CNN不停地优化自身能力，现在CNN的研究趋势是：加入GLU/GTU门机制来简化梯度传播，使用Dilated CNN增加覆盖长度，基于一维卷积层叠加深度并用Residual Connections辅助优化，优秀代表：增加网络深度的VDCNN和引进Gate机制的GCNN。

TextCNN 2014年，Kim[13]提出基于预训练Word2vec的TextCNN模型用于句子分类任务。CNN，通过卷积操作做特征检测，得到多个特征映射，然后通过池化操作对特征进行筛选，过滤噪音，提取关键信息，用来分类。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/nlp%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%84%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E6%83%B3%E6%BC%94%E8%BF%9B/">语言表征学习的思想演进
</a>
</h2>

  <time datetime="2019-06-30">30 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
参考文章：https://www.jiqizhixin.com/articles/2019-06-29-3
**在预训练语言模型 BERT 对自然语言处理的冲击还未平息时，CMU 和 Google 的研究员又放出了一个猛料：在 20 多项任务上全线碾压 BERT 的 XLNet。：由于在公众号中插入方式不方便，对于一个符号 &ldquo;a{b}^{c}&ldquo;，&rdquo;{b}&rdquo; 代表下标，&rdquo;{c}&rdquo; 代表上标。

1. 语言表征学习 深度学习的基本单元是向量。我们将建模对象对应到各自的向量 x (或者一组向量 x{1}, x{2}, &hellip;, x{n})，然后通过变换、整合得到新的向量 h，再基于向量 h 得到输出的判断 y。这里的 h 就是我们说的表征 (Representation)，它是一个向量，描述了我们的建模对象。而语言表征学习就是解决怎么样将一个词、一句话、一篇文章通过变换 (Transformation) 和整合 (Aggregation) 转化成对应的向量 h 的问题。
深度学习解决这个问题的办法是人工设计一个带有可调参数的模型，通过指定一系列的 (输入→输出) 对 (x → y)，让模型学习得到最优的参数。当参数确定之后，模型除了可以完成从 x 预测 y 的任务之外，其中间把 x 变换成 h 的方法也是可以用到其他任务的。这也是我们为什么要做表征学习。（就是说我们在做预测任务同时，顺便把表征学习这件事情给做了）
所以我们要解决的问题便是：
 怎么确定 (输入→输出) 对，即模型的预测任务
 这个模型怎么设计
  
2. 分布式语义假设 任何任务都可以用来做表征学习：情感分析 (输入句子，判断句子是正向情感还是负向情感)，机器翻译 (输入中文，输出英文)。但是这些任务的缺点是需要大量的人工标注，这些标注耗时耗力。当标注量不够时，模型很容易学出&rdquo;三长一短选最短&rdquo;的取巧方案 &ndash; 但我们想要的是真正的语言理解。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E6%83%B3%E6%BC%94%E8%BF%9B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/">Transformer技术实战
</a>
</h2>

  <time datetime="2019-06-25">25 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
欢迎访问Colab链接
https://colab.research.google.com/drive/1FH6ZpQA1HGX3GZCm0yxidq7ztoyd6KhB
这里以京东商城评论文本goods_zh.txt,10万条，标注为0的是差评，标注为1的是好评。中文商品评论短文本分类器，可用于情感分析。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/">基于UNet神经网络的城市人流预测
</a>
</h2>

  <time datetime="2019-06-24">24 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
1 利用手机信令数据计算人口流动数据 手机信令数据是研究人口的整体流动情况的重要数据来源。移动运营商在为手机用户提供实时通讯服务时，积累了大量的基站与设备的服务配对数据。根据配对和唤醒发生的时间以及基站的地理位置，可以很自然划定一定时间和空间范围，统计每一个时间范围内在特定空间区域内手机设备的停留，进入和离开数据，并据此估算相应的人流数据。
传统的人口网格分析过程一般只关注单个网格内某个时间点人口流动的截面数据以及城市当中不同区域的人口分布统计情况；没有将时间和空间融合考虑，不能对城市整体的人口流动形成一个完整连续的直观描述。但是，作为城市安全运营管理者和规划人员职责是需要把握好城市人口流动规律，建立有效的时空数据分析模型，从而为城市安全运行管理做好相应的人口短时预测与应急管理服务。 2 人口流动数据网格图像流处理 2.1 流处理思路 原始手机信令数据，按照一定的时间间隔（例如15分钟），划分出每个时间段的信令数据情况。主要包括：时间，格网ID，格网中心点经度，格网中心点维度，时间段内格网的停留人数，进入人数，离开人数。
根据原始数据的时空关系，将原始数据转化为4维向量空间矩阵，维度分别为时间维度、空间维度横坐标，空间维度纵坐标以及停留，进入或离开的类别：Matrix[t,i,j,k]=p意味着在t时刻，第i行第j列的空间栅格位置，k=0时则停留人数为p，k=1时则进入人数为p，k=2时则离开人数为p。
在这样的转换关系下，可以将源数据处理为3通道的时空数据。考虑到单个人员流动的时空连续性，可以认为表示人口流通的整体统计量的时空矩阵也具备一定局部关联性，换而言之，一个栅格点的人口流动数据会与该栅格附近的人口流行数据相互关联，也会与前后时间段该栅格的人口流动数据相互关联。而具体的关联形式和影响强度，则需要我们利用卷积神经，对历史数据进行学习来发现和记录相应的关联关系。
更进一步地，通过数据洞察注意到，不同栅格网络间人口流动的时间变化曲线往往倾向于若干种固定模式，直观上，商业区，住宅区，办公区域会呈现出不同的人流曲线变化模式。这种模式与地理位置，用地规划，交通路网信息等属性息息相关。本模型后续将进一步讨论不同用地类型的栅格人口流动模式的比较分析。
   TIME TAZID STAY ENTER EXIT     2017-04-05 00:00:00 1009897 460 460 52    2.2 人口栅格数据矢量化 基于一定的空间距离间隔（例如250m），将分析的目标空间划分为若干网格(141*137)。统计T时间内，属于网格M_(p,q)的手机设备停留、进入和离开的数据。按照业务需求，将手机设备数扩样为人口数量，将停留、进入和离开的数据标准化到（0,255）的空间，并将标准化后的数据作为图像的3个颜色通道，据此将T时间的整体网格数据转化为一张三通道格式的图片数据。按照时间维度将经过上述处理的图像作为视频的每一帧图像。
import pandas as pd import numpy as np import h5py # 数据转换成张量类型 data_enter_exit_sz = pd.read_csv('data/sz/data/TBL_ENTER_EXIT_SZ20170401-20170431.csv') time_list = data_enter_exit_sz['TIME'].unique() N = len(time_list) string_to_ix = {string:i for i,string in enumerate(time_list)} tensor_data = np.zeros([N,141,137,3]) for _,line in data_enter_exit_sz.
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/">Transformer技术详解
</a>
</h2>

  <time datetime="2019-06-23">23 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
参考文章：http://jalammar.github.io/illustrated-transformer/
谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。
Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。
在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。
Attention is All You Need：https://arxiv.org/abs/1706.03762
谷歌团队近期提出的用于生成词向量的BERT算法在NLP的11项任务中取得了效果的大幅提升，堪称2018年深度学习领域最振奋人心的消息。而BERT算法的最重要的部分便是本文中提出的Transformer的概念。
正如论文的题目所说的，Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。
作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：
 间片 的计算依赖 时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。  Transformer的提出解决了上面两个问题，首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。

1. Transformer 详解 
1.1 宏观视角认识Transformer整体框架 论文中的验证Transformer的实验室基于机器翻译的，下面我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为如图1：可以看到它是由编码组件、解码组件和它们之间的连接组成。
图1：Transformer用于机器翻译
图2：Transformer的Encoder-Decoder结构
编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
图3：Transformer的Encoder和Decoder均由6个block堆叠而成
所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。
图4：Transformer由self-attention和Feed Forward neural network组成
Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">Paddle_hub和pytorch_hub预训练模型迁移学习
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
paddlehub预训练模型迁移学习
https://colab.research.google.com/drive/1PIRRwzCcEVVQ4jAOTm7yJnlduuxM-pLc
pytorch_hub预训练模型迁移学习
https://colab.research.google.com/drive/1oCmWDakpYwLL8AVmAUxr4FLVSHQTTXey
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/">特征工程方法论
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
欢迎浏览我的Azure Notebooks
https://notebooks.azure.com/messi7125/projects/feature-engineering
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/">Read more &rarr;</a>

</div>


</article>

  
  
<div class="container pagination">
  


<a aria-label="First" href="https://7125messi.github.io/">
  <span aria-hidden="true">««</span>
</a>

<a aria-label="Previous" href="https://7125messi.github.io/">
  <span aria-hidden="true">«</span>
</a>


<a href="https://7125messi.github.io/">
  1
</a>

<a class="active" href="https://7125messi.github.io/page/2/">
  2
</a>

<a href="https://7125messi.github.io/page/3/">
  3
</a>


<a aria-label="Next" href="https://7125messi.github.io/page/3/">
  <span aria-hidden="true">»</span>
</a>

<a aria-label="Last" href="https://7125messi.github.io/page/3/">
  <span aria-hidden="true">»»</span>
</a>


</div>


</section>

      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

