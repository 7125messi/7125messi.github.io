<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>7125messi的博客 </title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="website" />



<link href="https://7125messi.github.io/index.xml" rel="alternate" type="application/rss+xml" title="7125messi的博客" />

<link rel="canonical" href="https://7125messi.github.io/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<section id="main-content" class="container main_content homepage">
  <header class="container header">
    <h1>7125messi的博客
</h1>

    <span>last update: <time datetime="2020-08-03T21:31:10&#43;08:00">3 August at 9:31pm</time>
</span>

  </header>
  
  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">UER Py高质量中文BERT预训练模型
</a>
</h2>

  <time datetime="2019-06-30">30 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
最近在github看到了一个项目:
https://github.com/dbiir/UER-py
貌似是腾讯出品，良心之作，特地去Colab撸了一下，确实可以，封装了很多模块，后续估计还要迭代，直至打成包就更好了。
有兴趣的同学可以去我的Colab上，玩一下。
https://colab.research.google.com/drive/1N81AYxPolDWPMdbZpYO1NnfV4T403Bxz
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/nlp%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%84%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">NLP深度学习的各类模型综述
</a>
</h2>

  <time datetime="2019-06-30">30 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
参考来源：https://www.jiqizhixin.com/articles/2019-06-21
导读：NLP，让人与机器的交互不再遥远；深度学习，让语言解析不再是智能系统的瓶颈。本文尝试回顾NLP发展的历史，解读NLP升级迭代过程中一些重要而有意思的模型，总结不同任务场景应用模型的思路，对NLP未来发展趋势发表自己的看法。笔者经验有限，如有不全或不当地方，请予指正。

1 背景 自然语言处理（英语：Natural Language Process，简称NLP）是计算机科学、信息工程以及人工智能的子领域，专注于人机语言交互，探讨如何处理和运用自然语言。自然语言处理的研究，最早可以说开始于图灵测试，经历了以规则为基础的研究方法，流行于现在基于统计学的模型和方法，从早期的传统机器学习方法，基于高维稀疏特征的训练方式，到现在主流的深度学习方法，使用基于神经网络的低维稠密向量特征训练模型。最近几年，随着深度学习以及相关技术的发展，NLP领域的研究取得一个又一个突破，研究者设计各种模型和方法，来解决NLP的各类问题。下图是Young等[1]统计了过去6年ACL、EMNLP、EACL和NAACL上发表深度学习长篇论文的比例逐年增加，而2018年下半场基本是ELMo、GPT、BERT等深度学习模型光芒四射的showtime，所以本文会将更多的笔墨用于陈述分析深度学习模型。
机器学习是计算机通过模式和推理、而不是明确指令的方式，高效执行指定任务的学习算法。贝叶斯概率模型、逻辑回归、决策树、SVM、主题模型、HMM模型等，都是常见的用于NLP研究的传统机器学习算法。而深度学习是一种基于特征学习的机器学习方法，把原始数据通过简单但非线性的模块转变成更高层次、更加抽象的特征表示，通过足够多的转换组合，非常复杂的函数也能被学习。在多年的实验中，人们发现了认知的两个重要机制：抽象和迭代，从原始信号，做底层抽象，逐渐向高层抽象迭代，在迭代中抽象出更高层的模式。如何形象地理解？在机器视觉领域会比较容易理解，深度学习通过多层神经网络依次提取出图像信息的边缘特征、简单形状特征譬如嘴巴的轮廓、更高层的形状特征譬如脸型；而在自然语言处理领域则没有那么直观的理解，我们可以通过深度学习模型学习到文本信息的语法特征和语义特征。可以说，深度学习，代表自然语言处理研究从机器学习到认知计算的进步。
要讲深度学习，得从语言模型开始讲起。自然语言处理的基础研究便是人机语言交互，以机器能够理解的算法来反映人类的语言，核心是基于统计学的语言模型。语言模型（英语：Language Model，简称LM），是一串词序列的概率分布。通过语言模型，可以量化地评估一串文字存在的可能性。对于一段长度为n的文本，文本中的每个单词都有通过上文预测该单词的过程，所有单词的概率乘积便可以用来评估文本存在的可能性。在实践中，如果文本很长，P(w_i|context(w_i))的估算会很困难，因此有了简化版：N元模型。在N元模型中，通过对当前词的前N个词进行计算来估算该词的条件概率。对于N元模型。常用的有unigram、bigram和trigram，N越大，越容易出现数据稀疏问题，估算结果越不准。为了解决N元模型估算概率时的数据稀疏问题，研究者尝试用神经网络来研究语言模型。
早在2000年，就有研究者提出用神经网络研究语言模型的想法，经典代表有2003年Bengio等[2]提出的NNLM，但效果并不显著，深度学习用于NLP的研究一直处在探索的阶段。直到2011年，Collobert等[3]用一个简单的深度学习模型在命名实体识别NER、语义角色标注SRL、词性标注POS-tagging等NLP任务取得SOTA成绩，基于深度学习的研究方法得到越来越多关注。2013年，以Word2vec、Glove为代表的词向量大火，更多的研究从词向量的角度探索如何提高语言模型的能力，研究关注词内语义和上下文语义。此外，基于深度学习的研究经历了CNN、RNN、Transormer等特征提取器，研究者尝试用各种机制优化语言模型的能力，包括预训练结合下游任务微调的方法。最近最吸睛的EMLo、GPT和BERT模型，便是这种预训练方法的优秀代表，频频刷新SOTA。

2 模型 接下来文章会以循序渐进的方式分成五个部分来介绍深度学习模型：第一部分介绍一些基础模型；第二部分介绍基于CNN的模型；第三部分介绍基于RNN的模型；第四部分介绍基于Attention机制的模型；第五部分介绍基于Transformer的模型，讲述一段语言模型升级迭代的故事。NLP有很多模型和方法，不同的任务场景有不同的模型和策略来解决某些问题。笔者认为，理解了这些模型，再去理解别的模型便不是很难的事情，甚至可以自己尝试设计模型来满足具体任务场景的需求。
Basic Embedding Model
NNLM是非常经典的神经网络语言模型，虽然相比传统机器学习并没有很显著的成效，但大家说起NLP在深度学习方向的探索，总不忘提及这位元老。分布式表示的概念很早就有研究者提出并应用于深度学习模型，Word2vec使用CBOW和Skip-gram训练模型，意外的语意组合效果才使得词向量广泛普及。FastText在语言模型上没有什么特别的突破，但模型的优化使得深度学习模型在大规模数据的训练非常快甚至秒级，而且文本分类的效果匹敌CNN/RNN之类模型，在工业化应用占有一席之地。

NNLM 2003年Bengio等[2]提出一种基于神经网络的语言模型NNLM，模型同时学习词的分布式表示，并基于词的分布式表示，学习词序列的概率函数，这样就可以用词序列的联合概率来表示句子，而且，模型还能够产生指数量级的语义相似的句子。作者认为，通过这种方式训练出来的语言模型具有很强的泛化能力，对于训练语料里没有出现过的句子，模型能够通过见过的相似词组成的相似句子来学习。
模型训练的目标函数是长度为n的词序列的联合概率，分解成两部分：一是特征映射，通过映射矩阵C，将词典V中的每个单词都能映射成一个特征向量C(i) ∈ Rm；二是计算条件概率分布，通过函数g，将输入的词向量序列(C(wt−n+1),··· ,C(wt−1))转化成一个概率分布y ∈ R|V|，g的输出是个向量，第i个元素表示词序列第n个词是Vi的概率。网络输出层采用softmax函数。

Word2vec 对于复杂的自然语言任务进行建模，概率模型应运而生成为首选的方法，但在最开始的时候，学习语言模型的联合概率函数存在致命的维数灾难问题。假如语言模型的词典大小为100000，要表示10个连续词的联合分布，模型参数可能就要有1050个。相应的，模型要具备足够的置信度，需要的样本量指数级增加。为了解决这个问题，最早是1986年Hinton等[5]提出分布式表示（Distributed Representation），基本思想是将词表示成n维连续的实数向量。分布式表示具备强大的特征表示能力，n维向量，每维有k个值，便能表示 kn个特征。
词向量是NLP深度学习研究的基石，本质上遵循这样的假设：语义相似的词趋向于出现在相似的上下文。因此在学习过程中，这些向量会努力捕捉词的邻近特征，从而学习到词汇之间的相似性。有了词向量，便能够通过计算余弦距离等方式来度量词与词之间的相似度。
Mikolov等[4]对词向量的广泛普及功不可没，他们用CBOW和Skip-gram模型训练出来的词向量具有神奇的语意组合特效，词向量的加减计算结果刚好是对应词的语意组合，譬如v(King) - v(Man) + v(Woman) = v(Queen)。这种意外的特效使得Word2vec快速流行起来。至于为什么会有这种行为呢？有意思的是Gittens等[10]做了研究，尝试给出了理论假设：词映射到低维分布式空间必须是均匀分布的，词向量才能有这个语意组合效果。
CBOW和Skip-gram是Word2vec的两种不同训练方式。CBOW指抠掉一个词，通过上下文预测该词；Skip-gram则与CBOW相反，通过一个词预测其上下文。
以CBOW为例。CBOW模型是一个简单的只包含一个隐含层的全连接神经网络，输入层采用one-hot编码方式，词典大小为V；隐含层大小是N；输出层通过softmax函数得到词典里每个词的概率分布。层与层之间分别有两个权重矩阵W ∈ RV ×N和W′ ∈ RH×V。词典里的每个单词都会学习到两个向量vc和vw，分别代表上下文向量和目标词向量。因此，给予上下文c，出现目标词w的概率是：p(w|c)，该模型需要学习的网络参数θ 。
词向量是用概率模型训练出来的产物，对训练语料库出现频次很低甚至不曾出现的词，词向量很难准确地表示。词向量是词级别的特征表示方式，单词内的形态和形状信息同样有用，研究者提出了基于字级别的表征方式，甚至更细粒度的基于Byte级别的表征方式。2017年，Mikolov等[9]提出用字级别的信息丰富词向量信息。此外，在机器翻译相关任务里，基于Byte级别的特征表示方式BPE还被用来学习不同语言之间的共享信息，主要以拉丁语系为主。2019年，Lample等[11]提出基于BERT优化的跨语言模型XLM，用BPE编码方式来提升不同语言之间共享的词汇量。
虽然通过词向量，一个单词能够很容易找到语义相似的单词，但单一词向量，不可避免一词多义问题。对于一词多义问题，最常用有效的方式便是基于预训练模型，结合上下文表示单词，代表模型有EMLo、GPT和BERT，都能有效地解决这个问题。
词，可以通过词向量进行表征，自然会有人思考分布式表示这个概念是否可以延伸到句子、文章、主题等。词袋模型Bag-of-Words忽略词序以及词语义，2014年Mikolov等[8]将分布式向量的表示扩展到句子和文章，模型训练类似word2vec。关于句子的向量表示，2015年Kiros等提出skip-thought，2018年Logeswaran等提出改进版quick-thought。

FastText 2016年，Mikolov等[7]提出一种简单轻量、用于文本分类的深度学习模型，架构跟Mikolov等[4]提出的word2vec的CBOW模型相似，但有所不同：各个词的embedding向量，补充字级别的n-gram特征向量，然后将这些向量求和平均，用基于霍夫曼树的分层softmax函数，输出对应的类别标签。FastText能够做到效果好、速度快，优化的点有两个：一是引入subword n-gram的概念解决词态变化的问题，利用字级别的n-gram信息捕获字符间的顺序关系，依次丰富单词内部更细微的语义；二是用基于霍夫曼树的分层softmax函数，将计算复杂度从O(kh)降低到O(h log2(k))，其中，k是类别个数，h是文本表示的维数。相比char-CNN之类的深度学习模型需要小时或天的训练时间，FastText只需要秒级的训练时间。
CNN-based Model
词向量通过低维分布式空间能够有效地表示词，使其成为NLP深度学习研究的基石。在词向量的基础上，需要有一种有效的特征提取器从词向量序列里提取出更高层次的特征，应用到NLP任务中去，譬如机器翻译、情感分析、问答、摘要，等。鉴于卷积神经网络CNN在机器视觉领域非常出色的特征提取能力，自然而然被研究者尝试用于自然语言处理领域。
回顾CNN应用于NLP领域的过去，好像一段民间皇子误入宫廷的斗争史。CNN用于句子建模最早可以追溯到2008年Collobert等[12]的研究，使用的是查找表映射词的表征方式，这可以看作是一种原始的词向量方法。2011年Collobert等[3]扩展了他的研究工作，提出一种基于CNN的通用框架用于各种NLP任务。Collobert[3]、Kalchbrenner[14]、Kim[13]等基于CNN的研究工作，推进了CNN应用在NLP研究的普及。
CNN擅长捕获局部特征，能够将信息量丰富的潜在语义特征用于下游任务，譬如TextCNN用于文本分类。但CNN却有长距离依赖问题，研究者尝试用动态卷积网络DCNN来改善这个不足。跟RNN的竞争过程中，CNN不停地优化自身能力，现在CNN的研究趋势是：加入GLU/GTU门机制来简化梯度传播，使用Dilated CNN增加覆盖长度，基于一维卷积层叠加深度并用Residual Connections辅助优化，优秀代表：增加网络深度的VDCNN和引进Gate机制的GCNN。

TextCNN 2014年，Kim[13]提出基于预训练Word2vec的TextCNN模型用于句子分类任务。CNN，通过卷积操作做特征检测，得到多个特征映射，然后通过池化操作对特征进行筛选，过滤噪音，提取关键信息，用来分类。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/nlp%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%84%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E6%83%B3%E6%BC%94%E8%BF%9B/">语言表征学习的思想演进
</a>
</h2>

  <time datetime="2019-06-30">30 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
参考文章：https://www.jiqizhixin.com/articles/2019-06-29-3
**在预训练语言模型 BERT 对自然语言处理的冲击还未平息时，CMU 和 Google 的研究员又放出了一个猛料：在 20 多项任务上全线碾压 BERT 的 XLNet。：由于在公众号中插入方式不方便，对于一个符号 &ldquo;a{b}^{c}&ldquo;，&rdquo;{b}&rdquo; 代表下标，&rdquo;{c}&rdquo; 代表上标。

1. 语言表征学习 深度学习的基本单元是向量。我们将建模对象对应到各自的向量 x (或者一组向量 x{1}, x{2}, &hellip;, x{n})，然后通过变换、整合得到新的向量 h，再基于向量 h 得到输出的判断 y。这里的 h 就是我们说的表征 (Representation)，它是一个向量，描述了我们的建模对象。而语言表征学习就是解决怎么样将一个词、一句话、一篇文章通过变换 (Transformation) 和整合 (Aggregation) 转化成对应的向量 h 的问题。
深度学习解决这个问题的办法是人工设计一个带有可调参数的模型，通过指定一系列的 (输入→输出) 对 (x → y)，让模型学习得到最优的参数。当参数确定之后，模型除了可以完成从 x 预测 y 的任务之外，其中间把 x 变换成 h 的方法也是可以用到其他任务的。这也是我们为什么要做表征学习。（就是说我们在做预测任务同时，顺便把表征学习这件事情给做了）
所以我们要解决的问题便是：
 怎么确定 (输入→输出) 对，即模型的预测任务
 这个模型怎么设计
  
2. 分布式语义假设 任何任务都可以用来做表征学习：情感分析 (输入句子，判断句子是正向情感还是负向情感)，机器翻译 (输入中文，输出英文)。但是这些任务的缺点是需要大量的人工标注，这些标注耗时耗力。当标注量不够时，模型很容易学出&rdquo;三长一短选最短&rdquo;的取巧方案 &ndash; 但我们想要的是真正的语言理解。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E6%83%B3%E6%BC%94%E8%BF%9B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/">Transformer技术实战
</a>
</h2>

  <time datetime="2019-06-25">25 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
欢迎访问Colab链接
https://colab.research.google.com/drive/1FH6ZpQA1HGX3GZCm0yxidq7ztoyd6KhB
这里以京东商城评论文本goods_zh.txt,10万条，标注为0的是差评，标注为1的是好评。中文商品评论短文本分类器，可用于情感分析。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/">基于UNet神经网络的城市人流预测
</a>
</h2>

  <time datetime="2019-06-24">24 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
1 利用手机信令数据计算人口流动数据 手机信令数据是研究人口的整体流动情况的重要数据来源。移动运营商在为手机用户提供实时通讯服务时，积累了大量的基站与设备的服务配对数据。根据配对和唤醒发生的时间以及基站的地理位置，可以很自然划定一定时间和空间范围，统计每一个时间范围内在特定空间区域内手机设备的停留，进入和离开数据，并据此估算相应的人流数据。
传统的人口网格分析过程一般只关注单个网格内某个时间点人口流动的截面数据以及城市当中不同区域的人口分布统计情况；没有将时间和空间融合考虑，不能对城市整体的人口流动形成一个完整连续的直观描述。但是，作为城市安全运营管理者和规划人员职责是需要把握好城市人口流动规律，建立有效的时空数据分析模型，从而为城市安全运行管理做好相应的人口短时预测与应急管理服务。 2 人口流动数据网格图像流处理 2.1 流处理思路 原始手机信令数据，按照一定的时间间隔（例如15分钟），划分出每个时间段的信令数据情况。主要包括：时间，格网ID，格网中心点经度，格网中心点维度，时间段内格网的停留人数，进入人数，离开人数。
根据原始数据的时空关系，将原始数据转化为4维向量空间矩阵，维度分别为时间维度、空间维度横坐标，空间维度纵坐标以及停留，进入或离开的类别：Matrix[t,i,j,k]=p意味着在t时刻，第i行第j列的空间栅格位置，k=0时则停留人数为p，k=1时则进入人数为p，k=2时则离开人数为p。
在这样的转换关系下，可以将源数据处理为3通道的时空数据。考虑到单个人员流动的时空连续性，可以认为表示人口流通的整体统计量的时空矩阵也具备一定局部关联性，换而言之，一个栅格点的人口流动数据会与该栅格附近的人口流行数据相互关联，也会与前后时间段该栅格的人口流动数据相互关联。而具体的关联形式和影响强度，则需要我们利用卷积神经，对历史数据进行学习来发现和记录相应的关联关系。
更进一步地，通过数据洞察注意到，不同栅格网络间人口流动的时间变化曲线往往倾向于若干种固定模式，直观上，商业区，住宅区，办公区域会呈现出不同的人流曲线变化模式。这种模式与地理位置，用地规划，交通路网信息等属性息息相关。本模型后续将进一步讨论不同用地类型的栅格人口流动模式的比较分析。
   TIME TAZID STAY ENTER EXIT     2017-04-05 00:00:00 1009897 460 460 52    2.2 人口栅格数据矢量化 基于一定的空间距离间隔（例如250m），将分析的目标空间划分为若干网格(141*137)。统计T时间内，属于网格M_(p,q)的手机设备停留、进入和离开的数据。按照业务需求，将手机设备数扩样为人口数量，将停留、进入和离开的数据标准化到（0,255）的空间，并将标准化后的数据作为图像的3个颜色通道，据此将T时间的整体网格数据转化为一张三通道格式的图片数据。按照时间维度将经过上述处理的图像作为视频的每一帧图像。
import pandas as pd import numpy as np import h5py # 数据转换成张量类型 data_enter_exit_sz = pd.read_csv('data/sz/data/TBL_ENTER_EXIT_SZ20170401-20170431.csv') time_list = data_enter_exit_sz['TIME'].unique() N = len(time_list) string_to_ix = {string:i for i,string in enumerate(time_list)} tensor_data = np.zeros([N,141,137,3]) for _,line in data_enter_exit_sz.
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/">特征工程方法论
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
欢迎浏览我的Azure Notebooks
https://notebooks.azure.com/messi7125/projects/feature-engineering
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/">Nlp中的词向量
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
词向量对比：word2vec/glove/fastText/elmo/GPT/bert（非常好）
本文以QA形式对自然语言处理中的词向量进行总结：包含word2vec/glove/fastText/elmo/bert。
目录 一、文本表示和各词向量间的对比
 1、文本表示哪些方法？
 2、怎么从语言模型理解词向量？怎么理解分布式假设？
 3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？
 4、word2vec和NNLM对比有什么区别？（word2vec vs NNLM）
 5、word2vec和fastText对比有什么区别？（word2vec vs fastText）
 6、glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）
 7、elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）
  二、深入解剖word2vec
 1、word2vec的两种模型分别是什么？
 2、word2vec的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？
  三、深入解剖Glove详解
 1、GloVe构建过程是怎样的？
 2、GloVe的训练过程是怎样的？
 3、Glove损失函数是如何确定的？
  四、深入解剖bert（与elmo和GPT比较）
 1、为什么bert采取的是双向Transformer Encoder，而不叫decoder？
 2、elmo、GPT和bert在单双向语言模型处理上的不同之处？
 3、bert构建双向语言模型不是很简单吗？不也可以直接像elmo拼接Transformer decoder吗？
 4、为什么要采取Marked LM，而不直接应用Transformer Encoder？
 5、bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？
  一、文本表示和各词向量间的对比
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/">Read more &rarr;</a>

</div>


</article>

  
  
<div class="container pagination">
  


<a aria-label="First" href="https://7125messi.github.io/">
  <span aria-hidden="true">««</span>
</a>

<a aria-label="Previous" href="https://7125messi.github.io/">
  <span aria-hidden="true">«</span>
</a>


<a href="https://7125messi.github.io/">
  1
</a>

<a class="active" href="https://7125messi.github.io/page/2/">
  2
</a>


<a class="disabled" aria-label="Next" href="#">
  <span aria-hidden="true">»</span>
</a>

<a aria-label="Last" href="https://7125messi.github.io/page/2/">
  <span aria-hidden="true">»»</span>
</a>


</div>


</section>

      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

