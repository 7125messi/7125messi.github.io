<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Transformer技术详解  &middot; 7125messi的博客</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Transformer技术详解  &middot; 7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2019-06-23T16:49:10&#43;08:00" />
<meta property="og:article:modified_time" content="2019-06-23T16:49:10&#43;08:00" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Transformer技术详解" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/" />
<meta name="twitter:domain" content="https://7125messi.github.io">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Transformer技术详解",
    "author": {
      "@type": "Person",
      "name": ""
    },
    "datePublished": "2019-06-23",
    "description": "",
    "wordCount":  211 
  }
</script>



<link rel="canonical" href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Transformer技术详解
</h1>

  <div class="metas">
<time datetime="2019-06-23">23 Jun, 2019</time>


  
  &middot; Read in about 1 min
  &middot; (211 Words)
  <br>
  


</div>

</header>

  <div class="container content">
  

<p>参考文章：<a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>

<p>谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。</p>

<p>Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。</p>

<p>在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。<br />Attention is All You Need：<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>

<p>谷歌团队近期提出的用于生成词向量的BERT算法在NLP的11项任务中取得了效果的大幅提升，堪称2018年深度学习领域最振奋人心的消息。而BERT算法的最重要的部分便是本文中提出的Transformer的概念。</p>

<p>正如论文的题目所说的，<strong>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。</strong>更准确地讲，<strong>Transformer由且仅由self-Attenion和Feed Forward Neural Network组成</strong>。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。</p>

<p>作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p>

<ol>
<li>间片 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1561270898289-33cf9186-b06d-4f14-ae50-568a2497019b.svg#align=left&amp;display=inline&amp;height=16&amp;originHeight=16&amp;originWidth=7&amp;size=0&amp;status=done&amp;width=7" alt="" /> 的计算依赖 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1561270898301-1d28f538-0c17-4d2c-92dd-aa93182c2ac5.svg#align=left&amp;display=inline&amp;height=19&amp;originHeight=19&amp;originWidth=39&amp;size=0&amp;status=done&amp;width=39" alt="" /> 时刻的计算结果，这样限制了模型的并行能力；</li>
<li>顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。</li>
</ol>

<p>Transformer的提出解决了上面两个问题，<strong>首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量</strong>；<strong>其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。</strong>论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</p>

<p><a name="bM4cy"></a></p>

<h1 id="1-transformer-详解">1. Transformer 详解</h1>

<p><a name="e8hc4"></a></p>

<h2 id="1-1-宏观视角认识transformer整体框架">1.1 宏观视角认识Transformer整体框架</h2>

<p>论文中的验证Transformer的实验室基于机器翻译的，下面我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为如图1：可以看到它是由编码组件、解码组件和它们之间的连接组成。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274719819-c483f9e4-cb5f-4cb8-a24a-8037e6101277.png#align=left&amp;display=inline&amp;height=195&amp;name=image.png&amp;originHeight=294&amp;originWidth=1127&amp;size=37539&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274475431-94ce6bb7-20a9-44e1-91a4-ea51d326eaeb.png#align=left&amp;display=inline&amp;height=153&amp;name=image.png&amp;originHeight=221&amp;originWidth=1080&amp;size=107920&amp;status=done&amp;width=746" alt="image.png" /><br />图1：Transformer用于机器翻译<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274708439-bf7ac03f-f48c-4516-91e5-3cf8afecfb9b.png#align=left&amp;display=inline&amp;height=468&amp;name=image.png&amp;originHeight=474&amp;originWidth=756&amp;size=32434&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274502340-1feb1d52-4d7b-41e7-8625-28253550d61d.png#align=left&amp;display=inline&amp;height=508&amp;name=image.png&amp;originHeight=624&amp;originWidth=917&amp;size=158027&amp;status=done&amp;width=746" alt="image.png" /><br />图2：Transformer的Encoder-Decoder结构</p>

<p>编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274696198-0bf96dc0-233f-496a-8d44-dba240c4a4c8.png#align=left&amp;display=inline&amp;height=486&amp;name=image.png&amp;originHeight=793&amp;originWidth=1218&amp;size=121019&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274550473-b5da3ece-a954-4ebd-9573-6c07ca29b657.png#align=left&amp;display=inline&amp;height=523&amp;name=image.png&amp;originHeight=663&amp;originWidth=946&amp;size=272944&amp;status=done&amp;width=746" alt="image.png" /></p>

<p>图3：Transformer的Encoder和Decoder均由6个block堆叠而成<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561272307495-3e017537-c600-4b3e-b8a1-df6ccaf93998.png#align=left&amp;display=inline&amp;height=406&amp;name=image.png&amp;originHeight=812&amp;originWidth=1404&amp;size=187574&amp;status=done&amp;width=702" alt="image.png" /><br />
<br /><strong>所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274684417-73533a8d-5122-42a1-affb-02e369120de2.png#align=left&amp;display=inline&amp;height=387&amp;name=image.png&amp;originHeight=411&amp;originWidth=792&amp;size=26157&amp;status=done&amp;width=746" alt="image.png" /><br />
<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274564739-e079e86d-a3a6-45cf-a2ef-8f0a1eda1980.png#align=left&amp;display=inline&amp;height=332&amp;name=image.png&amp;originHeight=468&amp;originWidth=1051&amp;size=84579&amp;status=done&amp;width=746" alt="image.png" /><br />图4：Transformer由self-attention和Feed Forward neural network组成</p>

<p>Decoder的结构如图5所示，它和encoder的不同之处在于<strong>Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：</strong></p>

<ul>
<li>Self-Attention：当前翻译和已经翻译的前文之间的关系；</li>
<li>Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。</li>
</ul>

<p>从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在<strong>对每个单词编码时关注输入句子的其他单词</strong>。我们将在稍后的文章中更深入地研究自注意力。</p>

<ul>
<li><strong>自注意力层的输出会传递到前馈（feed-forward）神经网络中</strong>。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。</li>
<li><strong>解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层</strong>。除此之外，这两个层之间还有一个注意力层，用来<strong>关注输入句子的相关部分（和seq2seq模型的注意力作用相似）</strong>。</li>
</ul>

<p><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561272602932-1a889803-b75d-4a20-9a53-c23578859b32.png#align=left&amp;display=inline&amp;height=237&amp;name=image.png&amp;originHeight=279&amp;originWidth=879&amp;size=32061&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274646044-151b2bcd-3709-46d7-9755-084664fe013c.png#align=left&amp;display=inline&amp;height=228&amp;name=image.png&amp;originHeight=330&amp;originWidth=1080&amp;size=130772&amp;status=done&amp;width=746" alt="image.png" /><br />图5：Transformer的解码器由self-attention，encoder-decoder attention以及FFNN组成</p>

<p><a name="44FxX"></a></p>

<h2 id="1-2-输入编码">1.2 输入编码</h2>

<p>我们已经了解了模型的主要部分，接下来我们看一下各种向量或张量（译注：张量概念是矢量概念的推广，可以简单理解矢量是一阶张量、矩阵是二阶张量。）是怎样在模型的不同部分中，将输入转化为输出的。</p>

<p>像大部分NLP应用一样，我们首先<strong>将每个输入单词通过词嵌入算法转换为词向量。</strong></p>

<p>每个单词都被嵌入为512维的向量，我们用这些简单的方框来表示这些向量。</p>

<p>词嵌入过程只发生在最底层的编码器中。所有的编码器都有一个相同的特点，<strong>即它们接收一个向量列表，列表中的每个向量大小为512维。</strong>在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。<strong>向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。</strong></p>

<p><strong>将输入序列进行词嵌入之后，每个单词都会流经编码器中的两个子层。</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273465995-31cacbf2-9c1a-4b2b-b9bc-230bcb4d7f83.png#align=left&amp;display=inline&amp;height=84&amp;name=image.png&amp;originHeight=168&amp;originWidth=1398&amp;size=53198&amp;status=done&amp;width=699" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273088336-738f0a97-c2ab-4116-bbbf-63c6acb3bec7.png#align=left&amp;display=inline&amp;height=90&amp;name=image.png&amp;originHeight=99&amp;originWidth=824&amp;size=6691&amp;status=done&amp;width=746" alt="image.png" /><br />图6：单词的输入编码</p>

<p>接下来我们看看Transformer的一个核心特性，在这里<strong>输入序列中每个位置的单词都有自己独特的路径流入编码器</strong>。在自注意力层中，这些路径之间存在依赖关系。而<strong>前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。</strong></p>

<p>我们将以一个更短的句子为例，<strong>看看编码器的每个子层中发生了什么。</strong></p>

<p>在最底层的block中， <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1561273104270-d2b87bb6-ebf0-4e82-8821-cc93ec2d1639.svg#align=left&amp;display=inline&amp;height=13&amp;originHeight=13&amp;originWidth=11&amp;size=0&amp;status=done&amp;width=11" alt="" /> 将直接作为Transformer的输入，而在其他层中，输入则是上一个block的输出。为了画图更简单，我们使用更简单的例子来表示接下来的过程，如图7所示：<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273126871-bb8f15a5-bc4c-4192-bc61-edd8b0689c88.png#align=left&amp;display=inline&amp;height=478&amp;name=image.png&amp;originHeight=694&amp;originWidth=1082&amp;size=50307&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273244218-59c1e6eb-9177-46ba-bb11-cec814f5a44d.png#align=left&amp;display=inline&amp;height=386&amp;name=image.png&amp;originHeight=771&amp;originWidth=1268&amp;size=72167&amp;status=done&amp;width=634" alt="image.png" /><br />图7：输入编码作为一个tensor输入到encoder中
<a name="Hd2vn"></a></p>

<h2 id="1-3-self-attention">1.3 Self-Attention</h2>

<p>如上述已经提到的，一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，将输出结果传递到下一个编码器中。</p>

<p><strong>输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。</strong></p>

<p>Self-Attention是Transformer最核心的内容，然而作者并没有详细讲解，下面我们来补充一下作者遗漏的地方。回想Bahdanau等人提出的<strong>用Attention，其核心内容是为输入向量的每个单词学习一个权重</strong>，例如在下面的例子中我们判断it代指的内容，</p>

<pre><code>The animal didn't cross the street because it was too tired
</code></pre>

<p>通过加权之后可以得到类似图8的加权情况，在讲解self-attention的时候我们也会使用图8类似的表示方式</p>

<p><a name="jXqDM"></a></p>

<h3 id="从宏观视角看自注意力机制">从宏观视角看自注意力机制</h3>

<p>这个“it”在这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。</p>

<p>当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。</p>

<p>随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。<br />如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词/向量的表示与它正在处理的当前单词/向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。</p>

<p>当我们在编码器#5（栈中最上层编码器）中编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。</p>

<p>请务必检查Tensor2Tensor notebook ，在里面你可以下载一个Transformer模型，并用交互式可视化的方式来检验。</p>

<p><a name="cogcY"></a></p>

<h3 id="从微观视角看自注意力机制">从微观视角看自注意力机制</h3>

<p>首先我们了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。</p>

<p>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。</p>

<p>可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273268187-e981f15f-0487-4ed6-ada2-8315deda44ee.png#align=left&amp;display=inline&amp;height=705&amp;name=image.png&amp;originHeight=413&amp;originWidth=437&amp;size=42563&amp;status=done&amp;width=746" alt="image.png" />图8：经典Attention可视化示例图<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273317781-b9383836-b91e-4ca1-810b-9f25fef4305e.png#align=left&amp;display=inline&amp;height=87&amp;name=image.png&amp;originHeight=174&amp;originWidth=1410&amp;size=65069&amp;status=done&amp;width=705" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273407130-b8b5e3a9-4810-417d-bcf7-2fedc8bfb01a.png#align=left&amp;display=inline&amp;height=471&amp;name=image.png&amp;originHeight=552&amp;originWidth=875&amp;size=31610&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561275368171-2ceccdab-2d6f-47c2-a5f8-2d4c9b518c34.png#align=left&amp;display=inline&amp;height=425&amp;name=image.png&amp;originHeight=616&amp;originWidth=1080&amp;size=164927&amp;status=done&amp;width=746" alt="image.png" /><br />图9：Q，K，V的计算示例图<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273437031-486cb462-0e7b-4d41-8d80-45b79d9b8367.png#align=left&amp;display=inline&amp;height=312&amp;name=image.png&amp;originHeight=624&amp;originWidth=1420&amp;size=151341&amp;status=done&amp;width=710" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273579945-61196af3-a5de-44d4-8934-70a957c6cff1.png#align=left&amp;display=inline&amp;height=709&amp;name=image.png&amp;originHeight=747&amp;originWidth=786&amp;size=54643&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561275397951-65150539-39ce-488e-8e07-4b68c4009f22.png#align=left&amp;display=inline&amp;height=723&amp;name=image.png&amp;originHeight=723&amp;originWidth=746&amp;size=184506&amp;status=done&amp;width=746" alt="image.png" /><br />图10：Self-Attention计算示例图<br />
<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273632063-aabeb3ea-2ac6-4a75-9d65-ff88236ae116.png#align=left&amp;display=inline&amp;height=25&amp;name=image.png&amp;originHeight=50&amp;originWidth=1360&amp;size=22201&amp;status=done&amp;width=680" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273650253-f3ecc2d9-9c6e-44a0-b2cd-ec8658e3e24c.png#align=left&amp;display=inline&amp;height=845&amp;name=image.png&amp;originHeight=658&amp;originWidth=581&amp;size=21319&amp;status=done&amp;width=746" alt="image.png" /><br />图11：Q，V，K的矩阵表示<br />图10总结为如图12所示的矩阵形式：<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273722801-f3cb490e-b842-4f51-a039-a21df476f2e8.png#align=left&amp;display=inline&amp;height=292&amp;name=image.png&amp;originHeight=349&amp;originWidth=893&amp;size=20891&amp;status=done&amp;width=746" alt="image.png" /><br />图12：Self-Attention的矩阵表示<br />这里也就是公式1的计算方式。</p>

<p><a name="6Igm3"></a></p>

<h3 id="残差模块">残差模块</h3>

<p>我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。<br />层-归一化步骤：<a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a></p>

<p><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561276899291-e1f8ad70-456a-47c3-ba7a-3eb5168bb66f.png#align=left&amp;display=inline&amp;height=504&amp;name=image.png&amp;originHeight=648&amp;originWidth=960&amp;size=273162&amp;status=done&amp;width=746" alt="image.png" /></p>

<p><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273835888-37fb8cd1-bfb9-4ebb-8924-46c092f0957b.png#align=left&amp;display=inline&amp;height=541&amp;name=image.png&amp;originHeight=483&amp;originWidth=666&amp;size=57085&amp;status=done&amp;width=746" alt="image.png" /></p>

<p>如果我们去可视化这些向量以及这个和自注意力相关联的层-归一化操作，那么看起来就像下面这张图描述一样<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561276964759-a7b8fedb-3c31-4a0a-830c-825deda1d330.png#align=left&amp;display=inline&amp;height=682&amp;name=image.png&amp;originHeight=586&amp;originWidth=641&amp;size=239717&amp;status=done&amp;width=746" alt="image.png" /></p>

<pre><code>    ![image.png](https://cdn.nlark.com/yuque/0/2019/png/200056/1561274220831-259cb492-a4fa-4716-b089-5488f1c7ed20.png)
</code></pre>

<p><strong>解码器的子层也是这样样的。如果我们想象一个2 层编码-解码结构的transformer，它看起来会像下面这张图一样：</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561277006434-e95812cd-384d-4e44-89c4-bc6aeffd6dbe.png#align=left&amp;display=inline&amp;height=436&amp;name=image.png&amp;originHeight=375&amp;originWidth=641&amp;size=222389&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274250805-cfcd1789-d067-4f95-9473-ac704445a765.png#align=left&amp;display=inline&amp;height=424&amp;name=image.png&amp;originHeight=804&amp;originWidth=1415&amp;size=180905&amp;status=done&amp;width=746" alt="image.png" /><br />
<br />在self-attention需要强调的最后一点是其采用了<a href="https://zhuanlan.zhihu.com/p/42706477">残差网络</a> [5]中的short-cut结构，目的是解决深度学习中的退化问题，得到的最终结果如图13。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561277118820-431097f5-f897-478a-a3e0-99f569b13c9b.png#align=left&amp;display=inline&amp;height=541&amp;name=image.png&amp;originHeight=483&amp;originWidth=666&amp;size=57085&amp;status=done&amp;width=746" alt="image.png" /><br />图13：Self-Attention中的short-cut连接<br /></p>

<p><a name="nitdJ"></a></p>

<h2 id="1-4-multi-head-attention">1.4 Multi-Head Attention</h2>

<p>通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：</p>

<p><strong>1.它扩展了模型专注于不同位置的能力。</strong>在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。</p>

<p><strong>2.它给出了注意力层的多个“表示子空间”（representation subspaces）。</strong>接下来我们将看到，对于“多头”注意机制，我们<strong>有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。</strong></p>

<p>3.在“多头”注意机制下，我们为<strong>每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵</strong>。和之前一样，我们拿X乘以WQ/WK/WV矩阵来产生查询/键/值矩阵。如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。</p>

<p>4.这给我们带来了一点挑战。前馈层不需要8个矩阵，它<strong>只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵WO与它们相乘。</strong></p>

<p>5.这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。</p>

<p>整体流程：<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273945791-7e222c98-ccfd-4a63-a2ab-6148cbd0fb9d.png#align=left&amp;display=inline&amp;height=229&amp;name=image.png&amp;originHeight=458&amp;originWidth=1330&amp;size=104060&amp;status=done&amp;width=665" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274298284-381fe051-657a-4817-ab59-55b037313948.png#align=left&amp;display=inline&amp;height=441&amp;name=image.png&amp;originHeight=774&amp;originWidth=1310&amp;size=49464&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561275577045-b89fdccf-8a3c-42b5-8acf-0a8826654905.png#align=left&amp;display=inline&amp;height=509&amp;name=image.png&amp;originHeight=437&amp;originWidth=641&amp;size=133960&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274325585-7be26371-88e4-4fd0-9395-179ad4540a9c.png#align=left&amp;display=inline&amp;height=354&amp;name=image.png&amp;originHeight=483&amp;originWidth=1018&amp;size=39797&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561275593152-291d39f1-443f-45f1-8701-abfe20484078.png#align=left&amp;display=inline&amp;height=383&amp;name=image.png&amp;originHeight=488&amp;originWidth=951&amp;size=38604&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274340387-4a28b0e7-fb79-4b61-85ed-d1bab3a9fcc3.png#align=left&amp;display=inline&amp;height=413&amp;name=image.png&amp;originHeight=759&amp;originWidth=1372&amp;size=83217&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561275705829-886ef303-20c1-4473-ae5c-1d2e5fc26c49.png#align=left&amp;display=inline&amp;height=396&amp;name=image.png&amp;originHeight=573&amp;originWidth=1080&amp;size=205431&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561273988391-70ddddda-cba8-4240-be71-ad33a8c769e7.png#align=left&amp;display=inline&amp;height=418&amp;name=image.png&amp;originHeight=804&amp;originWidth=1436&amp;size=159531&amp;status=done&amp;width=746" alt="image.png" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561275767031-3eb7e01f-4ed6-4d63-8c16-ea9178a73ba1.png#align=left&amp;display=inline&amp;height=414&amp;name=image.png&amp;originHeight=599&amp;originWidth=1080&amp;size=341471&amp;status=done&amp;width=746" alt="image.png" /><br />图14：Multi-Head Attention<br />同self-attention一样，multi-head attention也加入了short-cut机制。</p>

<p><a name="8gwSE"></a></p>

<h2 id="1-5-解码组件">1.5 解码组件</h2>

<p>既然我们已经谈到了大部分编码器的概念，那么我们基本上也就知道解码器是如何工作的了。但最好还是看看解码器的细节。</p>

<p><strong>编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。</strong>这些向量将被<strong>每个解码器用于自身的“编码-解码注意力层”，</strong>而这些层可以帮助解码器关注输入序列哪些位置合适：</p>

<p>在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列（在这个例子里，是英语翻译的句子）的元素</p>

<p>接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示transformer的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。</p>

<p>而那些<strong>解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf）。</strong></p>

<p>这<strong>个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵。</strong></p>

<p><img src="https://cdn.nlark.com/yuque/0/2019/gif/200056/1561277313125-eb97da1a-3d09-44df-85ed-b1949d2e9974.gif#align=left&amp;display=inline&amp;height=790&amp;name=transformer_decoding_1.gif&amp;originHeight=790&amp;originWidth=1438&amp;size=3696607&amp;status=done&amp;width=1438" alt="transformer_decoding_1.gif" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/gif/200056/1561277957178-e6e125d9-8695-4fe7-9de5-57fdc4e81403.gif#align=left&amp;display=inline&amp;height=790&amp;name=transformer_decoding_2.gif&amp;originHeight=790&amp;originWidth=1438&amp;size=6168687&amp;status=done&amp;width=1438" alt="transformer_decoding_2.gif" />
<a name="geo6U"></a></p>

<h2 id="1-6-encoder-decoder-attention">1.6 Encoder-Decoder Attention</h2>

<p><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274095614-3e647ff7-dea5-430d-bc36-791b0380061a.png#align=left&amp;display=inline&amp;height=177&amp;name=image.png&amp;originHeight=354&amp;originWidth=1396&amp;size=103682&amp;status=done&amp;width=698" alt="image.png" /></p>

<p><a name="kFiGm"></a></p>

<h2 id="1-7-最终的线性变换和softmax层">1.7 最终的线性变换和Softmax层</h2>

<p><strong>解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。</strong></p>

<p>线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。</p>

<p><strong>不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。</strong></p>

<p>接下来的<strong>Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561277758311-a70c13f7-68f7-4080-8507-2e2e66e45f5d.png#align=left&amp;display=inline&amp;height=472&amp;name=image.png&amp;originHeight=574&amp;originWidth=907&amp;size=161531&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561277743329-0e0904cb-9aea-4cfb-837e-d354d37cd19d.png#align=left&amp;display=inline&amp;height=482&amp;name=image.png&amp;originHeight=561&amp;originWidth=869&amp;size=64311&amp;status=done&amp;width=746" alt="image.png" />
<a name="j3NUS"></a></p>

<h2 id="1-8-损失层-br">1.8 损失层<br /></h2>

<p><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561274187245-dcdd9214-b15c-4056-b7f1-91652d2e3eb9.png#align=left&amp;display=inline&amp;height=126&amp;name=image.png&amp;originHeight=252&amp;originWidth=1394&amp;size=85626&amp;status=done&amp;width=697" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278149748-1528cadb-1efb-4482-ac53-f9fc4900c6c3.png#align=left&amp;display=inline&amp;height=1041&amp;name=image.png&amp;originHeight=968&amp;originWidth=694&amp;size=300748&amp;status=done&amp;width=746" alt="image.png" /></p>

<p><a name="cSmbY"></a></p>

<h1 id="2-位置编码">2. 位置编码</h1>

<p><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278215740-1ef89efa-fa36-48c9-9da5-bb23dbfbc92d.png#align=left&amp;display=inline&amp;height=282&amp;name=image.png&amp;originHeight=564&amp;originWidth=1394&amp;size=164853&amp;status=done&amp;width=697" alt="image.png" /></p>

<p>到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。</p>

<p>为了解决这个问题，Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278368760-24728c5c-1c4b-4835-a8f2-8e56ec302990.png#align=left&amp;display=inline&amp;height=423&amp;name=image.png&amp;originHeight=612&amp;originWidth=1080&amp;size=174607&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278355743-0160bbe2-acf1-4392-961a-514041156c7e.png#align=left&amp;display=inline&amp;height=411&amp;name=image.png&amp;originHeight=793&amp;originWidth=1438&amp;size=83284&amp;status=done&amp;width=746" alt="image.png" /><br />
        <img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278651969-7d8a01b7-e9ef-4ed0-a3c3-0d22cd3b9896.png" alt="image.png" /></p>

<p><br /></p>

<p>为了让模型理解单词的顺序，我们添加了位置编码向量，这些向量的值遵循特定的模式。<br />如果我们假设词嵌入的维数为4，则实际的位置编码如下：<img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278544223-281fcc2a-a6c1-4843-8e5d-427ca16453d0.png#align=left&amp;display=inline&amp;height=210&amp;name=image.png&amp;originHeight=292&amp;originWidth=1035&amp;size=110564&amp;status=done&amp;width=746" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278555740-bb0d6c4b-025f-42de-b61e-5d9dbaf8f2e1.png#align=left&amp;display=inline&amp;height=207&amp;name=image.png&amp;originHeight=290&amp;originWidth=1044&amp;size=35137&amp;status=done&amp;width=746" alt="image.png" /><br />尺寸为4的迷你词嵌入位置编码实例<br />这个模式会是什么样子？<br />在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1561278778099-3ebb8fb4-660f-4ddd-a4f0-f18d29218e7f.png#align=left&amp;display=inline&amp;height=547&amp;name=image.png&amp;originHeight=907&amp;originWidth=1238&amp;size=54134&amp;status=done&amp;width=746" alt="image.png" /><br />20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为<strong>左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。</strong>然后将它们<strong>拼在一起而得到每一个位置编码向量。</strong></p>

<p><strong>在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。</strong><br />**
<a name="Yta2h"></a></p>

<h1 id="3-总结">3. 总结</h1>

<p><strong>优点</strong>：</p>

<ul>
<li>（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其<strong>抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。</strong></li>
<li>（2）Transformer的设计<strong>最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。</strong></li>
<li>（3）Transformer<strong>不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。</strong></li>
<li>（4）算法的并行性非常好，<strong>符合目前的硬件（主要指GPU）环境。</strong></li>
</ul>

<p><strong>缺点</strong>：</p>

<ul>
<li>（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也<strong>使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果</strong>。</li>
<li>（2）Transformer<strong>失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</strong></li>
</ul>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" title="Paddle_hub和pytorch_hub预训练模型迁移学习">
      Previous
    </a>
    

    
    <a class="next" href="https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/" title="基于UNet神经网络的城市人流预测">
      Next
    </a>
    

  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

