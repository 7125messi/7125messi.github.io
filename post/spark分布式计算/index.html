<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Spark分布式计算  &middot; 7125messi的博客</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Spark分布式计算  &middot; 7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/post/spark%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2022-01-14T10:49:07&#43;08:00" />
<meta property="og:article:modified_time" content="2022-01-14T10:49:07&#43;08:00" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Spark分布式计算" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="https://7125messi.github.io/post/spark%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/" />
<meta name="twitter:domain" content="https://7125messi.github.io">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Spark分布式计算",
    "author": {
      "@type": "Person",
      "name": ""
    },
    "datePublished": "2022-01-14",
    "description": "",
    "wordCount":  934 
  }
</script>



<link rel="canonical" href="https://7125messi.github.io/post/spark%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Spark分布式计算
</h1>

  <div class="metas">
<time datetime="2022-01-14">14 Jan, 2022</time>


  
  &middot; Read in about 5 min
  &middot; (934 Words)
  <br>
  


</div>

</header>

  <div class="container content">
  <p>[项目经验总结]</p>

<p>从事数据相关工作，最喜欢用的工具就是基于Pandas、Jupyter Lab等工具，拿到样本数据，单机上快速迭代试验验证想法，这确实很方便，但是等到模型部署上线的时候，数据量很大，很难单机就搞定，目前主流的做法是用Spark分布式计算解决。</p>

<p>但是如果利用纯 PySpark API，就需要将Pandas API重写成PySpark的API，虽然很多API很类似，但是多少有些不一样，而且有些逻辑用用Pandas生态很容易实现，而利用PySpark却很复杂，遇到PySpark没有的API，动辄就要写UDF函数了，所以实际生产部署的时候，如果采用此方式，改造成本会有点高。</p>

<p>有没有简单的方法？</p>

<p>我们知道通常Spark也是作为客户端，使用Hadoop的YARN作为集群的资源管理和调度器。Spark集群由Driver, Cluster Manager（Standalone,Yarn 或 Mesos），以及Worker Node组成。对于每个Spark应用程序，Worker Node上存在一个Executor进程，Executor进程中包括多个Task线程。对于PySpark,为了不破坏Spark已有的运行时架构，Spark在外围包装一层Python API。在Driver端，借助Py4j实现Python和Java的交互，进而实现通过Python编写Spark应用程序。在Executor端，则不需要借助Py4j，因为Executor端运行的Task逻辑是由Driver发过来的，那是序列化后的字节码。</p>

<p>Spark运行流程</p>

<ul>
<li>Application首先被Driver构建DAG图并分解成Stage。</li>
<li>然后Driver向Cluster Manager申请资源。</li>
<li>Cluster Manager向某些Work Node发送征召信号。</li>
<li>被征召的Work Node启动Executor进程响应征召，并向Driver申请任务。</li>
<li>Driver分配Task给Work Node。</li>
<li>Executor以Stage为单位执行Task，期间Driver进行监控。</li>
<li>Driver收到Executor任务完成的信号后向Cluster Manager发送注销信号。</li>
<li>Cluster Manager向Work Node发送释放资源信号。</li>
<li>Work Node对应Executor停止运行。</li>
</ul>

<p>所以简单的做法跑PySparkr任务时利用YARN的分发机制，将可以并行计算的任务同时分发到不同Work Node计算，然后每个节点则利用由原来的Pandas API计算即可。</p>

<pre><code class="language-python">import sys
import calendar
from typing import Tuple,List
import pandas as pd
import numpy as np
from sklearn import linear_model

from pyspark.sql.types import StringType, IntegerType, DoubleType, DateType
from pyspark.sql import functions as F
from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, broadcast
from pyspark.sql import Row
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.window import Window

from foo.utils.conn_utils import SparkInit, spark_write_to_hive


def sales_diff_mom_feature(df, unit):
    diff_list = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (3, 5), (0, 12), (8, 12), (11, 12)]
    for (i, j) in diff_list:
        df[f'{target}_{unit}_diff_{i}_{j}'] = df[f'{target}_{unit}_lag_{i}'] - df[f'{target}_{unit}_lag_{j}']
        df[f'{target}_{unit}_mom_{i}_{j}'] = (df[f'{target}_{unit}_lag_{i}'] + 1) / (df[f'{target}_{unit}_lag_{j}'] + 1)

    return df


def sales_rolling_feature(df, window, unit):
    columns = [f'{target}_{unit}_lag_{i}' for i in range(window)]
    df[f'{target}_{unit}_lag_rolling_{window}_mean'] = df[columns].mean(axis=1).astype(np.float32)
    df[f'{target}_{unit}_lag_rolling_{window}_std'] = df[columns].std(axis=1).astype(np.float32)
    df[f'{target}_{unit}_lag_rolling_{window}_max'] = df[columns].max(axis=1).astype(np.float32)
    df[f'{target}_{unit}_lag_rolling_{window}_min'] = df[columns].min(axis=1).astype(np.float32)
    df[f'{target}_{unit}_lag_rolling_{window}_median'] = df[columns].median(axis=1).astype(np.float32)

    return df


def value_sku_agg_rate_lag_feature(df, aggregation, lags):
    df[f'value_sku-{&quot;_&quot;.join(aggregation)}_rate'] = df['value'] / df.groupby(aggregation)['value'].transform('sum')
    for lag in lags:
        df[f'value_sku-{&quot;_&quot;.join(aggregation)}_rate_lag_{lag}'] = df.groupby([division, 'material_code'])[f'value_sku-{&quot;_&quot;.join(aggregation)}_rate'].shift(lag).astype(np.float16)
    df = df.drop([f'value_sku-{&quot;_&quot;.join(aggregation)}_rate'], axis=1)

    return df


def get_days_of_month(days_dict, year_month):
    if year_month not in days_dict:
        year = int(str(year_month)[:4])
        month = int(str(year_month)[4:6])
        days = calendar.monthrange(year, month)[1]
        days_dict[year_month] = days

    return days_dict[year_month]


def time_features(df, division):
    df['year'] = df['year_month'].apply(lambda x: int(str(x)[:4])).astype('category')
    df['month'] = df['year_month'].apply(lambda x: int(str(x)[4:6])).astype('category')
    df['on_market_month'] = (df['month_idx'] - df['min_month_idx'] + 1).astype(np.int8)
    df['on_market_day_first_month'] = df.apply(lambda x: get_days_of_month({}, x['year_month']) - int(str(x['mindate'])[6:8]) + 1, axis=1)
    df[f'on_market_month_{division}'] = (df['month_idx'] - df[f'min_month_idx_{division}_sku'] + 1).astype(np.int8)

    return df


def sales_rolling_mean_feature_level(df, window, target, level):
    columns = [f'{target}_material_code_lag_{i}' for i in range(window)]
    df2 = pd.DataFrame(df.groupby([level, 'year_month'])[columns].sum()).reset_index()
    df2[f'{target}_lag_rolling_{window}_mean_{level}'] = df2[columns].mean(axis=1).astype(np.float32)
    df2 = df2.rename(columns={f'{target}_material_code_lag_{i}': f'{target}_{level}_lag_{i}' for i in range(window)})

    return df2[[level, 'year_month', f'{target}_lag_rolling_{window}_mean_{level}', f'{target}_{level}_lag_0']]


def latest_sale_proportion_feature(df, window, target, level):
    df2 = sales_rolling_mean_feature_level(df, window, target, level)
    df2[f'latest_{window}_{target}_proportion_{level}'] = df2[f'{target}_{level}_lag_0'] / df2[f'{target}_lag_rolling_{window}_mean_{level}']
    df = pd.merge(df, df2[[level, 'year_month', f'latest_{window}_{target}_proportion_{level}']], on=[level, 'year_month'], how='left')
    return df


def get_trend_by_lr(x, df_all, unit, window, target):
    current_month = x['month_idx']
    sku = x[unit]
    start_month = current_month - window
    condition = (df_all['month_idx'] &gt;= start_month) &amp; (df_all['month_idx'] &lt;= current_month) &amp; (df_all[unit] == sku)
    train_data = df_all.loc[condition, [target, 'month_idx']].drop_duplicates()

    coef = np.nan
    if len(train_data):
        model = linear_model.LinearRegression()
        model.fit(train_data['month_idx'].to_numpy().reshape(-1, 1), train_data[target])
        coef = model.coef_[0]
    return coef


def add_trend_feature(df, unit, window, target):
    df_unit = df.loc[df['on_market_month'] &gt; 3].groupby([unit, 'month_idx'])[target].sum().reset_index()
    df_unit = df_unit.dropna(axis=0, subset=[target])
    if df_unit.empty:
        return df
    try:
        df_unit[f'trend_slope_{unit}_{window}'] = df_unit.apply(lambda x: get_trend_by_lr(x, df_unit, unit, window, target), axis=1)
        df = pd.merge(df, df_unit[[unit, 'month_idx', f'trend_slope_{unit}_{window}']], on=[unit, 'month_idx'], how='left')
    except Exception as e:
        print(e)

    return df


def feature_engineer(df, calendar, division, target):
    df = df.sort_values(['material_code', division, 'year_month'])
    unit_list = ['material_code']

    for unit in unit_list:
        for lag in range(13):
            df[f'{target}_{unit}_lag_{lag}'] = df.groupby([division, 'material_code'])[target].shift(lag)
        df = sales_diff_mom_feature(df, unit)
    print('sales_lag_feature finished!')
    print('sales_lag_feature diff &amp; mon finished!')

    window_list = [2, 3, 6, 9, 12]
    for unit in unit_list:
        for window in window_list:
            df = sales_rolling_feature(df, window, unit)
    print('sales_rolling_feature finished!')

    agg_list = [['year_month', 'material_code']]
    for agg in agg_list:
        df = value_sku_agg_rate_lag_feature(df, agg, range(6))
    print('value_sku-aggregation_rate_feature finished!')

    df = time_features(df, division)
    print('time_features finished!')

    for level in ['material_code', 'sub_brand', 'category']:
        df = latest_sale_proportion_feature(df, 12, target, level)
    print(&quot;latest_sale_proportion_feature finished!&quot;)

    for unit in ['material_code', 'store']:
        for window in [3, 4, 5, 6, 9]:
            df = add_trend_feature(df, unit, window, target)
    print(&quot;trend feature finished!&quot;)

    cat_columns = ['month', 'material_code', 'brand', 'sub_brand', 'franchise', 'category', 'series', 'signature',
                   'area', 'axe', 'sub_axe', 'class', 'function_id', 'mstatus', division, 'sales_level',
                   'level3', 'level5', 'level6', 'level5_6', 'level3_5_6', 'oj1_brand', 'l2_label',
                   f'{division}_values_level', f'{division}_level', 'seasonality_flag', 'sku_type', 'Franchise', 'citycode', 'line_city', 'prvnname_ch', 'regionname_ch', 'area', 'nation']
    for column in cat_columns:
        if column in df.columns:
            df[column] = df[column].astype('category')
    print('category_feature finished!')

    for lag in range(1, 16):
        df[f'target_m_{lag}'] = df.groupby([division, 'material_code'])[target].shift(-lag).astype(np.float32)
    print('define Y finished!')

    for lag in range(1, 16):
        df[f'active_sku_filter_m_{lag}'] = df.groupby([division, 'material_code'])['filter'].shift(-lag).astype(np.float32)
        df[f'active_sku_filter_m_{lag}'] = df[f'active_sku_filter_m_{lag}'].fillna(0)
    df = df.drop('filter', axis=1)
    print('active_sku_filter finished!')

    column_list = df.columns.to_list()
    column_list.remove('brand')
    column_list.remove('category')
    df = df[column_list + ['brand', 'category']]
    
    return df


def make_feature_engineer(rows, calendar_b, division, target):
    &quot;&quot;&quot;
        groupbyKey -- category
    :param rows:
    :param calendar_b:
    :param division:
    :param target:
    :return:
    &quot;&quot;&quot;
    row_list = list()
    for row in rows:
        row_list.append(row.asDict())
    df = pd.DataFrame(row_list)
    
    # 广播变量的值
    calendar = calendar_b.value

    df = feature_engineer(df, calendar, division, target)

    dfRow = Row(*df.columns)
    row_list = []
    for r in df.values:
        row_list.append(dfRow(*r))

    return row_list


def spark_dis_com(spark, processor_data, calendar_b, division, target, repartition, parallel_column):
    #### distributed compute
    feature_data_rdd = processor_data.rdd. \
        map(lambda x: (x[parallel_column], x)). \
        groupByKey(). \
        flatMap(lambda x: make_feature_engineer(x[1], calendar_b, division, target))

    #### write table
    spark_write_to_hive(
        spark.createDataFrame(feature_data_rdd.repartition(repartition)),
        'ldlgtm_dpt.ld_feature_store_bh'
    )



if __name__ == '__main__':
    ############################# offline test #############################
    ###################### Configuring
    # ...
    # processor_data = pd.read_csv('...')
    # calendar = pd.read_csv('...')
    # feature_data = feature_engineer(processor_data, calendar, division, target)
    # feature_data.to_pickle('...')
    
    ############################# online pre/prd #############################
    ###################### Init Spark
    spark =  SparkInit(f'sf-app-gtm-art-fcsting-POS-LD')

    ###################### Configuring
    ...

    # 加载数据
    processor_data = spark.sql(f&quot;&quot;&quot; ... &quot;&quot;&quot;)
    calendar = get_calendar_data(spark)
    
    # 广播变量
    calendar_b = spark.sparkContext.broadcast(calendar.toPandas())
    
    spark_dis_com(spark, processor_data, calendar_b, division, target, 24, 'category')
    print(&quot;feature_data is successful&quot;)
    print(&quot;feature_data write table successful&quot;)
    
    spark.stop()
</code></pre>

<p>以上述代码举例说明：</p>

<ul>
<li>offline test 是在线下测试的代码，如函数 feature_engineer 即是普通的基于Pandas API 的纯Python代码；</li>
<li>online pre/prd 是线上开发和生产环境的代码，可以看到函数 make_feature_engineer 和 spark_com_dis 的代码对于 feature_engineer  稍加改动就变成了分布式计算的代码，主要有以下几点：

<ul>
<li>利用spark.sql 读取 hive表里存储的预处理好的数据 processor_data（pyspark.sql.dataframe.DataFrame），基于processor_data做特征工程计算；</li>
<li>processor_data 可以根据<strong>某个字段或某几个字段</strong>做map分组分发；</li>
<li>flatMap API 根据make_feature_engineer函数做分布式计算，并将最后结果合并；</li>
<li>make_feature_engineer 函数先将每个分组内的pyspark.sql.Row准成 Python dict，再转成 list, 继而生成一个Pandas DataFrame,然后继续使用 feature_engineer 函数计算，最后还原成由pyspark.sql.Row组成的list；</li>
<li>利用spark.createDataFrame API 创建 Spark DataFrame 写表；</li>
<li>对于其他的辅助变量 例如calendar_b，需要广播到各个节点。</li>
</ul></li>
</ul>

<p>可以看到，这里分布式计算较为灵活，可以根据<strong>某个字段或某几个字段</strong>（需要根据自己的数据需求）做map分组分发，比如这里我是根据 category 分组分发计算（我这里的数据必须根据每个category 训练模型），非常实用。</p>

<p>当然有的时候我们还可以根据下面的方式分组分发，这里我是根据每个门店 store 做分发：</p>

<ul>
<li>利用 spark.sparkContext.parallelize(store_list, 24) 生成 rdd（使用已经存在的迭代器或者集合通过调用spark驱动程序提供的parallelize函数来创建并行集合，并行集合被创建用来在分布式集群上并行计算，这里的24表示将RDD切分多少个分区）;</li>
<li>对于其他变量进行广播，每组RDD内的store数据直接使用 flatMap API计算；</li>
</ul>

<pre><code class="language-python">def map_make_post_process(model_output_b, calendar_b, odf_df_b, event_df_b, sales_df_pro_b, sku_delisting_df_b, forecast_list, end_date, M0, M1_5_list, store):
    ######## calendar &amp; odp data &amp; event data &amp; ld_month_actual_event_normal_ratio data broadcast
    model_output = model_output_b.value
    calendar = calendar_b.value
    odp_df = odf_df_b.value
    event_df = event_df_b.value
    sales_df_pro = sales_df_pro_b.value
    sku_delisting_df = sku_delisting_df_b.value

    output_formated = model_output.loc[model_output['store'] == store]
    
    ...
    
    output_formatedRow = Row(*output_formated.columns)
    row_list = []
    for r in output_formated.values:
        row_list.append(output_formatedRow(*r))

    return row_list
    
if __name__ == '__main__':
    # ......
    store_list = model_output['store'].unique()
    store_rdd = spark.sparkContext.parallelize(store_list, 24)
    output_formated_data_row_list = store_rdd.flatMap(
        lambda x: map_make_post_process(model_output_b, calendar_b, odp_df_b, event_df_b, sales_df_pro_b, sku_delisting_df_b, forecast_list, end_date, M0, M1_5_list, x)
    )
    output_formated_data_sdf = output_formated_data_row_list.toDF().repartition(24).persist()
    ......
</code></pre>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="https://7125messi.github.io/post/python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/" title="Python并行计算">
      Previous
    </a>
    

    
    <a class="next" href="https://7125messi.github.io/post/spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/" title="Spark性能调优">
      Next
    </a>
    

  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

