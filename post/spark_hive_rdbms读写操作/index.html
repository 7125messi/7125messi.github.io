<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Spark_Hive_RDBMS读写操作  &middot; 7125messi的博客</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Spark_Hive_RDBMS读写操作  &middot; 7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2020-09-10T22:10:29&#43;08:00" />
<meta property="og:article:modified_time" content="2020-09-10T22:10:29&#43;08:00" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Spark_Hive_RDBMS读写操作" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/" />
<meta name="twitter:domain" content="https://7125messi.github.io">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Spark_Hive_RDBMS读写操作",
    "author": {
      "@type": "Person",
      "name": ""
    },
    "datePublished": "2020-09-10",
    "description": "",
    "wordCount":  1016 
  }
</script>



<link rel="canonical" href="https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Spark_Hive_RDBMS读写操作
</h1>

  <div class="metas">
<time datetime="2020-09-10">10 Sep, 2020</time>


  
  &middot; Read in about 5 min
  &middot; (1016 Words)
  <br>
  


</div>

</header>

  <div class="container content">
  

<p>[项目总结提炼]</p>

<p>前面我们在做数据工程化过程中会大量用到数据的读写操作,现总结如下！！！</p>

<p>主要有以下几个文件：</p>

<pre><code>config.ini                         配置文件

func_forecast.sh                   工程执行文件

mysql-connector-java-8.0.11.jar    MySQL连接jdbc驱动

predict_pred.py                    执行主代码

utils.py                           工具项代码
</code></pre>

<h1 id="1-config-ini">1 config.ini</h1>

<p>主要定义参数值</p>

<pre><code class="language-ini">[spark]
executor_memory = 2g
driver_memory = 2g
sql_execution_arrow_enabled = true
executor_cores = 2
executor_instances = 2
jar = /root/spark-rdbms/mysql-connector-java-8.0.11.jar

[mysql]
host = 11.23.32.16
port = 3306
db = test
user = test
password = 123456

[postgresql]
host = 11.23.32.16
port = 3306
db = test
user = test
password = 123456

[dbms_parameters]
mysql_conn_info_passwd @@@ =4bI=prodha
mysql_conn_info_database = test_db
</code></pre>

<h1 id="2-utils-py">2 utils.py</h1>

<p>主要定义各个函数</p>

<pre><code class="language-python">from configparser import ConfigParser
from pyspark import SparkConf
import psycopg2
import pymysql

# pip install pymysql
# pip install psycopg2-binary


def get_config():
    &quot;&quot;&quot;
    获取整个配置文件信息
    
    Returns
    -------
        cf: ConfigParser
            配置文件信息
    &quot;&quot;&quot;
    cf = ConfigParser()
    cf.read('config.ini', encoding='utf-8')
    return cf

def get_user_pwd():
    &quot;&quot;&quot;
    获取用户、密码
    
    Returns
    -------
        strUser: Str
            连接用户
        strPassword: Str
            连接密码
    &quot;&quot;&quot;
    cf = get_config()
    
    strUser = cf.get('mysql', 'user')
    strPassword = cf.get('mysql', 'password')
    
    return strUser, strPassword

def get_conn_url():
    &quot;&quot;&quot;
    通过jdbc方式获取连接URL
    
    Returns
    -------
        strUrl: Str
            连接URL
    &quot;&quot;&quot;
    cf = get_config()
    
    host = cf.get('mysql', 'host')
    port = cf.get('mysql', 'port')
    db = cf.get('mysql', 'db')
    strUrl = f'mysql://{host}:{port}/{db}?useSSL=false'
    return strUrl

def get_conn_properties():
    &quot;&quot;&quot;
    获取连接用户名+密码字典
    
    Returns
    -------
        strProperties: Dict
            用户、密码组成的字典
    &quot;&quot;&quot;
    cf = get_config()
    
    user = cf.get('mysql', 'user')
    password = cf.get('mysql', 'password')
    strProperties = {'user': user, 'password': password}
    
    return strProperties

def get_spark_conf():
    &quot;&quot;&quot;
    获取SparkConf
    
    Returns
    -------
        sparkConf: pyspark.sql.SparkConf
            spark配置参数
    &quot;&quot;&quot;
    cf = get_config()
    sparkConf = SparkConf()
    sparkConf.set(&quot;spark.executor.memoryOverhead&quot;, &quot;1g&quot;)
    sparkConf.set(&quot;spark.driver.maxResultSize&quot;, &quot;4g&quot;)
    sparkConf.set('spark.some.config.option', 'some-value')
    sparkConf.set('spark.executor.memory', cf.get('spark', 'executor_memory'))
    sparkConf.set('spark.driver.memory', cf.get('spark', 'driver_memory'))
    sparkConf.set('spark.executor.instances', cf.get('spark', 'executor_instances'))
    sparkConf.set('spark.executor.cores', cf.get('spark', 'executor_cores'))
    sparkConf.set('spark.sql.execution.arrow.enabled', cf.get('spark', 'sql_execution_arrow_enabled'))
    
    return sparkConf

def read_dataset(spark, strURL, strDBTable, strUser, strPassword, isConcurrent,
                 partitionColumn='', lowerBound='', upperBound='', numPartitions=1):
    &quot;&quot;&quot;
    Spark读取数据 jdbc
    
    Parameters
    ----------
        spark: pyspark.sql.SparkSession
            SparkSession
        strURL: Str
            连接URL
        strDBTable: Str
            表名
        strUser: Str
            用户
        strPassword: Str
            密码
        isConcurrent: Bool
            是否并发读取（默认并发为1）
        partitionColumn: Str
            Must be a numeric, date, or timestamp column from the table.
        lowerBound: Str
            Used to decide the partition stride.
        upperBound: Str
            Used to decide the partition stride.
        numPartitions: Str
            The maximum number of partitions that can be used for parallelism
    Returns
    -------
        pyspark.sql.DataFrame
    &quot;&quot;&quot;
    if isConcurrent:
        jdbcDf=spark.read.format(&quot;jdbc&quot;) \
                         .option('url', f&quot;jdbc:{strURL}&quot;) \
                         .option('dbtable', strDBTable) \
                         .option('user', strUser) \
                         .option('password', strPassword) \
                         .option('partitionColumn', partitionColumn) \
                         .option('lowerBound', lowerBound) \
                         .option('upperBound', upperBound) \
                         .option('numPartitions', numPartitions) \
                         .load()
        return jdbcDf
    else:
        jdbcDf=spark.read.format(&quot;jdbc&quot;) \
                         .option('url', f&quot;jdbc:{strURL}&quot;) \
                         .option('dbtable', strDBTable) \
                         .option('user', strUser) \
                         .option('password', strPassword) \
                         .load()
        return jdbcDf

def write_dataset(srcDF, strURL, desDBTable, dictProperties, mode=&quot;append&quot;):
    &quot;&quot;&quot;
    Spark写入数据 jdbc
    
    Parameters
    ----------
        srcDF: pyspark.sql.DataFrame
            待写入源DataFrame
        strURL: Str
            连接URL
        desDBTable: Str
            写入数据库的表名
        dictProperties: Dict
            用户密码组成的字典
        mode: Str
            插入模式（&quot;append&quot;: 增量更新； &quot;overwrite&quot;: 全量更新）

    &quot;&quot;&quot;
    try:
        srcDF.write.jdbc(
            f'jdbc:{strURL}', 
            table=desDBTable, 
            mode=mode, 
            properties=dictProperties
        )
    except BaseException as e:
        print(repr(e))

def psycopg_execute(strSql):
    &quot;&quot;&quot;
    使用psycopg2方式执行sql
    
    Parameters
    ----------
        strSql: Str
            待执行的sql
    &quot;&quot;&quot;
    cf = get_config()
    
    # 连接数据库
    conn = psycopg2.connect(
        dbname=cf.get('postgresql', 'db'),
        user=cf.get('postgresql', 'user'),
        password=cf.get('postgresql', 'password'), 
        host=cf.get('postgresql', 'host'), 
        port=cf.get('postgresql', 'port')
    )

    # 创建cursor以访问数据库
    cur = conn.cursor()

    try:
        # 执行操作
        cur.execute(strSql)
        # 提交事务
        conn.commit()
    except psycopg2.Error as e:
        print('DML操作异常')
        print(e)
        # 有异常回滚事务
        conn.rollback()
    finally:
        # 关闭连接
        cur.close()
        conn.close()

def get_sql_conn():
    &quot;&quot;&quot;
    连接sql数据库的engine：可用于使用数据库连接
    
    Parameters
    ----------
        conn: Str
            数据库的engine
    &quot;&quot;&quot;
    cf = get_config()
    conn = pymysql.connect(
        host=cf.get('mysql', 'host'), 
        port=int(cf.get('mysql', 'port')),
        user=cf.get('mysql', 'user'),
        password=cf.get('mysql', 'password'),
        database=cf.get('mysql', 'db'),
        charset='utf8'   # charset=&quot;utf8&quot;，编码不要写成&quot;utf-8&quot;
    )
    return conn

def mysql_query_data(strSql):
    &quot;&quot;&quot;
    使用mysql查询
    
    Parameters
    ----------
        strSql: Str
            待执行的sql

    Returns
    -------
        list[dict]
    &quot;&quot;&quot;    
    # 连接数据库
    conn = get_sql_conn()

    # 创建cursor以访问数据库
    cur = conn.cursor(pymysql.cursors.DictCursor)

    try:
        # 执行操作
        cur.execute(strSql)
        return cur.fetchall()
    finally:
        # 关闭连接
        cur.close()
        conn.close()

def mysql_insert_or_update_data(strSql):
    &quot;&quot;&quot;
    使用mysql执行insert或update操作
    
    Parameters
    ----------
        strSql: Str
            待执行的sql
    &quot;&quot;&quot;    
    # 连接数据库
    conn = get_sql_conn()

    # 创建cursor以访问数据库
    cur = conn.cursor()

    try:
        # 执行操作
        cur.execute(strSql)
        # 提交事务
        conn.commit()
    except Exception:
        print('DML操作异常')
        # 有异常回滚事务
        conn.rollback()
    finally:
        # 关闭连接
        cur.close()
        conn.close()
</code></pre>

<h1 id="3-测试代码-predict-pred-py">3 测试代码 predict_pred.py</h1>

<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql import functions as fn
from pyspark.ml import Pipeline, util
from pyspark.ml import feature as ft

import pandas as pd
import time

import sys
print(sys.path)
sys.path.append('/root/spark-rdbms') #把模块目录加到sys.path列表中
print(sys.path)

from utils import get_config
from utils import get_user_pwd
from utils import get_conn_url
from utils import get_conn_properties
from utils import get_spark_conf
from utils import read_dataset
from utils import write_dataset
from utils import get_sql_conn
from utils import mysql_query_data
from utils import mysql_insert_or_update_data


if __name__ == '__main__':
    # (1) spark读取hive
    # 启动spark
    spark = SparkSession.\
        builder.\
        appName(&quot;spark_io&quot;).\
        config(conf=get_spark_conf()).\
        enableHiveSupport().\
        getOrCreate()
    
    select_dt = &quot;2020-09-09&quot;
    strSql = f&quot;&quot;&quot;
        select * from yth_src.kclb where dt = '{select_dt}'
    &quot;&quot;&quot;
    df = spark.sql(strSql)
    print(df.show())


    # (2) spark写入hive
    # dt = time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()) 
    # print(dt)
    # # 打开动态分区
    # spark.sql(&quot;set hive.exec.dynamic.partition.mode=nonstrict&quot;)
    # spark.sql(&quot;set hive.exec.dynamic.partition=true&quot;)
    # spark.sql(f&quot;&quot;&quot;
    # insert overwrite table yth_dw.shop_inv_turnover_rate partition (dt)
    # select 
    #     shop_code, 
    #     pro_code,

    #     current_timestamp as created_time, 
    #     dt = '{dt}'
    # from shop_inv_turnover_rate_table_db
    # &quot;&quot;&quot;)

    # (2) spark读取MySQL
    # 获取用户密码
    strUser, strPwd = get_user_pwd()
    jdbcDf = read_dataset(
        spark = spark, 
        strURL = get_conn_url(), 
        strDBTable = 'hive_kclb', 
        strUser = strUser, 
        strPassword = strPwd,
        isConcurrent = False
    )
    print(jdbcDf.show())

    # (2) spark写入mysql
    # write_dataset(
    #     srcDF = jdbcDf,
    #     strURL = get_conn_url(),
    #     table='hive_kclb2', 
    #     mode='append', 
    #     properties=get_conn_properties()
    # )

    # (3) pymysql读取MySQL
    strSql2 = f&quot;&quot;&quot;
        select * from hive_kclb limit 5
    &quot;&quot;&quot;
    df2 = mysql_query_data(strSql2)   # 返回 list[dict]
    df2 = pd.DataFrame(df2)
    print(df2)

    # (3) pymysql读取MySQL————pandas读取
    engine = get_sql_conn()
    df3 = pd.read_sql(strSql2,con = engine)
    print(df3)
</code></pre>

<h1 id="4-执行代码-func-forecast-sh">4 执行代码 func_forecast.sh</h1>

<pre><code class="language-shell"># !/bin/bash

# spark解释器
spark_interpreter=&quot;/opt/spark-2.4.4/bin/spark-submit&quot;

# python解释器
python_interpreter=&quot;/app/anaconda3/bin/python&quot;

function func_forecast()
{
    cd /root/spark-rdbms

    # (1) 
    # --packages  jar包的maven地址
    # --packages  mysql:mysql-connector-java:5.1.27 --repositories http://maven.aliyun.com/nexus/content/groups/public/
    #                                               --repositories 为mysql-connector-java包的maven地址，若不给定，则会使用该机器安装的maven默认源中下载
    # 若依赖多个包，则重复上述jar包写法，中间以逗号分隔
    # 默认下载的包位于当前用户根目录下的.ivy/jars文件夹中
    # 应用场景：本地可以没有，集群中服务需要该包的的时候，都是从给定的maven地址，直接下载

    # (2)
    # --jars JARS
    # 应用场景：要求本地必须要有对应的jar文件
    # --driver-class-path 作用于driver的额外类路径，使用–jar时会自动添加路径,多个包之间用冒号(:)分割

    # 执行程序
    ${spark_interpreter} --master yarn \
                        --conf spark.pyspark.python=${python_interpreter} \
                        --driver-class-path mysql-connector-java-8.0.11.jar \
                        --jars mysql-connector-java-8.0.11.jar predict_pred.py

    if [[ $? -ne 0 ]]; then
        echo &quot;--&gt; 执行失败&quot;
    exit 1
    fi
}
func_forecast
</code></pre>

<hr />

<hr />

<h1 id="5-shell中配置文件的读取">5 Shell中配置文件的读取</h1>

<p>编写统一的读取文件<code>ini_pro.sh</code></p>

<pre><code class="language-shell">###
 # @Author: ydzhao
 # @Description: read configuration file's parameters
 # @Date: 2020-07-27 13:10:31
 # @LastEditTime: 2020-09-13 07:33:14
 # @FilePath: /git/code/ydzhao/spark-hive-rdbms/conf/ini.sh
### 
#!/usr/bin/bash

INIFILE=$1
SECTION=$2
ITEM=$3
SPLIT=$4
NEWVAL=$5


function ReadINIfile(){ 
  ReadINI=`awk -F $SPLIT '/\['$SECTION'\]/{a=1}a==1&amp;&amp;$1~/'$ITEM'/{print $2;exit}' $INIFILE`
  echo $ReadINI
}


function WriteINIfile(){
   WriteINI=`sed -i &quot;/^\[$SECTION\]/,/^\[/ {/^\[$SECTION\]/b;/^\[/b;s/^$ITEM*=.*/$ITEM=$NEWVAL/g;}&quot; $INIFILE`
  echo $WriteINI
}

if [ &quot;$5&quot; = &quot;&quot; ] ;then 
   ReadINIfile $1 $2 $3 $4
else
   WriteINIfile $1 $2 $3 $4 $5
fi

# ./ini.sh $1 $2 $3 $4       读取ini
# ./ini.sh $1 $2 $3 $4 $5    写入ini &quot;newval&quot;
</code></pre>

<p>我们读取上面的<code>config.ini</code>文件</p>

<p>可以这样写</p>

<pre><code class="language-shell">[root@ythbdnode01 ~]# mysql_conn_info_user=`bash /root/ini_pro.sh /root/config.ini dbms_parameters mysql_conn_info_user =`
[root@ythbdnode01 ~]# echo $mysql_conn_info_user
test_db
</code></pre>

<p>这里传入4个变量分别是：</p>

<ul>
<li>INIFILE  配置文件名</li>
<li>SECTION  某个区块</li>
<li>ITEM     某个区块下的明细</li>
<li>SPLIT    分隔符</li>
</ul>

<pre><code class="language-shell">[root@ythbdnode01 ~]# mysql_conn_info_passwd=`bash /root/ini_pro.sh /root/config.ini dbms_parameters mysql_conn_info_passwd @@@`
[root@ythbdnode01 ~]# echo $mysql_conn_info_passwd
=4bI=prodha
</code></pre>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/" title="数据工程化案例介绍">
      Previous
    </a>
    

    

  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

