<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 7125messi的博客</title>
    <link>https://7125messi.github.io/post/</link>
    <description>Recent content in Posts on 7125messi的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>(c) 2018 7125messi.</copyright>
    <lastBuildDate>Mon, 03 Aug 2020 21:31:10 +0800</lastBuildDate>
    
	<atom:link href="https://7125messi.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>数据工程化案例介绍</title>
      <link>https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 03 Aug 2020 21:31:10 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/</guid>
      <description>数据工程化案例介绍 好久没写博客了😃😃😃😃😃😃，最近做完了一个偏数据工程的项目，系统的使用了大数据相关组件，学习了Hadoop生态圈技术以及数据仓库相关知识。下面将一些体会写下。。。

1 项目背景和业务特点 
XXX医药业务场景：以终端消费者为中心的服务，以门店、连锁加盟、批发模式触达，当前核心竞争力是品牌加盟和供应链采购能力。随着加盟业务快速成长，致力于成为中国最大的零售药房加盟商，需要配套成长的供应链物流能力和信息化建设。“高库存、高退货、高效期” 等环节精细化运营的薄弱是主要原因，具体表现在以下几点：
 (1) 门店补货靠经验，造成了“高退货”; (2) 加盟店和批发商等物流能力尚未触达、物流信息尚未线上化; (3) 与供应商信息沟通均为线下,补货频次较为传统; (4) 采购计划依赖采购员个人经验采用公式计算，未考虑复杂因素;  
项目目标是构建以智能补货为智慧大脑的需求驱动的全局、动态、平衡的数字化供应链运营体系，提供安全、高效、精准的供应链能力。
主要包括以下部分：
 (1) 数据清洗 (2) 特种工程 (3) 模型训练 (4) 模型融合 (5) 数据工程化  
其中前4个部分是机器学习的常见方法和步骤,考虑到线上生产环境要能正常执行,第5部分数据工程化部分启动非常重要的地位,下面对这个部分进行详细的叙述。

2 数据工程化流程架构 
这里我们的数据源主要Oracle业务数据以及一些客户提供的人工数据,利用sqoop每日凌晨00:40定时同步数据至Hadoop集群src层。按照经典的Kappa数据仓库分层架构分为:src-&amp;gt;ods-&amp;gt;dw1-&amp;gt;dw2-&amp;gt;app.
与传统的数仓建模不同的是我们主要的目的是利用机器学习方法进行预测补货,数据仓库ods/dw1都是数据清洗和复杂业务逻辑处理,dw2是特征工程处理生成宽表用于训练模型。在数据清洗的过程中会有一些指标用于KPI展示以及app模型预测补货结果我们会同步至MySQL,这些都是作为数据应用服务。
整个数据工程基于Hadoop生态圈技术为载体,数据存储主要是HDFS,数据计算主要是Hive/Spark,元数据管理是Apache Atlas,数据质量分析用的是Apache Griffin,数据任务流调度系统用的是Azkaban,数据OLAP数据库是Presto,数据分析可视化Dashboard用的是Superset。这些大数据组件采用Cloudera Manager(CM)进行统一安装配置,为了保证服务的高可用(HA)采用Zookeeper进行资源调度和管理。

3 数据工程生产环境部署 

3.1 可配置项 
配置项对于数据工程的重要性不言而喻,可灵活调整部署代码,方便控制管理
├─conf │ │ config.ini 所有可配置参数 │ │ env_name.conf 生产环境和测试环境的标识符 │ │ ini.sh 读写配置文件的函数</description>
    </item>
    
    <item>
      <title>SVM调优详解</title>
      <link>https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 02 Aug 2019 21:21:35 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</guid>
      <description>在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲，RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们就需要使用RBF，在享受RBF对非线性数据的良好分类效果前，我们需要对主要的超参数进行选取。本文我们就对scikit-learn中 SVM RBF的调参做一个小结。

1 SVM RBF 主要超参数概述 如果是SVM分类模型，这两个超参数分别是惩罚系数和RBF核函数的系数。当然如果是nu-SVC的话，惩罚系数C代替为分类错误率上限nu, 由于惩罚系数C和分类错误率上限nu起的作用等价，因此本文只讨论带惩罚系数C的分类SVM**

1.1 SVM分类模型 ###（1） 惩罚系数
 惩罚系数C即上一篇里讲到的松弛变量ξ的系数。它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数。
 当惩罚系数C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合。
 当惩罚系数C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单。scikit-learn中默认值是1。
  
（2）RBF核函数的系数 另一个超参数是RBF核函数的参数。回忆下RBF 核函数
γ主要定义了单个样本对整个分类超平面的影响。
 当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量
 当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量**，或者说整个模型的支持向量也会多。scikit-learn中默认值是1/n_features**
  
（3）惩罚系数和RBF核函数的系数 如果把惩罚系数和RBF核函数的系数一起看：
 当C比较大、 γ比较大时，会有更多的支持向量，模型会比较复杂，较容易过拟合 当C比较小、γ比较小时，模型会变得简单，支持向量的个数会少  
1.2 SVM回归模型 SVM回归模型的RBF核比分类模型要复杂一点，因为此时除了惩罚系数C和RBF核函数的系数γ之外，还多了一个损失距离度量ϵ。如果是nu-SVR的话，损失距离度量ϵ代替为分类错误率上限nu，由于损失距离度量ϵ和分类错误率上限nu起的作用等价，因此本文只讨论带距离度量ϵ的回归SVM。
 对于惩罚系数C和RBF核函数的系数γ，回归模型和分类模型的作用基本相同。
 对于损失距离度量ϵ，它决定了样本点到超平面的距离损失.当ϵ比较大时，损失较小，更多的点在损失距离范围之内，模型较简单;当ϵ比较小时，损失函数会较大，模型也会变得复杂；scikit-learn中默认值是0.1
  惩罚系数C、RBF核函数的系数γ和损失距离度量ϵ一起看
 当C比较大、 γ比较大、ϵ比较小时，会有更多的支持向量，模型会比较复杂，容易过拟合一些;
 当C比较小、γ比较小、ϵ比较大时**，模型会变得简单，支持向量的个数会少
  
2 SVM RBF 主要调参方法 对于SVM的RBF核，主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类。
当然也可以使用cross_val_score类来调参，但是个人觉得没有GridSearchCV方便。本文只讨论用GridSearchCV**来进行SVM的RBF核的调参。
将GridSearchCV类用于SVM RBF调参时要注意的参数有：</description>
    </item>
    
    <item>
      <title>RandomForest调优详解</title>
      <link>https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 02 Aug 2019 21:14:54 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</guid>
      <description>原文来自：http://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/

为什么要调整机器学习算法？ 一个月以前，我在kaggle上参加了一个名为TFI的比赛。 我第一次提交的结果在50%。 我不懈努力在特征工程上花了超过2周的时间，勉强达到20%。 出乎我意料的事是，在调整机器学习算法参数之后，我能够达到前10%。
这就是机器学习算法参数调优的重要性。 随机森林是在工业界中使用的最简单的机器学习工具之一。 在我们以前的文章中，我们已经向您介绍了随机森林和和CART模型进行了对比 。 机器学习工具包正由于这些算法的表现而被人所熟知。
随机森林是什么？ 随机森林是一个集成工具，它使用观测数据的子集（BootStraping）和特征变量的子集（随机选择特征变量）来建立一个决策树。 它建立多个这样的决策树，然后将他们合并在一起以获得更准确和稳定的预测。 这样做最直接的事实是，在这一组独立的预测结果中，用投票方式得到一个最高投票结果，这个比单独使用最好模型预测的结果要好。
我们通常将随机森林作为一个黑盒子，输入数据然后给出了预测结果，无需担心模型是如何计算的。这个黑盒子本身有几个我们可以摆弄的杠杆。 每个杠杆都能在一定程度上影响模型的性能或资源时间平衡。 在这篇文章中，我们将更多地讨论我们可以调整的杠杆，同时建立一个随机森林模型。
调整随机森林的参数杠杆 随机森林的参数即可以增加模型的预测能力，又可以使训练模型更加容易。 以下我们将更详细地谈论各个参数（请注意，这些参数，我使用的是Python常规的命名法）：
1.使模型预测更好的特征 主要有3类特征可以被调整，以改善该模型的预测能力

A. max_features： 随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：
 Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。
 sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。
 0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。
   If &amp;ldquo;auto&amp;rdquo;, then max_features=sqrt(n_features).
 If &amp;ldquo;sqrt&amp;rdquo;, then max_features=sqrt(n_features) (same as &amp;ldquo;auto&amp;rdquo;).
 If &amp;ldquo;log2&amp;rdquo;, then max_features=log2(n_features).
 If None, then max_features=n_features.
  max_features如何影响性能和速度？
增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它 同时也降低了单个树的多样性 ，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。</description>
    </item>
    
    <item>
      <title>GBDT调优详解</title>
      <link>https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 02 Aug 2019 21:08:50 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</guid>
      <description>原文地址：Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python by Aarshay Jain
1.前言 如果一直以来你只把GBM当作黑匣子，只知调用却不明就里，是时候来打开这个黑匣子一探究竟了！
不像bagging算法只能改善模型高方差（high variance）情况，Boosting算法对同时控制偏差（bias）和方差（variance）都有非常好的效果，而且更加高效。
如果你需要同时处理模型中的方差和偏差，认真理解这篇文章一定会对你大有帮助，
本文会用Python阐明GBM算法，更重要的是会介绍如何对GBM调参，而恰当的参数往往能令结果大不相同。

2.目录  Boosing是怎么工作的？
 理解GBM模型中的参数
 学会调参（附详例）
  
3.Boosting是如何工作的？ Boosting可以将一系列弱学习因子（weak learners）相结合来提升总体模型的预测准确度。在任意时间t，根据t-1时刻得到的结果我们给当前结果赋予一个权重。之前正确预测的结果获得较小权重，错误分类的结果得到较大权重。回归问题的处理方法也是相似的。
让我们用图像帮助理解：
 图一： 第一个弱学习因子的预测结果（从左至右）
 一开始所有的点具有相同的权重（以点的尺寸表示）。
 分类线正确地分类了两个正极和五个负极的点。
  图二： 第二个弱学习因子的预测结果
 在图一中被正确预测的点有较小的权重（尺寸较小），而被预测错误的点则有较大的权重。
 这时候模型就会更加注重具有大权重的点的预测结果，即上一轮分类错误的点，现在这些点被正确归类了，但其他点中的一些点却归类错误。
   对图3的输出结果的理解也是类似的。这个算法一直如此持续进行直到所有的学习模型根据它们的预测结果都被赋予了一个权重，这样我们就得到了一个总体上更为准确的预测模型。
现在你是否对Boosting更感兴趣了？不妨看看下面这些文章（主要讨论GBM）：
 Learn Gradient Boosting Algorithm for better predictions (with codes in R)
 Quick Introduction to Boosting Algorithms in Machine Learning</description>
    </item>
    
    <item>
      <title>XGBoost调优指南</title>
      <link>https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 02 Aug 2019 21:05:56 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/</guid>
      <description>原文地址：Complete Guide to Parameter Tuning in XGBoost by Aarshay Jain

1. 简介 如果你的预测模型表现得有些不尽如人意，那就用XGBoost吧。XGBoost算法现在已经成为很多数据工程师的重要武器。它是一种十分精致的算法，可以处理各种不规则的数据。
构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难(至少我觉得十分纠结)。这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。在解决实际问题的时候，有些问题是很难回答的——你需要调整哪些参数？这些参数要调到什么值，才能达到理想的输出？
这篇文章最适合刚刚接触XGBoost的人阅读。在这篇文章中，我们会学到参数调优的技巧，以及XGboost相关的一些有用的知识。以及，我们会用Python在一个数据集上实践一下这个算法。

2. 你需要知道的 XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。在前文章中，基于Python的Gradient Boosting算法参数调整完全指南，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。

3. 内容列表 1、XGBoost的优势 2、理解XGBoost的参数 3、调参示例

4. XGBoost的优势 XGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势：

4.1 正则化  标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。
 实际上，XGBoost以正则化提升(regularized boosting)技术而闻名。  
4.2 并行处理  XGBoost可以实现并行处理，相比GBM有了速度的飞跃。 不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢?每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。(构造决策树的结构时，样本分割点位置，可以使用并行计算)
 XGBoost 也支持Hadoop实现。  
4.3 高度的灵活性  XGBoost 允许用户定义自定义优化目标函数和评价标准 它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。  
4.4 缺失值处理  XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</description>
    </item>
    
    <item>
      <title>LightGBM学习</title>
      <link>https://7125messi.github.io/post/lightgbm%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 02 Aug 2019 20:57:29 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/lightgbm%E5%AD%A6%E4%B9%A0/</guid>
      <description>[参考整理]
1 LightGBM原理
 
1.1 GBDT和 LightGBM对比 GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT 也是各种数据挖掘竞赛的致命武器，据统计 Kaggle 上的比赛有一半以上的冠军方案都是基于 GBDT。
LightGBM（Light Gradient Boosting Machine）同样是一款基于决策树算法的分布式梯度提升框架。为了满足工业界缩短模型计算时间的需求，LightGBM的设计思路主要是两点：
 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据； 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。  由此可见，LightGBM的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具。
XGBoost和LightGBM都是基于决策树提升(Tree Boosting)的工具，都拥有对输入要求不敏感、计算复杂度不高和效果好的特点，适合在工业界中进行大量的应用。
主页地址：http://lightgbm.apachecn.org
LightGBM （Light Gradient Boosting Machine）是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有以下优点：
 更快的训练速度 更低的内存消耗 更好的准确率 分布式支持，可以快速处理海量数据  如下图，在 Higgs 数据集上 LightGBM 比 XGBoost 快将近 10 倍，内存占用率大约为 XGBoost 的1/6，并且准确率也有提升。

1.2 LightGBM 的动机 常用的机器学习算法，例如神经网络等算法，都可以以 mini-batch 的方式训练，训练数据的大小不会受到内存限制。
而 GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的。
 LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。</description>
    </item>
    
    <item>
      <title>特征工程———特征选择的原理和实现</title>
      <link>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Thu, 01 Aug 2019 06:58:37 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</guid>
      <description>[参考总结提炼]
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。
那特征工程是什么？
 特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。
 特征工程又包含了Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等子问题，本文主要讨论特征选择相关的方法及实现。在实际项目中，我们可能会有大量的特征可使用，有的特征携带的信息丰富，有的特征携带的信息有重叠，有的特征则属于无关特征，如果所有特征不经筛选地全部作为训练特征，经常会出现维度灾难问题，甚至会降低模型的准确性。因此，我们需要进行特征筛选，排除无效/冗余的特征，把有用的特征挑选出来作为模型的训练数据。
01 特征选择介绍 1.特征按重要性分类  相关特征：
对于学习任务（例如分类问题）有帮助，可以提升学习算法的效果；
 无关特征：
对于我们的算法没有任何帮助，不会给算法的效果带来任何提升；
 冗余特征：
不会对我们的算法带来新的信息，或者这种特征的信息可以由其他的特征推断出；
  2.特征选择的目的 对于一个特定的学习算法来说，哪一个特征是有效的是未知的。因此，需要从所有特征中选择出对于学习算法有益的相关特征。而且在实际应用中，经常会出现维度灾难问题。如果只选择所有特征中的部分特征构建模型，那么可以大大减少学习算法的运行时间，也可以增加模型的可解释性。
3.特征选择的原则 获取尽可能小的特征子集，不显著降低分类精度、不影响分类分布以及特征子集应具有稳定、适应性强等特点。 
02 特征选择的方法 
1.Filter方法（过滤式） 先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。
主要思想：对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
主要方法：
 Chi-squared test（卡方检验）
 Information gain（信息增益）
 Correlation coefficient scores（相关系数）
  优点：运行速度快，是一种非常流行的特征选择方法。
缺点：无法提供反馈，特征选择的标准规范的制定是在特征搜索算法中完成，学习算法无法向特征搜索算法传递对特征的需求。另外，可能处理某个特征时由于任意原因表示该特征不重要，但是该特征与其他特征结合起来则可能变得很重要。
2.Wrapper方法（封装式） 直接把最后要使用的分类器作为特征选择的评价函数，对于特定的分类器选择最优的特征子集。
主要思想：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA、PSO（如：优化算法-粒子群算法）、DE、ABC（如：优化算法-人工蜂群算法）等。
主要方法：递归特征消除算法。
优点：对特征进行搜索时围绕学习算法展开的，对特征选择的标准规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，真正关注的是学习问题本身。由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。
缺点：运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
3.Embedded方法（嵌入式） 将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。
主要思想：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。
主要方法：用带有L1正则化的项完成特征选择（也可以结合L2惩罚项来优化）、随机森林平均不纯度减少法/平均精确度减少法。
优点：对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于Wrapper方法，比较节省时间。
缺点：运行速度慢。
03 特征选择实现方法一：去掉取值变化小的特征 （Removing features with low variance） 该方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。 考察某个特征下，样本的方差值，可以认为给定一个阈值，抛弃哪些小于某个阈值的特征。</description>
    </item>
    
    <item>
      <title>半参数回归模型</title>
      <link>https://7125messi.github.io/post/%E5%8D%8A%E5%8F%82%E6%95%B0%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 24 Jul 2019 23:42:37 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E5%8D%8A%E5%8F%82%E6%95%B0%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</guid>
      <description>[原创]
常见的处理非线性关系的方法有数据转换方法和神经转换、SVM、投影寻踪和基于树的方法等高计算强度方法。实际在应用回归问题有很大的局限性，模型的可解释性差。使用非参数和半参数回归方法来处理非线性关系一定程度可避免这些问题。
从数据本身出发来估计适合数据本身的函数形式。
 1.忽略非线性的后果  对模型所有的连续自变量都进行检验来判断是否具有非线性作用（xy），通过数据变换解决非线性问题。
 2.数据变换  幂转换：仅能够对正数取值的变量才能使用。所以才使用非参数和半参数回归方法。
 3.非参数和半参数回归方法  从数据本身出发来估计适合数据本身的函数形式。用局部估计取代全局估计。 非参数回归的局部估计是通过数据估计两个变量之间的函数形式，而全局估计通过假设来对函数形式作出规定。 半参数回归模型：利用多元模型把全局估计和局部估计结合起来。 半参数回归模型：广义可加模型（GAM）（特殊：加性模型）。 GAM指自变量为离散或连续变量的半参数回归模型，可对自变量做非参数估计，对一些自变量采取标准的方式估计。 GAM中怀疑具有非线性函数形式的连续自变量可以用非参数估计，而模型中其他变量仍以参数形式估计。 GAM依赖非参数回归，X与Y之间的全局拟合被局部拟合取代，放弃了全部拟合的假设，仍保留了加性的假设。 GAM的加性假设使得模型比神经网络、支持向量机等更容易解释，比完全的参数模型更灵活。 GAM可以适用于许多类型的因变量：连续的、计数的、二分类的，定序的和时间等等。 GAM提供了诊断非线性的框架，简单线性模型和幂转换模型是嵌套在GAM中。 GAM的局部估计可以使用F检验或似然比检验来检验线性，二次项或任何其他幂转换模型的拟合效果。如果半参数回归模型优于线性模型或幂转换模型，它就应该被采用。 半参数回归模型的检验非线性和模型比较作用给予了半参数回归模型的强大功能，对于任意的连续自变量，都应采用半参数方法进行诊断或建模。   1 非参数估计 参数回归与非参数回归的优缺点比较
&amp;gt; 参数模型 &amp;gt; &amp;gt; 优点： &amp;gt; (1)模型形式简单明确，仅由一些参数表达 &amp;gt; (2)在经济中，模型的参数具有一般都具有明确的经济含义 &amp;gt; (3)当模型参数假设成立，统计推断的精度较高，能经受实际检验 &amp;gt; (4)模型能够进行外推运算 &amp;gt; (5)模型可以用于小样本的统计推断 &amp;gt; &amp;gt; 缺点： &amp;gt; (1)回归函数的形式预先假定 &amp;gt; (2)模型限制较多：一般要求样本满足某种分布要求，随机误差满足正态假设，解释变量间独立，解释变量与随机误差不相关等 &amp;gt; (3)需要对模型的参数进行严格的检验推断，步骤较多 &amp;gt; (4)模型泛化能力弱，缺乏稳健性，当模型假设不成立，拟合效果不好，需要修正或者甚至更换模型 &amp;gt; &amp;gt; 非参数模型 &amp;gt; &amp;gt; 优点： &amp;gt; &amp;gt; (1)**回归函数形式自由，受约束少，对数据的分布一般不做任何要求** &amp;gt; (2)**适应能力强，稳健性高，回归模型完全由数据驱动** &amp;gt; (3)**模型的精度高 ;** &amp;gt; (4)**对于非线性、非齐次问题，有非常好的效果**  使用非参数回归时，利用数据来估计F的函数形式。 事先的线性假设被更弱的假设光滑总体函数所代替。这个更弱的假设的代价是两方面的： * 第一，计算方面的代价是巨大的，但考虑到现代计算技术的速度，这不再是大问题； * 第二，失去了一些可解释性，但同时也得到了一个更具代表性的估计。</description>
    </item>
    
    <item>
      <title>Seq2Seq模型</title>
      <link>https://7125messi.github.io/post/seq2seq%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sat, 20 Jul 2019 08:48:44 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/seq2seq%E6%A8%A1%E5%9E%8B/</guid>
      <description>[参考整理]
前 言 Seq2Seq，全称Sequence to Sequence。它是一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中。Seq2Seq并不是GNMT（Google Neural Machine Translation）系统的官方开源实现。框架的目的是去完成更广泛的任务，而神经机器翻译只是其中之一。在循环神经网络中我们了解到如何将一个序列转化成定长输出。在本文中，我们将探究如何将一个序列转化成一个不定长的序列输出（如机器翻译中，源语言和目标语言的句子往往并没有相同的长度）。

1 简单入门 
设计目标  通用 这个框架最初是为了机器翻译构建的，但是后来使用它完成了各种其他任务，包括文本摘要、会话建模和图像字幕。只要我们的任务，可以将输入数据以一种格式编码并将其以另一种格式解码，我们就可以使用或者扩展这个框架。
 可用性 支持多种类型的输入数据，包括标准的原始文本。
 重现性 用YAML文件来配置我们的pipelines和models，容易复现。
 可扩展性 代码以模块化的方式构建，添加一种新的attention机制或编码器体系结构只需要最小的代码更改。
 文档化:所有的代码都使用标准的Python文档字符串来记录，并且编写了使用指南来帮助我们着手执行常见的任务。
 良好的性能:为了代码的简单性，开发团队并没有试图去尽力压榨每一处可能被拓展的性能，但是对于几乎所有的生产和研究项目，当前的实现已经足够快了。此外，tf-seq2seq还支持分布式训练。
  
主要概念 Configuration 许多objects都是使用键值对来配置的。这些参数通常以YAML的形式通过配置文件传递，或者直接通过命令行传递。配置通常是嵌套的，如下例所示:
model_params: attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau attention.params: num_units: 512 embedding.dim: 1024 encoder.class: seq2seq.encoders.BidirectionalRNNEncoder encoder.params: rnn_cell: cell_class: LSTMCell cell_params: num_units: 512   Input Pipeline
  InputPipeline定义了如何读取、解析数据并将数据分隔成特征和标签。如果您想要读取新的数据格式，我们需要实现自己的输入管道。
 Encoder(编码)
 Decoder（解码）
 Model（Attention）</description>
    </item>
    
    <item>
      <title>Attention那些事儿</title>
      <link>https://7125messi.github.io/post/attention%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</link>
      <pubDate>Wed, 10 Jul 2019 22:30:51 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/attention%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</guid>
      <description>[参考整理]
原文：遍地开花的 Attention ，你真的懂吗？
阿里机器智能
Attention PPT .pdf
Attention 自2015年被提出后，在 NLP 领域，图像领域遍地开花。Attention 赋予模型区分辨别能力，从纷繁的信息中找到应当 focus 的重点。2017年 self attention 的出现，使得 NLP 领域对词句 representation 能力有了很大的提升，整个 NLP 领域开启了全面拥抱 transformer 的年代。

1 初识Attention 
1.1 History 
Attention 的发展可以粗暴地分为两个阶段。
2015-2017年，自从 attention 提出后，基本就成为 NLP 模型的标配，各种各样的花式 attention 铺天盖地。不仅在 Machine Translation，在 Text summarization，Text Comprehend（Q&amp;amp;A）, Text Classification 也广泛应用。奠定基础的几篇文章如下：
2015年 ICLR 《Neural machine translation by jointly learning to align and translate》首次提出 attention（基本上算是公认的首次提出），文章提出了最经典的 Attention 结构（additive attention 或者 又叫 bahdanau attention）用于机器翻译，并形象直观地展示了 attention 带来源语目标语的对齐效果，解释深度模型到底学到了什么，人类表示服气。</description>
    </item>
    
    <item>
      <title>中台案例理解</title>
      <link>https://7125messi.github.io/post/%E4%B8%AD%E5%8F%B0%E6%A1%88%E4%BE%8B%E7%90%86%E8%A7%A3/</link>
      <pubDate>Wed, 10 Jul 2019 22:26:42 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E4%B8%AD%E5%8F%B0%E6%A1%88%E4%BE%8B%E7%90%86%E8%A7%A3/</guid>
      <description>[参考整理]
缘起 百度指数搜索“中台”，可以发现，中台一词前几年几乎都没有搜索，反倒是今年5月21号开始蹭蹭往上涨！
百度指数
仔细搜索了一下原来5月21号腾讯召开了全球数字生态大会，会议上腾讯高级副总裁汤道生提出“开放中台能力，助力产业升级”。汤道生介绍，腾讯技术委员会正在推动“开源协同”和“自研上云”，通过技术整合实现高效的能力交付。同时，基于在即时通讯、社交、游戏等优势领域中的技术积累，腾讯将进一步开放业界领先的包括用户中台、内容中台、应用中台等在内的数据中台，以及包括通信中台、AI中台、安全中台等在内的技术中台。企业与开发者可以灵活地把这些技术应用到业务场景中。中台一词开始步入大家的视角。
其实腾讯并不是最早弄中台的，但今年中台是被腾讯带火的。国内最早弄中台的公司是阿里巴巴！说到阿里巴巴的中台就不得不说到芬兰的一家游戏公司Supercell！

芬兰游戏公司Supercell 2015年年中，马云带领阿里巴巴集团高管，拜访了位于芬兰赫尔辛基的移动游戏公司Supercell。Supercell当时号称是世界上最成功的移动游戏公司，Supercell由6名资深游戏开发者在2010年创立，旗下拥有《部落冲突》、《皇室战争》、《海岛奇兵》和《卡通农场》这四款超级现象级产品。Supercell是一家典型的以小团队模式进行游戏开发的公司，以2到5个员工、最多不超过7个员工组成独立的开发团队，称之为Cell(细胞) ，这也是公司名字Supercell （超级细胞）的由来。团队自己决定做什么样的产品，然后最快时间推出产品公测版，看看游戏是否受用户欢迎。如果用户不欢迎，迅速放弃这个产品，再进行新的尝试，期间几乎没有管理角色的介入。团队研发的产品失败后，不但不会受到惩罚，甚至还会举办庆祝仪式，以庆祝他们从失败中学到了东西。这种模式让Supercell公司成为了年税前利润15亿美金的游戏公司。
2016年6月，腾讯以86亿美元收购了员工数不超过200人的Supercell公司84.3%的股权，每一名员工人均贡献的估值超过3.54亿人民币。Supercell的成功很大原因就在于其高效的“部落”组织策略。在supercell仅有的100多人中，被分成若干个小前台组织，每个小组虽然人不多，但都包含了做一款游戏需要的所有人才。本来就不大的公司被分成若干个小组，这样做的好处是可以快速决策，快速研发，快速把产品推向市场，而游戏引擎、服务器等后台基础则不需要操心。
Supercell 的模式给参加此次拜访的阿里高管们很大的震撼，在大家反复的心得交流和讨论中，一个非常重要的问题引起了很多人的反思：信息时代的公司架构到底应该是怎样的？正是有了这次拜访才真正让阿里巴巴的领导层有了足够的决心要将组织架构进行调整，在此次拜访的半年后，阿里集团CEO逍遥子发出内部邮件，组织架构全面升级，建设整合阿里产品技术和数据能力的强大中台，组建“大中台，小前台”的组织和业务体制。

阿里中台 所谓的“中台”，并不是阿里巴巴首先提出的词语，从字面意思上理解，中台是基于前台和后台之间。阿里通过多年不懈的努力，在业务的不断催化滋养下，将自己的技术和业务能力沉淀出一套综合能力平台，具备了对于前台业务变化及创新的快速响应能力。阿里人将“中台战略”形象地比喻成陆海空三军立体化协同作战：
他们将中台分为六类，分别对应不同兵种：
业务中台，提供重用服务，例如用户中心、订单中心之类的开箱即用可重用能力，为战场提供了空军支援能力，随叫随到，威力强大；
数据中台，提供数据分析能力，帮助从数据中学习改进，调整方向，为战场提供了海军支援能力；
算法中台，提供算法能力，帮助提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡；
技术中台，提供自建系统部分的技术支撑能力，帮助解决基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备；
研发中台，提供自建系统部分的管理和技术实践支撑能力，帮助快速搭建项目、管理进度、测试、持续集成、持续交付，是前台特种兵的训练基地；
组织中台，为项目提供投资管理、风险管理、资源调度等，是战场的指挥部，战争的大脑，指挥前线，调度后方。
2018双11，阿里又一次实现了一次壮举，在2135亿的背后，在令人骄傲的战绩背后，缺少不了阿里中台铁军发挥的巨大力量。
阿里中台建设也并非一帆风顺。曾经淘宝就曾大费精力搭建了一个CRM平台，但大部分商家不买账，因为仅靠一套系统，根本无法满足不同行业、不同规模的几百万商家多样化的需求。后来淘宝采用中台战略思想，将15万家ISV的服务能力组织起来，进行组件化，搞定几百万家商家不同需求。通过开放赋能不同商家、不同业务，帮助商家实现业务创新。就此阿里中台战略在企业服务生态建设方面拉开了序幕。说完这么多中台的事情，可能你还是不知道什么是中台！再举一下董阳对阿里巴巴数据中台的理解。

白话数据中台 中台就是公共服务平台，数据中台就是将数据加工以后封装成一个公共的数据产品或服务。
家里厨房有油/盐/酱油/醋/料酒/生抽…很多种调料（数据），你（业务部门）特别喜欢吃糖醋排骨/糖醋鱼/糖醋里脊/糖醋猪蹄…（各种业务应用），你老妈（IT部门）觉得每天都按照比例调制糖醋汁很麻烦很浪费时间还每次都有偏差（每次数据有误差），于是你老妈决定按照“1料酒；2酱油；3白糖；4醋；5水”的比例（数据算法）调制好一大桶糖醋汁（数据产品），以后每天倒一点糖醋汁就可以很快做出一盘糖醋XX（业务应用）。
这个调制糖醋汁的过程就相当于构建了 一个数据中台，糖醋汁就是数据产品。数据产品往往不是直接提供给用户使用的，而是提供给业务应用使用的（类似于糖醋汁不是用来直接喝的，而是用来做糖醋XX的）。另外，为了调制更快更准确，可能还需要买一些密封大桶/漏斗/量杯（ETL/BI 等数据工具）。
当然，如果你家十天半个月才做一次糖醋XX(低频)，那就没有必要调制一大桶糖醋汁方哪儿（不需要构建这个数据产品）。类似这个逻辑，如果你家每天都做八宝粥，则可以把八种粮食（数据）混合好放一个大桶里做成八宝粥混料（数据产品）。
如果你老妈的糖醋XX做的特别好开了个餐馆，每天做给几百个人吃（需求量变大），就需要调制更多糖醋汁买个冰箱存起来（数据仓库），这也解决了随用随跳（实时取数）的效率瓶颈。所以，在做数据中台之前，先自问一下：
 有没有糖醋汁、八宝粥混料的需求？（有没有数据产品的需求？）
2. 有多少人吃？（使用这个数据产品的需求量大不大？）
3. 多久吃一次？（需要这个数据产品的频率高不高？）
如果以上都合理，就可以开始规划数据中台了。再回过头看，公司为什么要建中台？  在过去几年中，借着移动互联网的红利，许多公司都高速发展，进行大规模业务拓展，业务拓展的速度足够快，对公司自然是好事，但是随着而来的问题就是，公司内部出现了大量的重复建设和资源浪费的现象，重复造轮子。
如滴滴，有顺风车、快车、专车、代驾等多业务的垂直架构，这些业务虽然会有一些差别，但是核心系统和流程都是类似的。如果各自独立开发，也会出现各种各样的问题。开发成本过高，滴滴旗下的每个业务，其实都是可以单独支撑起一家公司的，如果每个业务都独立做到极致，那么开发成本和人力成本就会非常巨大，而如果为了控制成本，就把系统的建设放缓，则意味着，无论是核心系统本身的质量，还是对外的用户体验都不太好。在这样的背景下，滴滴也开始考虑将诸多业务，以及各个城市的系统统一规划，统一建设，提升服务前台的能力。

滴滴中台 2015年末滴滴启动了中台战略整合业务系统。决定构建业务中台主要出于四方面考虑：专业深度、人力资源、用户体验、全局打通。
· 专业深度。由于是多业务垂直化的架构，会有多个团队开发同样的架构，这就需要很多的工程师。每个团队都是用最快速的方式构建流程，所以技术很难做深。这样一来，导致客户端的流畅度不高，后端不稳定，影响可扩展性。
· 人力资源。原则上来说把每个团队加到足够的人，每个架构都能有很好的发展。但工程师的薪资都非常高，招聘大量工程师来做同样的架构，研发成本高昂。很还有些时候，愿意花钱，却招聘不到合适的人。
· 用户体验。流畅度、稳定性、扩展性、界面、交易流程等都是影响用户体验的重要因素。在当时的组织结构和研发情况下，会出现业务的颜色各异，交易流程却相同的问题，很影响用户的体验。
· 全局打通。所有业务本质都是出行，出行本质有协同效应。但在各自独立发展情况下，协同性就完全没有，在构建中台过程中，可以逐步把协同性加起来。

总结 看到这里，你估计对中台有个大概了解。中台不是凭空产生的，而是建立在业务之上，公司发展过程中一些项目有点不同，然后重新搭建架构，有点资源浪费，搭建中台系统完美解决重复造轮子问题。当然大公司才有这困惑，小公司就不用加戏了，不用盲目跟风，做好手上的事情，时间到了，风口就来了！</description>
    </item>
    
    <item>
      <title>自然语言处理中的预训练技术发展史</title>
      <link>https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/</link>
      <pubDate>Mon, 08 Jul 2019 22:39:47 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/</guid>
      <description>[参考整理]
本文来自新浪微博资深算法工程师张俊林老师的讲解，是非常好的学习材料。
Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。
本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。

图像领域的预训练 自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。
那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。
这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。
那么新的问题来了，为什么这种预训练的思路是可行的？
目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。
一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。
听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”
嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。
没听过？那下面就把这段陈年老账讲给你听听。

Word Embedding考古史 这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。
什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。
假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？
你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。
上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词 Wt = &amp;ldquo;Bert&amp;rdquo; 前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：
前面任意单词 Wi 用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 C(Wi) ，每个单词的C(Wi)拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C(Wi) 是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。
2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。
Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。
为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。
使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。
我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。
假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。
上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</description>
    </item>
    
    <item>
      <title>自然语言处理三大特征抽取器</title>
      <link>https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%89%E5%A4%A7%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%99%A8/</link>
      <pubDate>Mon, 08 Jul 2019 22:35:25 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%89%E5%A4%A7%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%99%A8/</guid>
      <description>[参考整理]
本文来自新浪微博资深算法工程师张俊林老师的讲解，是非常好的学习材料。
在上一篇介绍Bert的文章“自然语言处理中的预训练技术发展史”里，我曾大言不惭地宣称如下两个个人判断：
第一个是Bert这种两阶段的模式（预训练+Finetuning）必将成为NLP领域研究和工业应用的流行方法；
第二个是从NLP领域的特征抽取器角度来说，Transformer会逐步取代RNN成为最主流的的特征抽取器。
关于特征抽取器方面的判断，上面文章限于篇幅，只是给了一个结论，并未给出具备诱惑力的说明，看过我文章的人都知道我不是一个随便下结论的人（那位正在补充下一句：“你随便起来不是……”的同学请住口，请不要泄露国家机密，你可以继续睡觉，吵到其它同学也没有关系，哈哈），但是为什么当时我会下这个结论呢？本文可以看做是上文的一个外传，会给出比较详实的证据来支撑之前给出的结论。
如果对目前NLP里的三大特征抽取器的未来走向趋势做个宏观判断的话，我的判断是这样的：
 RNN人老珠黄，已经基本完成它的历史使命，将来会逐步退出历史舞台； CNN如果改造得当，将来还是有希望有自己在NLP领域的一席之地，如果改造成功程度超出期望，那么还有一丝可能作为割据一方的军阀，继续生存壮大，当然我认为这个希望不大，可能跟宋小宝打篮球把姚明打哭的概率相当； 而新欢Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。至于将来是否会出现新的特征抽取器，一枪将Tranformer挑落马下，继而取而代之成为新的特征抽取山大王？这种担忧其实是挺有必要的，毕竟李商隐在一千年前就告诫过我们说：“君恩如水向东流，得宠忧移失宠愁。 莫向樽前奏花落，凉风只在殿西头。”  当然这首诗看样子目前送给RNN是比较贴切的，至于未来Transformer是否会失宠？这个问题的答案基本可以是肯定的，无非这个时刻的来临是3年之后，还是1年之后出现而已。当然，我希望如果是在读这篇文章的你，或者是我，在未来的某一天，从街头拉来一位长相普通的淑女，送到韩国整容，一不小心偏离流水线整容工业的美女模板，整出一位天香国色的绝色，来把Transformer打入冷宫，那是最好不过。但是在目前的状态下，即使是打着望远镜，貌似还没有看到有这种资质的候选人出现在我们的视野之内。
我知道如果是一位严谨的研发人员，不应该在目前局势还没那么明朗的时候做出如上看似有些武断的明确结论，所以这种说法可能会引起争议。但是这确实就是我目前的真实想法，至于根据什么得出的上述判断？这种判断是否有依据？依据是否充分？相信你在看完这篇文章可以有个属于自己的结论。
可能谈到这里，有些平常吃亏吃的少所以喜欢挑刺的同学会质疑说：你凭什么说NLP的典型特征抽取器就这三种呢？你置其它知名的特征抽取器比如Recursive NN于何地?嗯，是，很多介绍NLP重要进展的文章里甚至把Recursive NN当做一项NLP里的重大进展，除了它，还有其它的比如Memory Network也享受这种部局级尊贵待遇。但是我一直都不太看好这两个技术，而且不看好很多年了，目前情形更坚定了这个看法。而且我免费奉劝你一句，没必要在这两个技术上浪费时间，至于为什么，因为跟本文主题无关，以后有机会再详细说。
上面是结论，下面，我们正式进入举证阶段。

战场侦查：NLP任务的特点及任务类型 NLP任务的特点和图像有极大的不同，上图展示了一个例子，NLP的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。如果你听到我对你说：“你欠我那一千万不用还了”和“我欠你那一千万不用还了”，你听到后分别是什么心情？两者区别了解一下；另外，句子中的长距离特征对于理解语义也非常关键，例子参考上图标红的单词，特征抽取器能否具备长距离特征捕获能力这一点对于解决NLP任务来说也是很关键的。
上面这几个特点请记清，一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性。这也是为何我在介绍RNN、CNN、Transformer等特征抽取器之前，先说明这些内容的原因。
NLP是个很宽泛的领域，包含了几十个子领域，理论上只要跟语言处理相关，都可以纳入这个范围。但是如果我们对大量NLP任务进行抽象的话，会发现绝大多数NLP任务可以归结为几大类任务。两个看似差异很大的任务，在解决任务的模型角度，可能完全是一样的。
通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。
解决这些不同的任务，从模型角度来讲什么最重要？是特征抽取器的能力。尤其是深度学习流行开来后，这一点更凸显出来。因为深度学习最大的优点是“端到端（end to end）”，当然这里不是指的从客户端到云端，意思是以前研发人员得考虑设计抽取哪些特征，而端到端时代后，这些你完全不用管，把原始输入扔给好的特征抽取器，它自己会把有用的特征抽取出来。
身为资深Bug制造者和算法工程师，你现在需要做的事情就是：选择一个好的特征抽取器，选择一个好的特征抽取器，选择一个好的特征抽取器，喂给它大量的训练数据，设定好优化目标（loss function），告诉它你想让它干嘛……..然后你觉得你啥也不用干等结果就行了是吧？那你是我见过的整个宇宙中最乐观的人…….你大量时间其实是用在调参上…….。从这个过程可以看出，如果我们有个强大的特征抽取器，那么中初级算法工程师沦为调参侠也就是个必然了，在AutoML（自动那啥）流行的年代，也许以后你想当调参侠而不得，李斯说的“吾欲与若复牵黄犬，俱出上蔡东门逐狡兔，岂可得乎！”请了解一下。所以请珍惜你半夜两点还在调整超参的日子吧，因为对于你来说有一个好消息一个坏消息，好消息是：对于你来说可能这样辛苦的日子不多了！坏消息是：对于你来说可能这样辛苦的日子不多了！！！那么怎么才能成为算法高手？你去设计一个更强大的特征抽取器呀。
下面开始分叙三大特征抽取器。

沙场老将RNN：廉颇老矣，尚能饭否 RNN模型我估计大家都熟悉，就不详细介绍了，模型结构参考上图，核心是每个输入对应隐层节点，而隐层节点之间形成了线性序列，信息由前向后在隐层之间逐步向后传递。我们下面直接进入我想讲的内容。
为何RNN能够成为解决NLP问题的主流特征抽取器
我们知道，RNN自从引入NLP界后，很快就成为吸引眼球的明星模型，在NLP各种任务中被广泛使用。但是原始的RNN也存在问题，它采取线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题。为了解决这个问题，后来引入了LSTM和GRU模型，通过增加中间状态信息直接向后传播，以此缓解梯度消失问题，获得了很好的效果，于是很快LSTM和GRU成为RNN的标准模型。其实图像领域最早由HighwayNet/Resnet等导致模型革命的skip connection的原始思路就是从LSTM的隐层传递机制借鉴来的。经过不断优化，后来NLP又从图像领域借鉴并引入了attention机制（从这两个过程可以看到不同领域的相互技术借鉴与促进作用），叠加网络把层深作深，以及引入Encoder-Decoder框架，这些技术进展极大拓展了RNN的能力以及应用效果。下图展示的模型就是非常典型的使用RNN来解决NLP任务的通用框架技术大礼包，在更新的技术出现前，你可以在NLP各种领域见到这个技术大礼包的身影。
上述内容简单介绍了RNN在NLP领域的大致技术演进过程。那么为什么RNN能够这么快在NLP流行并且占据了主导地位呢？主要原因还是因为RNN的结构天然适配解决NLP的问题，NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。所以RNN特别适合NLP这种线形序列应用场景，这是RNN为何在NLP界如此流行的根本原因。
RNN在新时代面临的两个严重问题
RNN在NLP界一直红了很多年（2014-2018？），在2018年之前，大部分各个子领域的State of Art的结果都是RNN获得的。但是最近一年来，眼看着RNN的领袖群伦的地位正在被动摇，所谓各领风骚3-5年，看来网红模型也不例外。
那这又是因为什么呢？主要有两个原因。
第一个原因在于一些后起之秀新模型的崛起，比如经过特殊改造的CNN模型，以及最近特别流行的Transformer，这些后起之秀尤其是Transformer的应用效果相比RNN来说，目前看具有明显的优势。这是个主要原因，老人如果干不过新人，又没有脱胎换骨自我革命的能力，自然要自觉或不自愿地退出历史舞台，这是自然规律。至于RNN能力偏弱的具体证据，本文后面会专门谈，这里不展开讲。当然，技术人员里的RNN保皇派们，这个群体规模应该还是相当大的，他们不会轻易放弃曾经这么热门过的流量明星的，所以也想了或者正在想一些改进方法，试图给RNN延年益寿。至于这些方法是什么，有没有作用，后面也陆续会谈。
另外一个严重阻碍RNN将来继续走红的问题是：RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力，这个乍一看好像不是太大的问题，其实问题很严重。如果你仅仅满足于通过改RNN发一篇论文，那么这确实不是大问题，但是如果工业界进行技术选型的时候，在有快得多的模型可用的前提下，是不太可能选择那么慢的模型的。一个没有实际落地应用支撑其存在价值的模型，其前景如何这个问题，估计用小脑思考也能得出答案。
那问题来了：为什么RNN并行计算能力比较差？是什么原因造成的？
我们知道，RNN之所以是RNN，能将其和其它模型区分开的最典型标志是：T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词Xt，这个不算特点，所有模型都要接收这个原始输入；关键的是另外一个输入，T时刻的隐层状态St还依赖T-1时刻的隐层状态S(t-1)的输出，这是最能体现RNN本质特征的一点，RNN的历史信息是通过这个信息传输渠道往后传输的，示意参考上图。那么为什么RNN的并行计算能力不行呢？问题就出在这里。因为T时刻的计算依赖T-1时刻的隐层计算结果，而T-1时刻的计算依赖T-2时刻的隐层计算结果……..这样就形成了所谓的序列依赖关系。就是说只能先把第1时间步的算完，才能算第2时间步的结果，这就造成了RNN在这个角度上是无法并行计算的，只能老老实实地按着时间步一个单词一个单词往后走。
而CNN和Transformer就不存在这种序列依赖问题，所以对于这两者来说并行计算能力就不是问题，每个时间步的操作可以并行一起计算。
那么能否针对性地对RNN改造一下，提升它的并行计算能力呢？如果可以的话，效果如何呢？下面我们讨论一下这个问题。
如何改造RNN使其具备并行计算能力？
上面说过，RNN不能并行计算的症结所在，在于T时刻对T-1时刻计算结果的依赖，而这体现在隐层之间的全连接网络上。既然症结在这里，那么要想解决问题，也得在这个环节下手才行。在这个环节多做点什么事情能够增加RNN的并行计算能力呢？你可以想一想。
其实留给你的选项并不多，你可以有两个大的思路来改进：一种是仍然保留任意连续时间步（T-1到T时刻）之间的隐层连接；而另外一种是部分地打断连续时间步（T-1到T时刻）之间的隐层连接 。
我们先来看第一种方法，现在我们的问题转化成了：我们仍然要保留任意连续时间步（T-1到T时刻）之间的隐层连接，但是在这个前提下，我们还要能够做到并行计算，这怎么处理呢？因为只要保留连续两个时间步的隐层连接，则意味着要计算T时刻的隐层结果，就需要T-1时刻隐层结果先算完，这不又落入了序列依赖的陷阱里了吗？嗯，确实是这样，但是为什么一定要在不同时间步的输入之间并行呢？没有人说RNN的并行计算一定发生在不同时间步上啊，你想想，隐层是不是也是包含很多神经元？那么在隐层神经元之间并行计算行吗？如果你要是还没理解这是什么意思，那请看下图。
上面的图只显示了各个时间步的隐层节点，每个时间步的隐层包含3个神经元，这是个俯视图，是从上往下看RNN的隐层节点的。另外，连续两个时间步的隐层神经元之间仍然有连接，上图没有画出来是为了看着简洁一些。这下应该明白了吧，假设隐层神经元有3个，那么我们可以形成3路并行计算（红色箭头分隔开成了三路），而每一路因为仍然存在序列依赖问题，所以每一路内仍然是串行的。大思路应该明白了是吧？但是了解RNN结构的同学会发现这样还遗留一个问题：隐层神经元之间的连接是全连接，就是说T时刻某个隐层神经元与T-1时刻所有隐层神经元都有连接，如果是这样，是无法做到在神经元之间并行计算的，你可以想想为什么，这个简单，我假设你有能力想明白。那么怎么办呢？很简单，T时刻和T-1时刻的隐层神经元之间的连接关系需要改造，从之前的全连接，改造成对应位置的神经元（就是上图被红箭头分隔到同一行的神经元之间）有连接，和其它神经元没有连接。这样就可以解决这个问题，在不同路的隐层神经元之间可以并行计算了。
第一种改造RNN并行计算能力的方法思路大致如上所述，这种方法的代表就是论文“Simple Recurrent Units for Highly Parallelizable Recurrence”中提出的SRU方法，它最本质的改进是把隐层之间的神经元依赖由全连接改成了哈达马乘积，这样T时刻隐层单元本来对T-1时刻所有隐层单元的依赖，改成了只是对T-1时刻对应单元的依赖，于是可以在隐层单元之间进行并行计算，但是收集信息仍然是按照时间序列来进行的。所以其并行性是在隐层单元之间发生的，而不是在不同时间步之间发生的。
这其实是比较巧妙的一种方法，但是它的问题在于其并行程度上限是有限的，并行程度取决于隐层神经元个数，而一般这个数值往往不会太大，再增加并行性已经不太可能。另外每一路并行线路仍然需要序列计算，这也会拖慢整体速度。SRU的测试速度为：在文本分类上和原始CNN（Kim 2014）的速度相当，论文没有说CNN是否采取了并行训练方法。 其它在复杂任务阅读理解及MT任务上只做了效果评估，没有和CNN进行速度比较，我估计这是有原因的，因为复杂任务往往需要深层网络，其它的就不妄作猜测了。
第二种改进典型的思路是：为了能够在不同时间步输入之间进行并行计算，那么只有一种做法，那就是打断隐层之间的连接，但是又不能全打断，因为这样基本就无法捕获组合特征了，所以唯一能选的策略就是部分打断，比如每隔2个时间步打断一次，但是距离稍微远点的特征如何捕获呢？只能加深层深，通过层深来建立远距离特征之间的联系。代表性模型比如上图展示的Sliced RNN。我当初看到这个模型的时候，心里忍不住发出杠铃般的笑声，情不自禁地走上前跟他打了个招呼：你好呀，CNN模型，想不到你这个糙汉子有一天也会穿上粉色裙装，装扮成RNN的样子出现在我面前啊，哈哈。了解CNN模型的同学看到我上面这句话估计会莞尔会心一笑：这不就是简化版本的CNN吗？不了解CNN的同学建议看完后面CNN部分再回头来看看是不是这个意思。
那经过这种改造的RNN速度改进如何呢？论文给出了速度对比实验，归纳起来，SRNN速度比GRU模型快5到15倍，嗯，效果不错，但是跟对比模型DC-CNN模型速度比较起来，比CNN模型仍然平均慢了大约3倍。这很正常但是又有点说不太过去，说正常是因为本来这就是把RNN改头换面成类似CNN的结构，而片段里仍然采取RNN序列模型，所以必然会拉慢速度，比CNN慢再正常不过了。说“说不过去”是指的是：既然本质上是CNN，速度又比CNN慢，那么这么改的意义在哪里？为什么不直接用CNN呢？是不是？前面那位因为吃亏吃的少所以爱抬杠的同学又会说了：也许人家效果特别好呢。嗯，从这个结构的作用机制上看，可能性不太大。你说论文实验部分证明了这一点呀，我认为实验部分对比试验做的不充分，需要补充除了DC-CNN外的其他CNN模型进行对比。当然这点纯属个人意见，别当真，因为我讲起话来的时候经常摇头晃脑，此时一般会有人惊奇地跟我反馈说：为什么你一讲话我就听到了水声？</description>
    </item>
    
    <item>
      <title>Feature_selector实现高效的特征选择</title>
      <link>https://7125messi.github.io/post/feature_selector%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</link>
      <pubDate>Wed, 03 Jul 2019 22:18:02 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/feature_selector%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</guid>
      <description>[参考整理]
具体可以参考我的Azure Notebook：
https://notebooks.azure.com/messi7125/projects/feature-engineering
下面简要介绍一下功能介绍
Introduction: Feature Selector Usage 在这个笔记本中，我们将使用FeatureSelector类来选择要从数据集中删除的要素。 此类有五种方法可用于查找要删除的功能：
 具有高missing-values百分比的特征 具有高相关性的特征 对模型预测结果无贡献的特征（即zero importance） 对模型预测结果只有很小贡献的特征（即low importance） 具有单个值的特征（即数据集中该特征取值的集合只有一个元素）  requirements：
lightgbm==2.1.1 matplotlib==2.1.2 seaborn==0.8.1 numpy==1.14.5 pandas==0.23.1 scikit-learn==0.19.1  from feature_selector.selector import FeatureSelector import pandas as pd  Example Dataset 该数据集被用作[Kaggle的[Home Credit Default Risk Competition]竞赛的一部分（https://www.kaggle.com/c/home-credit-default-risk/）。 它适用于受监督的机器学习分类任务，其目标是预测客户是否违约贷款。 整个数据集可以[这里]下载，我们将使用10,000行的小样本。
特征选择器设计用于机器学习任务，但可以应用于任何数据集。基于特征重要性的方法确实需要受监督的机器学习问题。
train = pd.read_csv(&#39;../data/credit_example.csv&#39;) train_labels = train[&#39;TARGET&#39;] train.head()  数据集中有几个分类列。 使用基于特征重要性的方法时，FeatureSelector使用独热编码处理这些。
train = train.drop(columns = [&#39;TARGET&#39;]) train.head()  Implementation FeatureSelector有五个函数用于识别要删除的列：
 identify_missing identify_single_unique identify_collinear identify_zero_importance identify_low_importance  这些方法根据指定的标准查找要删除的功能。 标识的特征存储在FeatureSelector的ops属性（Python字典）中。 我们可以手动删除已识别的功能，或使用FeatureSelector中的remove功能来实际删除功能。</description>
    </item>
    
    <item>
      <title>UER Py高质量中文BERT预训练模型</title>
      <link>https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 30 Jun 2019 21:26:33 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</guid>
      <description>[原创]
最近在github看到了一个项目:
https://github.com/dbiir/UER-py
貌似是腾讯出品，良心之作，特地去Colab撸了一下，确实可以，封装了很多模块，后续估计还要迭代，直至打成包就更好了。
有兴趣的同学可以去我的Colab上，玩一下。
https://colab.research.google.com/drive/1N81AYxPolDWPMdbZpYO1NnfV4T403Bxz</description>
    </item>
    
    <item>
      <title>NLP深度学习的各类模型综述</title>
      <link>https://7125messi.github.io/post/nlp%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%84%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Sun, 30 Jun 2019 16:16:44 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/nlp%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%84%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</guid>
      <description>[参考整理]
参考来源：https://www.jiqizhixin.com/articles/2019-06-21
导读：NLP，让人与机器的交互不再遥远；深度学习，让语言解析不再是智能系统的瓶颈。本文尝试回顾NLP发展的历史，解读NLP升级迭代过程中一些重要而有意思的模型，总结不同任务场景应用模型的思路，对NLP未来发展趋势发表自己的看法。笔者经验有限，如有不全或不当地方，请予指正。

1 背景 自然语言处理（英语：Natural Language Process，简称NLP）是计算机科学、信息工程以及人工智能的子领域，专注于人机语言交互，探讨如何处理和运用自然语言。自然语言处理的研究，最早可以说开始于图灵测试，经历了以规则为基础的研究方法，流行于现在基于统计学的模型和方法，从早期的传统机器学习方法，基于高维稀疏特征的训练方式，到现在主流的深度学习方法，使用基于神经网络的低维稠密向量特征训练模型。最近几年，随着深度学习以及相关技术的发展，NLP领域的研究取得一个又一个突破，研究者设计各种模型和方法，来解决NLP的各类问题。下图是Young等[1]统计了过去6年ACL、EMNLP、EACL和NAACL上发表深度学习长篇论文的比例逐年增加，而2018年下半场基本是ELMo、GPT、BERT等深度学习模型光芒四射的showtime，所以本文会将更多的笔墨用于陈述分析深度学习模型。
机器学习是计算机通过模式和推理、而不是明确指令的方式，高效执行指定任务的学习算法。贝叶斯概率模型、逻辑回归、决策树、SVM、主题模型、HMM模型等，都是常见的用于NLP研究的传统机器学习算法。而深度学习是一种基于特征学习的机器学习方法，把原始数据通过简单但非线性的模块转变成更高层次、更加抽象的特征表示，通过足够多的转换组合，非常复杂的函数也能被学习。在多年的实验中，人们发现了认知的两个重要机制：抽象和迭代，从原始信号，做底层抽象，逐渐向高层抽象迭代，在迭代中抽象出更高层的模式。如何形象地理解？在机器视觉领域会比较容易理解，深度学习通过多层神经网络依次提取出图像信息的边缘特征、简单形状特征譬如嘴巴的轮廓、更高层的形状特征譬如脸型；而在自然语言处理领域则没有那么直观的理解，我们可以通过深度学习模型学习到文本信息的语法特征和语义特征。可以说，深度学习，代表自然语言处理研究从机器学习到认知计算的进步。
要讲深度学习，得从语言模型开始讲起。自然语言处理的基础研究便是人机语言交互，以机器能够理解的算法来反映人类的语言，核心是基于统计学的语言模型。语言模型（英语：Language Model，简称LM），是一串词序列的概率分布。通过语言模型，可以量化地评估一串文字存在的可能性。对于一段长度为n的文本，文本中的每个单词都有通过上文预测该单词的过程，所有单词的概率乘积便可以用来评估文本存在的可能性。在实践中，如果文本很长，P(w_i|context(w_i))的估算会很困难，因此有了简化版：N元模型。在N元模型中，通过对当前词的前N个词进行计算来估算该词的条件概率。对于N元模型。常用的有unigram、bigram和trigram，N越大，越容易出现数据稀疏问题，估算结果越不准。为了解决N元模型估算概率时的数据稀疏问题，研究者尝试用神经网络来研究语言模型。
早在2000年，就有研究者提出用神经网络研究语言模型的想法，经典代表有2003年Bengio等[2]提出的NNLM，但效果并不显著，深度学习用于NLP的研究一直处在探索的阶段。直到2011年，Collobert等[3]用一个简单的深度学习模型在命名实体识别NER、语义角色标注SRL、词性标注POS-tagging等NLP任务取得SOTA成绩，基于深度学习的研究方法得到越来越多关注。2013年，以Word2vec、Glove为代表的词向量大火，更多的研究从词向量的角度探索如何提高语言模型的能力，研究关注词内语义和上下文语义。此外，基于深度学习的研究经历了CNN、RNN、Transormer等特征提取器，研究者尝试用各种机制优化语言模型的能力，包括预训练结合下游任务微调的方法。最近最吸睛的EMLo、GPT和BERT模型，便是这种预训练方法的优秀代表，频频刷新SOTA。

2 模型 接下来文章会以循序渐进的方式分成五个部分来介绍深度学习模型：第一部分介绍一些基础模型；第二部分介绍基于CNN的模型；第三部分介绍基于RNN的模型；第四部分介绍基于Attention机制的模型；第五部分介绍基于Transformer的模型，讲述一段语言模型升级迭代的故事。NLP有很多模型和方法，不同的任务场景有不同的模型和策略来解决某些问题。笔者认为，理解了这些模型，再去理解别的模型便不是很难的事情，甚至可以自己尝试设计模型来满足具体任务场景的需求。
Basic Embedding Model
NNLM是非常经典的神经网络语言模型，虽然相比传统机器学习并没有很显著的成效，但大家说起NLP在深度学习方向的探索，总不忘提及这位元老。分布式表示的概念很早就有研究者提出并应用于深度学习模型，Word2vec使用CBOW和Skip-gram训练模型，意外的语意组合效果才使得词向量广泛普及。FastText在语言模型上没有什么特别的突破，但模型的优化使得深度学习模型在大规模数据的训练非常快甚至秒级，而且文本分类的效果匹敌CNN/RNN之类模型，在工业化应用占有一席之地。

NNLM 2003年Bengio等[2]提出一种基于神经网络的语言模型NNLM，模型同时学习词的分布式表示，并基于词的分布式表示，学习词序列的概率函数，这样就可以用词序列的联合概率来表示句子，而且，模型还能够产生指数量级的语义相似的句子。作者认为，通过这种方式训练出来的语言模型具有很强的泛化能力，对于训练语料里没有出现过的句子，模型能够通过见过的相似词组成的相似句子来学习。
模型训练的目标函数是长度为n的词序列的联合概率，分解成两部分：一是特征映射，通过映射矩阵C，将词典V中的每个单词都能映射成一个特征向量C(i) ∈ Rm；二是计算条件概率分布，通过函数g，将输入的词向量序列(C(wt−n+1),··· ,C(wt−1))转化成一个概率分布y ∈ R|V|，g的输出是个向量，第i个元素表示词序列第n个词是Vi的概率。网络输出层采用softmax函数。

Word2vec 对于复杂的自然语言任务进行建模，概率模型应运而生成为首选的方法，但在最开始的时候，学习语言模型的联合概率函数存在致命的维数灾难问题。假如语言模型的词典大小为100000，要表示10个连续词的联合分布，模型参数可能就要有1050个。相应的，模型要具备足够的置信度，需要的样本量指数级增加。为了解决这个问题，最早是1986年Hinton等[5]提出分布式表示（Distributed Representation），基本思想是将词表示成n维连续的实数向量。分布式表示具备强大的特征表示能力，n维向量，每维有k个值，便能表示 kn个特征。
词向量是NLP深度学习研究的基石，本质上遵循这样的假设：语义相似的词趋向于出现在相似的上下文。因此在学习过程中，这些向量会努力捕捉词的邻近特征，从而学习到词汇之间的相似性。有了词向量，便能够通过计算余弦距离等方式来度量词与词之间的相似度。
Mikolov等[4]对词向量的广泛普及功不可没，他们用CBOW和Skip-gram模型训练出来的词向量具有神奇的语意组合特效，词向量的加减计算结果刚好是对应词的语意组合，譬如v(King) - v(Man) + v(Woman) = v(Queen)。这种意外的特效使得Word2vec快速流行起来。至于为什么会有这种行为呢？有意思的是Gittens等[10]做了研究，尝试给出了理论假设：词映射到低维分布式空间必须是均匀分布的，词向量才能有这个语意组合效果。
CBOW和Skip-gram是Word2vec的两种不同训练方式。CBOW指抠掉一个词，通过上下文预测该词；Skip-gram则与CBOW相反，通过一个词预测其上下文。
以CBOW为例。CBOW模型是一个简单的只包含一个隐含层的全连接神经网络，输入层采用one-hot编码方式，词典大小为V；隐含层大小是N；输出层通过softmax函数得到词典里每个词的概率分布。层与层之间分别有两个权重矩阵W ∈ RV ×N和W′ ∈ RH×V。词典里的每个单词都会学习到两个向量vc和vw，分别代表上下文向量和目标词向量。因此，给予上下文c，出现目标词w的概率是：p(w|c)，该模型需要学习的网络参数θ 。
词向量是用概率模型训练出来的产物，对训练语料库出现频次很低甚至不曾出现的词，词向量很难准确地表示。词向量是词级别的特征表示方式，单词内的形态和形状信息同样有用，研究者提出了基于字级别的表征方式，甚至更细粒度的基于Byte级别的表征方式。2017年，Mikolov等[9]提出用字级别的信息丰富词向量信息。此外，在机器翻译相关任务里，基于Byte级别的特征表示方式BPE还被用来学习不同语言之间的共享信息，主要以拉丁语系为主。2019年，Lample等[11]提出基于BERT优化的跨语言模型XLM，用BPE编码方式来提升不同语言之间共享的词汇量。
虽然通过词向量，一个单词能够很容易找到语义相似的单词，但单一词向量，不可避免一词多义问题。对于一词多义问题，最常用有效的方式便是基于预训练模型，结合上下文表示单词，代表模型有EMLo、GPT和BERT，都能有效地解决这个问题。
词，可以通过词向量进行表征，自然会有人思考分布式表示这个概念是否可以延伸到句子、文章、主题等。词袋模型Bag-of-Words忽略词序以及词语义，2014年Mikolov等[8]将分布式向量的表示扩展到句子和文章，模型训练类似word2vec。关于句子的向量表示，2015年Kiros等提出skip-thought，2018年Logeswaran等提出改进版quick-thought。

FastText 2016年，Mikolov等[7]提出一种简单轻量、用于文本分类的深度学习模型，架构跟Mikolov等[4]提出的word2vec的CBOW模型相似，但有所不同：各个词的embedding向量，补充字级别的n-gram特征向量，然后将这些向量求和平均，用基于霍夫曼树的分层softmax函数，输出对应的类别标签。FastText能够做到效果好、速度快，优化的点有两个：一是引入subword n-gram的概念解决词态变化的问题，利用字级别的n-gram信息捕获字符间的顺序关系，依次丰富单词内部更细微的语义；二是用基于霍夫曼树的分层softmax函数，将计算复杂度从O(kh)降低到O(h log2(k))，其中，k是类别个数，h是文本表示的维数。相比char-CNN之类的深度学习模型需要小时或天的训练时间，FastText只需要秒级的训练时间。
CNN-based Model
词向量通过低维分布式空间能够有效地表示词，使其成为NLP深度学习研究的基石。在词向量的基础上，需要有一种有效的特征提取器从词向量序列里提取出更高层次的特征，应用到NLP任务中去，譬如机器翻译、情感分析、问答、摘要，等。鉴于卷积神经网络CNN在机器视觉领域非常出色的特征提取能力，自然而然被研究者尝试用于自然语言处理领域。
回顾CNN应用于NLP领域的过去，好像一段民间皇子误入宫廷的斗争史。CNN用于句子建模最早可以追溯到2008年Collobert等[12]的研究，使用的是查找表映射词的表征方式，这可以看作是一种原始的词向量方法。2011年Collobert等[3]扩展了他的研究工作，提出一种基于CNN的通用框架用于各种NLP任务。Collobert[3]、Kalchbrenner[14]、Kim[13]等基于CNN的研究工作，推进了CNN应用在NLP研究的普及。
CNN擅长捕获局部特征，能够将信息量丰富的潜在语义特征用于下游任务，譬如TextCNN用于文本分类。但CNN却有长距离依赖问题，研究者尝试用动态卷积网络DCNN来改善这个不足。跟RNN的竞争过程中，CNN不停地优化自身能力，现在CNN的研究趋势是：加入GLU/GTU门机制来简化梯度传播，使用Dilated CNN增加覆盖长度，基于一维卷积层叠加深度并用Residual Connections辅助优化，优秀代表：增加网络深度的VDCNN和引进Gate机制的GCNN。

TextCNN 2014年，Kim[13]提出基于预训练Word2vec的TextCNN模型用于句子分类任务。CNN，通过卷积操作做特征检测，得到多个特征映射，然后通过池化操作对特征进行筛选，过滤噪音，提取关键信息，用来分类。</description>
    </item>
    
    <item>
      <title>语言表征学习的思想演进</title>
      <link>https://7125messi.github.io/post/%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E6%83%B3%E6%BC%94%E8%BF%9B/</link>
      <pubDate>Sun, 30 Jun 2019 09:09:10 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E6%83%B3%E6%BC%94%E8%BF%9B/</guid>
      <description>[参考整理]
参考文章：https://www.jiqizhixin.com/articles/2019-06-29-3
**在预训练语言模型 BERT 对自然语言处理的冲击还未平息时，CMU 和 Google 的研究员又放出了一个猛料：在 20 多项任务上全线碾压 BERT 的 XLNet。：由于在公众号中插入方式不方便，对于一个符号 &amp;ldquo;a{b}^{c}&amp;ldquo;，&amp;rdquo;{b}&amp;rdquo; 代表下标，&amp;rdquo;{c}&amp;rdquo; 代表上标。

1. 语言表征学习 深度学习的基本单元是向量。我们将建模对象对应到各自的向量 x (或者一组向量 x{1}, x{2}, &amp;hellip;, x{n})，然后通过变换、整合得到新的向量 h，再基于向量 h 得到输出的判断 y。这里的 h 就是我们说的表征 (Representation)，它是一个向量，描述了我们的建模对象。而语言表征学习就是解决怎么样将一个词、一句话、一篇文章通过变换 (Transformation) 和整合 (Aggregation) 转化成对应的向量 h 的问题。
深度学习解决这个问题的办法是人工设计一个带有可调参数的模型，通过指定一系列的 (输入→输出) 对 (x → y)，让模型学习得到最优的参数。当参数确定之后，模型除了可以完成从 x 预测 y 的任务之外，其中间把 x 变换成 h 的方法也是可以用到其他任务的。这也是我们为什么要做表征学习。（就是说我们在做预测任务同时，顺便把表征学习这件事情给做了）
所以我们要解决的问题便是：
 怎么确定 (输入→输出) 对，即模型的预测任务
 这个模型怎么设计
  
2. 分布式语义假设 任何任务都可以用来做表征学习：情感分析 (输入句子，判断句子是正向情感还是负向情感)，机器翻译 (输入中文，输出英文)。但是这些任务的缺点是需要大量的人工标注，这些标注耗时耗力。当标注量不够时，模型很容易学出&amp;rdquo;三长一短选最短&amp;rdquo;的取巧方案 &amp;ndash; 但我们想要的是真正的语言理解。</description>
    </item>
    
    <item>
      <title>Transformer技术实战</title>
      <link>https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/</link>
      <pubDate>Tue, 25 Jun 2019 20:43:27 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/</guid>
      <description>[原创]
欢迎访问Colab链接
https://colab.research.google.com/drive/1FH6ZpQA1HGX3GZCm0yxidq7ztoyd6KhB
这里以京东商城评论文本goods_zh.txt,10万条，标注为0的是差评，标注为1的是好评。中文商品评论短文本分类器，可用于情感分析。</description>
    </item>
    
    <item>
      <title>基于UNet神经网络的城市人流预测</title>
      <link>https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/</link>
      <pubDate>Mon, 24 Jun 2019 21:34:14 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/</guid>
      <description>[原创]
1 利用手机信令数据计算人口流动数据 手机信令数据是研究人口的整体流动情况的重要数据来源。移动运营商在为手机用户提供实时通讯服务时，积累了大量的基站与设备的服务配对数据。根据配对和唤醒发生的时间以及基站的地理位置，可以很自然划定一定时间和空间范围，统计每一个时间范围内在特定空间区域内手机设备的停留，进入和离开数据，并据此估算相应的人流数据。
传统的人口网格分析过程一般只关注单个网格内某个时间点人口流动的截面数据以及城市当中不同区域的人口分布统计情况；没有将时间和空间融合考虑，不能对城市整体的人口流动形成一个完整连续的直观描述。但是，作为城市安全运营管理者和规划人员职责是需要把握好城市人口流动规律，建立有效的时空数据分析模型，从而为城市安全运行管理做好相应的人口短时预测与应急管理服务。 2 人口流动数据网格图像流处理 2.1 流处理思路 原始手机信令数据，按照一定的时间间隔（例如15分钟），划分出每个时间段的信令数据情况。主要包括：时间，格网ID，格网中心点经度，格网中心点维度，时间段内格网的停留人数，进入人数，离开人数。
根据原始数据的时空关系，将原始数据转化为4维向量空间矩阵，维度分别为时间维度、空间维度横坐标，空间维度纵坐标以及停留，进入或离开的类别：Matrix[t,i,j,k]=p意味着在t时刻，第i行第j列的空间栅格位置，k=0时则停留人数为p，k=1时则进入人数为p，k=2时则离开人数为p。
在这样的转换关系下，可以将源数据处理为3通道的时空数据。考虑到单个人员流动的时空连续性，可以认为表示人口流通的整体统计量的时空矩阵也具备一定局部关联性，换而言之，一个栅格点的人口流动数据会与该栅格附近的人口流行数据相互关联，也会与前后时间段该栅格的人口流动数据相互关联。而具体的关联形式和影响强度，则需要我们利用卷积神经，对历史数据进行学习来发现和记录相应的关联关系。
更进一步地，通过数据洞察注意到，不同栅格网络间人口流动的时间变化曲线往往倾向于若干种固定模式，直观上，商业区，住宅区，办公区域会呈现出不同的人流曲线变化模式。这种模式与地理位置，用地规划，交通路网信息等属性息息相关。本模型后续将进一步讨论不同用地类型的栅格人口流动模式的比较分析。
   TIME TAZID STAY ENTER EXIT     2017-04-05 00:00:00 1009897 460 460 52    2.2 人口栅格数据矢量化 基于一定的空间距离间隔（例如250m），将分析的目标空间划分为若干网格(141*137)。统计T时间内，属于网格M_(p,q)的手机设备停留、进入和离开的数据。按照业务需求，将手机设备数扩样为人口数量，将停留、进入和离开的数据标准化到（0,255）的空间，并将标准化后的数据作为图像的3个颜色通道，据此将T时间的整体网格数据转化为一张三通道格式的图片数据。按照时间维度将经过上述处理的图像作为视频的每一帧图像。
import pandas as pd import numpy as np import h5py # 数据转换成张量类型 data_enter_exit_sz = pd.read_csv(&#39;data/sz/data/TBL_ENTER_EXIT_SZ20170401-20170431.csv&#39;) time_list = data_enter_exit_sz[&#39;TIME&#39;].unique() N = len(time_list) string_to_ix = {string:i for i,string in enumerate(time_list)} tensor_data = np.zeros([N,141,137,3]) for _,line in data_enter_exit_sz.</description>
    </item>
    
    <item>
      <title>Transformer技术详解</title>
      <link>https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Sun, 23 Jun 2019 16:49:10 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/</guid>
      <description>[参考整理]
参考文章：http://jalammar.github.io/illustrated-transformer/
谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。
Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。
在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。
Attention is All You Need：https://arxiv.org/abs/1706.03762
谷歌团队近期提出的用于生成词向量的BERT算法在NLP的11项任务中取得了效果的大幅提升，堪称2018年深度学习领域最振奋人心的消息。而BERT算法的最重要的部分便是本文中提出的Transformer的概念。
正如论文的题目所说的，Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。
作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：
 间片 的计算依赖 时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。  Transformer的提出解决了上面两个问题，首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。

1. Transformer 详解 
1.1 宏观视角认识Transformer整体框架 论文中的验证Transformer的实验室基于机器翻译的，下面我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为如图1：可以看到它是由编码组件、解码组件和它们之间的连接组成。
图1：Transformer用于机器翻译
图2：Transformer的Encoder-Decoder结构
编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
图3：Transformer的Encoder和Decoder均由6个block堆叠而成
所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。
图4：Transformer由self-attention和Feed Forward neural network组成
Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：</description>
    </item>
    
    <item>
      <title>Paddle_hub和pytorch_hub预训练模型迁移学习</title>
      <link>https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Sat, 22 Jun 2019 23:03:09 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</guid>
      <description>[原创]
paddlehub预训练模型迁移学习
https://colab.research.google.com/drive/1PIRRwzCcEVVQ4jAOTm7yJnlduuxM-pLc
pytorch_hub预训练模型迁移学习
https://colab.research.google.com/drive/1oCmWDakpYwLL8AVmAUxr4FLVSHQTTXey</description>
    </item>
    
    <item>
      <title>特征工程方法论</title>
      <link>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/</link>
      <pubDate>Sat, 22 Jun 2019 14:35:17 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/</guid>
      <description>[原创]
欢迎浏览我的Azure Notebooks
https://notebooks.azure.com/messi7125/projects/feature-engineering</description>
    </item>
    
    <item>
      <title>Nlp中的词向量</title>
      <link>https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Sat, 22 Jun 2019 13:46:14 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/</guid>
      <description>[参考整理]
词向量对比：word2vec/glove/fastText/elmo/GPT/bert（非常好）
本文以QA形式对自然语言处理中的词向量进行总结：包含word2vec/glove/fastText/elmo/bert。
目录 一、文本表示和各词向量间的对比
 1、文本表示哪些方法？
 2、怎么从语言模型理解词向量？怎么理解分布式假设？
 3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？
 4、word2vec和NNLM对比有什么区别？（word2vec vs NNLM）
 5、word2vec和fastText对比有什么区别？（word2vec vs fastText）
 6、glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）
 7、elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）
  二、深入解剖word2vec
 1、word2vec的两种模型分别是什么？
 2、word2vec的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？
  三、深入解剖Glove详解
 1、GloVe构建过程是怎样的？
 2、GloVe的训练过程是怎样的？
 3、Glove损失函数是如何确定的？
  四、深入解剖bert（与elmo和GPT比较）
 1、为什么bert采取的是双向Transformer Encoder，而不叫decoder？
 2、elmo、GPT和bert在单双向语言模型处理上的不同之处？
 3、bert构建双向语言模型不是很简单吗？不也可以直接像elmo拼接Transformer decoder吗？
 4、为什么要采取Marked LM，而不直接应用Transformer Encoder？
 5、bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？
  一、文本表示和各词向量间的对比</description>
    </item>
    
  </channel>
</rss>