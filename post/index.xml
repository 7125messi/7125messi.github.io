<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 7125messi的博客</title>
    <link>https://7125messi.github.io/post/</link>
    <description>Recent content in Posts on 7125messi的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>(c) 2018 7125messi.</copyright>
    <lastBuildDate>Sat, 16 Oct 2021 14:34:09 +0800</lastBuildDate>
    
	<atom:link href="https://7125messi.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python并行计算</title>
      <link>https://7125messi.github.io/post/python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/</link>
      <pubDate>Sat, 16 Oct 2021 14:34:09 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/</guid>
      <description>[参考文献] * 《Effective Python：编写高质量Python代码的90个有效方法（原书第2版）》
并发（concurrency）指计算机似乎能在同一时刻做许多件不同的事情。例如，在只配有一个CPU核心的计算机上面，操作系统可以迅速切换这个处理器所运行的程序，因此尽管同一时刻最多只有一个程序在运行，但这些程序能够交替地使用这个核心，从而造成一种假象，让人觉得它们好像真的在同时运行。
并行（parallelism）与并发的区别在于，它强调计算机确实能够在同一时刻做许多件不同的事情。例如，若计算机配有多个CPU核心，那么它就真的可以同时执行多个程序。每个CPU核心执行的都是自己的那个程序之中的指令，这些程序能够同时向前推进。
在同一个程序之中，我们可以利用并发轻松地解决某些类型的问题。例如，并发可以让程序里面出现多条独立的执行路径，每条路径都可以处理它自己的I/O流，这就让我们觉得这些I/O任务好像真的是在各自的路径里面同时向前推进的。
并行与并发之间的区别，关键在于能不能提速（speedup）。如果程序把总任务量分给两条独立的执行路径去同时处理，而且这样做确实能让总时间下降到原来的一半，那么这就是并行，此时的总速度是原来的两倍。反过来说，假如无法实现加速，那即便程序里有一千条独立的执行路径，也只能叫作并发，因为这些路径虽然看起来是在同时推进，但实际上却没有产生相应的提速效果。
Python让我们很容易就能写出各种风格的并发程序。在并发量较小的场合可以使用线程（thread），如果要运行大量的并发函数，那么可以使用协程（coroutine）。
并行任务，可以通过系统调用、子进程与C语言扩展（Cextension）来实现，但要写出真正能够并行的Python代码，其实是很困难的。
● 即便计算机具备多核的CPU，Python线程也无法真正实现并行，因为它们会受全局解释器锁（GIL）牵制。
● 虽然Python的多线程机制受GIL影响，但还是非常有用的，因为我们很容易就能通过多线程模拟同时执行多项任务的效果。
● 多条Python线程可以并行地执行多个系统调用，这样就能让程序在执行阻塞式的I/O任务时，继续做其他运算。
● 虽然Python有全局解释器锁，但开发者还是得设法避免线程之间发生数据争用。
● 把未经互斥锁保护的数据开放给多个线程去同时修改，可能导致这份数据的结构遭到破坏。
● 可以利用threading内置模块之中的Lock类确保程序中的固定关系不会在多线程环境下受到干扰。
● 程序范围变大、需求变复杂之后，经常要用多条路径平行地处理任务。
● fan-out与fan-in是最常见的两种并发协调（concurrency coordination）模式，前者用来生成一批新的并发单元，后者用来等待现有的并发单元全部完工。（分派&amp;ndash;归集）
● Python提供了很多种实现fan-out与fan-in的方案。
但是： ● 每次都手工创建一批线程，是有很多缺点的，例如：创建并运行大量线程时的开销比较大，每条线程的内存占用量比较多，而且还必须采用Lock等机制来协调这些线程。
● 线程本身并不会把执行过程中遇到的异常抛给启动线程或者等待该线程完工的那个人，所以这种异常很难调试。
1 通过线程池 ThreadPoolExecutor 用多线程做并发（提升有限，I/O密集型） Python有个内置模块叫作concurrent.futures，它提供了ThreadPoolExecutor类。 这个类结合了线程（Thread）方案与队列（Queue）方案的优势，可以用来平行地处理 I/O密集型操作。
ThreadPoolExecutor方案仍然有个很大的缺点，就是I/O并行能力不高，即便把max_workers设成100，也无法高效地应对那种有一万多个单元格，且每个单元格都要同时做I/O的情况。如果你面对的需求，没办法用异步方案解决，而是必须执行完才能往后走（例如文件I/O），那么ThreadPoolExecutor是个不错的选择。然而在许多情况下，其实还有并行能力更强的办法可以考虑。
利用ThreadPoolExecutor，我们只需要稍微调整一下代码，就能够并行地执行简单的I/O操作，这种方案省去了每次fan-out（分派）任务时启动线程的那些开销。
虽然ThreadPoolExecutor不像直接启动线程的方案那样，需要消耗大量内存，但它的I/O并行能力也是有限的。因为它能够使用的最大线程数需要提前通过max_workers参数指定。
2 通过线程池 ProcessPoolExecutor 用多进程做并发（I/O、CPU密集型） 从开发者这边来看，这个过程似乎很简单，但实际上，multiprocessing模块与 ProcessPoolExecutor类要做大量的工作才能实现出这样的并行效果。同样的效果，假如改用其他语言来做，那基本上只需要用一把锁或一项原子操作就能很好地协调多个线程，从而实现并行。但这在Python里面不行，所以我们才考虑通过ProcessPoolExecutor来实现。然而这样做的开销很大，因为它必须在上级进程与子进程之间做全套的序列化与反序列化处理。这个方案对那种孤立的而且数据利用度较高的任务来说，比较合适。
● 所谓孤立（isolated），这里指每一部分任务都不需要跟程序里的其他部分共用状态信息。 ● 所谓数据利用度较高（high-leverage），这里指任务所使用的原始材料以及最终所给出的结果数据量都很小，因此上级进程与子进程之间只需要互传很少的信息就行，然而在把原始材料加工成最终产品的过程中，却需要做大量运算。刚才那个求最大公约数的任务就属于这样的例子，当然还有很多涉及其他数学算法的任务，也是如此。
如果你面对的计算任务不具备刚才那两项特征，那么使用ProcessPoolExecutor所引发的开销可能就会盖过因为并行而带来的好处。在这种情况下，我们可以考虑直接使用multiprocessing所提供的一些其他高级功能，例如共享内存（shared memory）、跨进程的锁（cross-process lock）、队列（queue）以及代理（proxy）等。但是，这些功能都相当复杂，即便两个Python线程之间所要共享的进程只有一条，也是要花很大工夫才能在内存空间里面将这些工具安排到位。假如需要共享的进程有很多条，而且还涉及socket，那么这种代码理解起来会更加困难。
总之，不要刚一上来，就立刻使用跟multiprocessing这个内置模块有关的机制，而是可以先试着用ThreadPoolExecutor来运行这种孤立且数据利用度较高的任务。把这套方案实现出来之后，再考虑向ProcessPoolExecutor方案迁移。如果ProcessPoolExecutor方案也无法满足要求，而且其他办法也全都试遍了，那么最后可以考虑直接使用multiprocessing模块里的高级功能来编写代码。
● 把需要耗费大量CPU资源的计算任务改用C扩展模块来写，或许能够有效提高程序的运行速度，同时又让程序里的其他代码依然能够利用Python语言自身的特性。但是，这样做的开销比较大，而且容易引入bug。
● Python自带的multiprocessing模块提供了许多强大的工具，让我们只需要耗费很少的精力，就可以把某些类型的任务平行地放在多个CPU核心上面处理。要想发挥出multiprocessing模块的优势，最好是通过concurrent.futures模块及其ProcessPoolExecutor类来编写代码，因为这样做比较简单。
● 只有在其他方案全都无效的情况下，才可以考虑直接使用multiprocessing里面的高级功能（那些功能用起来相当复杂）。
3 使用Joblib并行运行Python代码（实际工程中比较好用） 对于大多数问题，并行计算确实可以提高计算速度。 随着PC计算能力的提高，我们可以通过在PC中运行并行代码来简单地提升计算速度。Joblib就是这样一个可以简单地将Python代码转换为并行计算模式的软件包，它可非常简单并行我们的程序，从而提高计算速度。
Joblib是一组用于在Python中提供轻量级流水线的工具。 它具有以下功能：</description>
    </item>
    
    <item>
      <title>mmlspark订单量预测案例</title>
      <link>https://7125messi.github.io/post/mmlspark%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B/</link>
      <pubDate>Wed, 19 May 2021 22:36:40 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/mmlspark%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B/</guid>
      <description>本文是前文 Spark预测算法端到端案例的姊妹篇，那一篇用的是普通的lightgbm做的订单量预测，后面随着门店的增加，运行效率不高。
本文主要采用了微软开源的 Microsoft Machine Learning for Apache Spark
https://github.com/Azure/mmlspark
具体用法在上文 Lightgbm在spark实战介绍过，可参考学习。。。
1 门店分类 由于门店数据量巨大，所有门店数据在一起做模型训练，即使在Spark环境下也非常吃性能，所以这里先将门店进行聚类分类，然后在针对同一类型的门店一起训练。
根据 wfm_dw1.wfm_order_channel_half_hour 渠道半小时订单量数据 生成 7天每半小时特征数据 24*2*7
训练 数据:20201130--20210131 预测未来28天数据：20210201--20210228
tstart=&amp;ldquo;20201130&amp;rdquo; tend=&amp;ldquo;20210131&amp;rdquo;
原数据pdf +-------------------+--------+---------+---------+ |global_store_number|sale_day|half_hour|order_qty| +-------------------+--------+---------+---------+ | 28710|20201210| 20| 17.0| | 28710|20201230| 19| 15.0| | 28710|20201211| 13| 19.0| | 28710|20201203| 31| 1.0| | 28710|20201205| 11| 3.0| | 28710|20210111| 19| 14.0| | 28710|20210119| 16| 22.0| | 28710|20210130| 23| 5.0| | 28710|20210107| 16| 30.0| | 28710|20210117| 23| 8.</description>
    </item>
    
    <item>
      <title>Lightgbm在spark实战</title>
      <link>https://7125messi.github.io/post/lightgbm%E5%9C%A8spark%E5%AE%9E%E6%88%98/</link>
      <pubDate>Mon, 26 Apr 2021 22:19:13 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/lightgbm%E5%9C%A8spark%E5%AE%9E%E6%88%98/</guid>
      <description>通常业务中对计算性能有要求时，通常不使用GPU跑tf，会使用xgboost/lightgbm on Spark来解决，既保证速度，准确率也能接受。
LightGBM是使用基于树的学习算法的梯度增强框架。它被设计为分布式且高效的，具有以下优点：
根据官网的介绍 * LigthGBM训练速度更快，效率更高。LightGBM比XGBoost快将近10倍。 * 降低内存使用率。内存占用率大约为XGBoost的1/6。 * 准确性有相应提升。 * 支持并行和GPU学习。 * 能够处理大规模数据。
大部分使用和分析LigthGBM的都是在python单机版本上。要在spark上使用LigthGBM，需要安装微软的MMLSpark包。
MMLSpark可以通过&amp;ndash;packages安装。
spark * &amp;ndash;packages参数
根据jar包的maven地址，使用该包，该参数不常用，因为公司内部的数据平台的集群不一定能联网。 如下示例：
$ bin/spark-shell --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 http://maven.aliyun.com/nexus/content/groups/public/   &amp;ndash;repositories 为该包的maven地址，建议给定，不写则使用默认源。 若依赖多个包，则中间以逗号分隔，类似&amp;ndash;jars 默认下载的包位于当前用户根目录下的.ivy/jars文件夹中 应用场景：本地没有编译好的jar包，集群中服务需要该包的的时候，都是从给定的maven地址，直接下载  MMLSpark用法 1 .MMLSpark可以通&amp;ndash;packages选项方便地安装在现有的Spark集群上，例如:
spark-shell --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 pyspark --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 spark-submit --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1  这也可以在其他Spark contexts中使用，例如，可以通过将MMLSpark添加到.aztk/spark-default.conf文件中来在AZTK中使用MMLSpark。
2 .要在Python(或Conda)安装上尝试MMLSpark，首先通过pip安装PySpark, pip安装PySpark。接下来，使用&amp;ndash;package或在运行时添加包来获取scala源代码
import pyspark spark = pyspark.sql.SparkSession.builder.appName(&amp;quot;MyApp&amp;quot;).\ config(&amp;quot;spark.jars.packages&amp;quot;,&amp;quot;com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1&amp;quot;).\ getOrCreate() import mmlspark  3.python建模使用
# 分类 from mmlspark.lightgbm import LightGBMClassifier model = LightGBMClassifier(learningRate=0.</description>
    </item>
    
    <item>
      <title>Spark预测算法端到端案例</title>
      <link>https://7125messi.github.io/post/spark%E9%A2%84%E6%B5%8B%E7%AE%97%E6%B3%95%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A1%88%E4%BE%8B/</link>
      <pubDate>Sat, 27 Mar 2021 15:28:21 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/spark%E9%A2%84%E6%B5%8B%E7%AE%97%E6%B3%95%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A1%88%E4%BE%8B/</guid>
      <description>最近做完了一个预测算法，数据量巨大，需要分布式环境跑数据，算法本身用的是LightGBM，没什么好说的，主要是怎么用Spark每个driver端跑模型。
1.基础数据  订单数据：主数据，包括渠道订单量和品类销量（day和hour）
wfm_order_quantity_day wfm_order_quantity_half_hour
wfm_sale_quantity_day wfm_sale_quantity_half_hour
渠道：instore、mop(手机下单，店里取)、mod(外卖,打包) （店内、啡快、专星送） 销量预测：品类+销量 单量预测：渠道+订单量
 天气数据
 促销数据
 门店数据：商圈类型 城市级别
 商品数据：商品品类
 预测目标
 算法中除了考虑内部因子（如历史销售、市场促销、周中周末等）以外，还需纳入外部因子（如天气、节假日、季节、偶发事件等） 每家店的每日平均半小时预测准确率应超过75%（1-MAPE） 每家店的每月平均每日预测准确率应超过92%（1-MAPE） 每家店的每日预测准确率应超过80%（1-WMAPE）   2.特征处理  时间类：年月日，星期，小时等 历史统计类：最大，最小，均值，方差，中位数等 促销类：促销类型，粒度等 节假日：工作日，节假日【传统节日，法定节日等】  3.模型加工 一般预测未来N天有三种方式，本项目预测28天：
 循环发 每次预测一天 带入历史特征 滚动预测下一天 性能低下 gap 1-28 28个模型 效率低 分段 直接预测 比较合理  分7 14 21 28四个shift， 组成28天预测模型
预测1-7 采用 shift 7
预测8-14 采用 shift 14
预测15-21 采用 shift 21</description>
    </item>
    
    <item>
      <title>Xgboost_lightgbm_catboost_gridsearch_randomsearch_bayes_opt</title>
      <link>https://7125messi.github.io/post/xgboost_lightgbm_catboost_gridsearch_randomsearch_bayes_opt/</link>
      <pubDate>Thu, 26 Nov 2020 21:16:36 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/xgboost_lightgbm_catboost_gridsearch_randomsearch_bayes_opt/</guid>
      <description>本文主要是提高xgboost/lightgbm/catboost等模型在参数调优方面的工程化实现以及在stacking模型融合使用 * xgboost调优 * lightgbm调优 * catboost调优 * stacking融合 * 网格搜索、随机搜索和贝叶斯优化 * LightGBM各种操作
0 工具类函数 from __future__ import print_function from __future__ import division import numpy as np import pandas as pd import matplotlib matplotlib.use(&amp;quot;Agg&amp;quot;) import matplotlib.pyplot as plt from sklearn.metrics import roc_curve, auc from sklearn.metrics import confusion_matrix from sklearn.metrics import f1_score from itertools import chain import time import os def timer(func): &amp;quot;&amp;quot;&amp;quot;计时器&amp;quot;&amp;quot;&amp;quot; def wrapper(*args, **kwargs): t1 = time.time() func(*args, **kwargs) t2 = time.</description>
    </item>
    
    <item>
      <title>Spark_Hive_RDBMS读写操作</title>
      <link>https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Thu, 10 Sep 2020 22:10:29 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/</guid>
      <description>[项目总结提炼]
前面我们在做数据工程化过程中会大量用到数据的读写操作,现总结如下！！！
主要有以下几个文件：
config.ini 配置文件 func_forecast.sh 工程执行文件 mysql-connector-java-8.0.11.jar MySQL连接jdbc驱动 predict_pred.py 执行主代码 utils.py 工具项代码  1 config.ini 主要定义参数值
[spark] executor_memory = 2g driver_memory = 2g sql_execution_arrow_enabled = true executor_cores = 2 executor_instances = 2 jar = /root/spark-rdbms/mysql-connector-java-8.0.11.jar [mysql] host = 11.23.32.16 port = 3306 db = test user = test password = 123456 [postgresql] host = 11.23.32.16 port = 3306 db = test user = test password = 123456 [dbms_parameters] mysql_conn_info_passwd @@@ =4bI=prodha mysql_conn_info_database = test_db  2 utils.</description>
    </item>
    
    <item>
      <title>数据工程化案例介绍</title>
      <link>https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 03 Aug 2020 21:31:10 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/</guid>
      <description>[原创]
数据工程化案例介绍 好久没写博客了😃😃😃😃😃😃，最近做完了一个偏数据工程的项目，系统的使用了大数据相关组件，学习了Hadoop生态圈技术以及数据仓库相关知识。下面将一些体会写下。。。
1 项目背景和业务特点 XXX医药业务场景：以终端消费者为中心的服务，以门店、连锁加盟、批发模式触达，当前核心竞争力是品牌加盟和供应链采购能力。随着加盟业务快速成长，致力于成为中国最大的零售药房加盟商，需要配套成长的供应链物流能力和信息化建设。“高库存、高退货、高效期” 等环节精细化运营的薄弱是主要原因，具体表现在以下几点：
 (1) 门店补货靠经验，造成了“高退货”; (2) 加盟店和批发商等物流能力尚未触达、物流信息尚未线上化; (3) 与供应商信息沟通均为线下,补货频次较为传统; (4) 采购计划依赖采购员个人经验采用公式计算，未考虑复杂因素;  项目目标是构建以智能补货为智慧大脑的需求驱动的全局、动态、平衡的数字化供应链运营体系，提供安全、高效、精准的供应链能力。主要包括以下部分：
 (1) 数据清洗 (2) 特种工程 (3) 模型训练 (4) 模型融合 (5) 数据工程化  其中前4个部分是机器学习的常见方法和步骤,考虑到线上生产环境要能正常执行,第5部分数据工程化部分启动非常重要的地位,下面对这个部分进行详细的叙述。
2 数据工程化流程架构 这里我们的数据源主要Oracle业务数据以及一些客户提供的人工数据,利用sqoop每日凌晨00:40定时同步数据至Hadoop集群src层。按照经典的Kappa数据仓库分层架构分为:src-&amp;gt;ods-&amp;gt;dw1-&amp;gt;dw2-&amp;gt;app.
与传统的数仓建模不同的是我们主要的目的是利用机器学习方法进行预测补货,数据仓库ods/dw1都是数据清洗和复杂业务逻辑处理,dw2是特征工程处理生成宽表用于训练模型。在数据清洗的过程中会有一些指标用于KPI展示以及app模型预测补货结果我们会同步至MySQL,这些都是作为数据应用服务。
整个数据工程基于Hadoop生态圈技术为载体,数据存储主要是HDFS,数据计算主要是Hive/Spark,元数据管理是Apache Atlas,数据质量分析用的是Apache Griffin,数据任务流调度系统用的是Azkaban,数据OLAP数据库是Presto,数据分析可视化Dashboard用的是Superset。这些大数据组件采用Cloudera Manager(CM)进行统一安装配置,为了保证服务的高可用(HA)采用Zookeeper进行资源调度和管理。
3 数据工程生产环境部署 3.1 可配置项 配置项对于数据工程的重要性不言而喻,可灵活调整部署代码,方便控制管理
├─conf │ │ config.ini 所有可配置参数 │ │ env_name.conf 生产环境和测试环境的标识符 │ │ ini.sh 读写配置文件的函数  例如:
[replenish_parameters] start_date=2020-07-18 end_date=2020-08-31 rolling_day=7 rolling_num=50 [env_parameters] data_base_dir=/data base_dir_jar=/root  这样我们对于每张表的计算添加统一的配置项</description>
    </item>
    
    <item>
      <title>SVM调优详解</title>
      <link>https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 02 Aug 2019 21:21:35 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</guid>
      <description>[原创]
在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲，RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们就需要使用RBF，在享受RBF对非线性数据的良好分类效果前，我们需要对主要的超参数进行选取。本文我们就对scikit-learn中 SVM RBF的调参做一个小结。

1 SVM RBF 主要超参数概述 如果是SVM分类模型，这两个超参数分别是惩罚系数和RBF核函数的系数。当然如果是nu-SVC的话，惩罚系数C代替为分类错误率上限nu, 由于惩罚系数C和分类错误率上限nu起的作用等价，因此本文只讨论带惩罚系数C的分类SVM**

1.1 SVM分类模型 ###（1） 惩罚系数
 惩罚系数C即上一篇里讲到的松弛变量ξ的系数。它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数。
 当惩罚系数C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合。
 当惩罚系数C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单。scikit-learn中默认值是1。
  
（2）RBF核函数的系数 另一个超参数是RBF核函数的参数。回忆下RBF 核函数
γ主要定义了单个样本对整个分类超平面的影响。
 当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量
 当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量**，或者说整个模型的支持向量也会多。scikit-learn中默认值是1/n_features**
  
（3）惩罚系数和RBF核函数的系数 如果把惩罚系数和RBF核函数的系数一起看：
 当C比较大、 γ比较大时，会有更多的支持向量，模型会比较复杂，较容易过拟合 当C比较小、γ比较小时，模型会变得简单，支持向量的个数会少  
1.2 SVM回归模型 SVM回归模型的RBF核比分类模型要复杂一点，因为此时除了惩罚系数C和RBF核函数的系数γ之外，还多了一个损失距离度量ϵ。如果是nu-SVR的话，损失距离度量ϵ代替为分类错误率上限nu，由于损失距离度量ϵ和分类错误率上限nu起的作用等价，因此本文只讨论带距离度量ϵ的回归SVM。
 对于惩罚系数C和RBF核函数的系数γ，回归模型和分类模型的作用基本相同。
 对于损失距离度量ϵ，它决定了样本点到超平面的距离损失.当ϵ比较大时，损失较小，更多的点在损失距离范围之内，模型较简单;当ϵ比较小时，损失函数会较大，模型也会变得复杂；scikit-learn中默认值是0.1
  惩罚系数C、RBF核函数的系数γ和损失距离度量ϵ一起看
 当C比较大、 γ比较大、ϵ比较小时，会有更多的支持向量，模型会比较复杂，容易过拟合一些;
 当C比较小、γ比较小、ϵ比较大时**，模型会变得简单，支持向量的个数会少
  
2 SVM RBF 主要调参方法 对于SVM的RBF核，主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类。
当然也可以使用cross_val_score类来调参，但是个人觉得没有GridSearchCV方便。本文只讨论用GridSearchCV**来进行SVM的RBF核的调参。
将GridSearchCV类用于SVM RBF调参时要注意的参数有：</description>
    </item>
    
    <item>
      <title>RandomForest调优详解</title>
      <link>https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 02 Aug 2019 21:14:54 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</guid>
      <description>原文来自：http://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/

为什么要调整机器学习算法？ 一个月以前，我在kaggle上参加了一个名为TFI的比赛。 我第一次提交的结果在50%。 我不懈努力在特征工程上花了超过2周的时间，勉强达到20%。 出乎我意料的事是，在调整机器学习算法参数之后，我能够达到前10%。
这就是机器学习算法参数调优的重要性。 随机森林是在工业界中使用的最简单的机器学习工具之一。 在我们以前的文章中，我们已经向您介绍了随机森林和和CART模型进行了对比 。 机器学习工具包正由于这些算法的表现而被人所熟知。
随机森林是什么？ 随机森林是一个集成工具，它使用观测数据的子集（BootStraping）和特征变量的子集（随机选择特征变量）来建立一个决策树。 它建立多个这样的决策树，然后将他们合并在一起以获得更准确和稳定的预测。 这样做最直接的事实是，在这一组独立的预测结果中，用投票方式得到一个最高投票结果，这个比单独使用最好模型预测的结果要好。
我们通常将随机森林作为一个黑盒子，输入数据然后给出了预测结果，无需担心模型是如何计算的。这个黑盒子本身有几个我们可以摆弄的杠杆。 每个杠杆都能在一定程度上影响模型的性能或资源时间平衡。 在这篇文章中，我们将更多地讨论我们可以调整的杠杆，同时建立一个随机森林模型。
调整随机森林的参数杠杆 随机森林的参数即可以增加模型的预测能力，又可以使训练模型更加容易。 以下我们将更详细地谈论各个参数（请注意，这些参数，我使用的是Python常规的命名法）：
1.使模型预测更好的特征 主要有3类特征可以被调整，以改善该模型的预测能力

A. max_features： 随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：
 Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。
 sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。
 0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。
   If &amp;ldquo;auto&amp;rdquo;, then max_features=sqrt(n_features).
 If &amp;ldquo;sqrt&amp;rdquo;, then max_features=sqrt(n_features) (same as &amp;ldquo;auto&amp;rdquo;).
 If &amp;ldquo;log2&amp;rdquo;, then max_features=log2(n_features).
 If None, then max_features=n_features.
  max_features如何影响性能和速度？
增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它 同时也降低了单个树的多样性 ，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。</description>
    </item>
    
    <item>
      <title>GBDT调优详解</title>
      <link>https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 02 Aug 2019 21:08:50 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/</guid>
      <description>原文地址：Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python by Aarshay Jain
1.前言 如果一直以来你只把GBM当作黑匣子，只知调用却不明就里，是时候来打开这个黑匣子一探究竟了！
不像bagging算法只能改善模型高方差（high variance）情况，Boosting算法对同时控制偏差（bias）和方差（variance）都有非常好的效果，而且更加高效。
如果你需要同时处理模型中的方差和偏差，认真理解这篇文章一定会对你大有帮助，
本文会用Python阐明GBM算法，更重要的是会介绍如何对GBM调参，而恰当的参数往往能令结果大不相同。

2.目录  Boosing是怎么工作的？
 理解GBM模型中的参数
 学会调参（附详例）
  
3.Boosting是如何工作的？ Boosting可以将一系列弱学习因子（weak learners）相结合来提升总体模型的预测准确度。在任意时间t，根据t-1时刻得到的结果我们给当前结果赋予一个权重。之前正确预测的结果获得较小权重，错误分类的结果得到较大权重。回归问题的处理方法也是相似的。
让我们用图像帮助理解：
 图一： 第一个弱学习因子的预测结果（从左至右）
 一开始所有的点具有相同的权重（以点的尺寸表示）。
 分类线正确地分类了两个正极和五个负极的点。
  图二： 第二个弱学习因子的预测结果
 在图一中被正确预测的点有较小的权重（尺寸较小），而被预测错误的点则有较大的权重。
 这时候模型就会更加注重具有大权重的点的预测结果，即上一轮分类错误的点，现在这些点被正确归类了，但其他点中的一些点却归类错误。
   对图3的输出结果的理解也是类似的。这个算法一直如此持续进行直到所有的学习模型根据它们的预测结果都被赋予了一个权重，这样我们就得到了一个总体上更为准确的预测模型。
现在你是否对Boosting更感兴趣了？不妨看看下面这些文章（主要讨论GBM）：
 Learn Gradient Boosting Algorithm for better predictions (with codes in R)
 Quick Introduction to Boosting Algorithms in Machine Learning</description>
    </item>
    
    <item>
      <title>XGBoost调优指南</title>
      <link>https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 02 Aug 2019 21:05:56 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/</guid>
      <description>原文地址：Complete Guide to Parameter Tuning in XGBoost by Aarshay Jain

1. 简介 如果你的预测模型表现得有些不尽如人意，那就用XGBoost吧。XGBoost算法现在已经成为很多数据工程师的重要武器。它是一种十分精致的算法，可以处理各种不规则的数据。
构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难(至少我觉得十分纠结)。这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。在解决实际问题的时候，有些问题是很难回答的——你需要调整哪些参数？这些参数要调到什么值，才能达到理想的输出？
这篇文章最适合刚刚接触XGBoost的人阅读。在这篇文章中，我们会学到参数调优的技巧，以及XGboost相关的一些有用的知识。以及，我们会用Python在一个数据集上实践一下这个算法。

2. 你需要知道的 XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。在前文章中，基于Python的Gradient Boosting算法参数调整完全指南，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。

3. 内容列表 1、XGBoost的优势 2、理解XGBoost的参数 3、调参示例

4. XGBoost的优势 XGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势：

4.1 正则化  标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。
 实际上，XGBoost以正则化提升(regularized boosting)技术而闻名。  
4.2 并行处理  XGBoost可以实现并行处理，相比GBM有了速度的飞跃。 不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢?每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。(构造决策树的结构时，样本分割点位置，可以使用并行计算)
 XGBoost 也支持Hadoop实现。  
4.3 高度的灵活性  XGBoost 允许用户定义自定义优化目标函数和评价标准 它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。  
4.4 缺失值处理  XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</description>
    </item>
    
    <item>
      <title>LightGBM学习</title>
      <link>https://7125messi.github.io/post/lightgbm%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 02 Aug 2019 20:57:29 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/lightgbm%E5%AD%A6%E4%B9%A0/</guid>
      <description>[参考整理]
1 LightGBM原理
 
1.1 GBDT和 LightGBM对比 GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT 也是各种数据挖掘竞赛的致命武器，据统计 Kaggle 上的比赛有一半以上的冠军方案都是基于 GBDT。
LightGBM（Light Gradient Boosting Machine）同样是一款基于决策树算法的分布式梯度提升框架。为了满足工业界缩短模型计算时间的需求，LightGBM的设计思路主要是两点：
 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据； 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。  由此可见，LightGBM的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具。
XGBoost和LightGBM都是基于决策树提升(Tree Boosting)的工具，都拥有对输入要求不敏感、计算复杂度不高和效果好的特点，适合在工业界中进行大量的应用。
主页地址：http://lightgbm.apachecn.org
LightGBM （Light Gradient Boosting Machine）是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有以下优点：
 更快的训练速度 更低的内存消耗 更好的准确率 分布式支持，可以快速处理海量数据  如下图，在 Higgs 数据集上 LightGBM 比 XGBoost 快将近 10 倍，内存占用率大约为 XGBoost 的1/6，并且准确率也有提升。

1.2 LightGBM 的动机 常用的机器学习算法，例如神经网络等算法，都可以以 mini-batch 的方式训练，训练数据的大小不会受到内存限制。
而 GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的。
 LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。</description>
    </item>
    
    <item>
      <title>特征工程———特征选择的原理和实现</title>
      <link>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Thu, 01 Aug 2019 06:58:37 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</guid>
      <description>[参考总结提炼]
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。
那特征工程是什么？
 特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。
 特征工程又包含了Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等子问题，本文主要讨论特征选择相关的方法及实现。在实际项目中，我们可能会有大量的特征可使用，有的特征携带的信息丰富，有的特征携带的信息有重叠，有的特征则属于无关特征，如果所有特征不经筛选地全部作为训练特征，经常会出现维度灾难问题，甚至会降低模型的准确性。因此，我们需要进行特征筛选，排除无效/冗余的特征，把有用的特征挑选出来作为模型的训练数据。
01 特征选择介绍 1.特征按重要性分类  相关特征：
对于学习任务（例如分类问题）有帮助，可以提升学习算法的效果；
 无关特征：
对于我们的算法没有任何帮助，不会给算法的效果带来任何提升；
 冗余特征：
不会对我们的算法带来新的信息，或者这种特征的信息可以由其他的特征推断出；
  2.特征选择的目的 对于一个特定的学习算法来说，哪一个特征是有效的是未知的。因此，需要从所有特征中选择出对于学习算法有益的相关特征。而且在实际应用中，经常会出现维度灾难问题。如果只选择所有特征中的部分特征构建模型，那么可以大大减少学习算法的运行时间，也可以增加模型的可解释性。
3.特征选择的原则 获取尽可能小的特征子集，不显著降低分类精度、不影响分类分布以及特征子集应具有稳定、适应性强等特点。 
02 特征选择的方法 
1.Filter方法（过滤式） 先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。
主要思想：对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
主要方法：
 Chi-squared test（卡方检验）
 Information gain（信息增益）
 Correlation coefficient scores（相关系数）
  优点：运行速度快，是一种非常流行的特征选择方法。
缺点：无法提供反馈，特征选择的标准规范的制定是在特征搜索算法中完成，学习算法无法向特征搜索算法传递对特征的需求。另外，可能处理某个特征时由于任意原因表示该特征不重要，但是该特征与其他特征结合起来则可能变得很重要。
2.Wrapper方法（封装式） 直接把最后要使用的分类器作为特征选择的评价函数，对于特定的分类器选择最优的特征子集。
主要思想：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA、PSO（如：优化算法-粒子群算法）、DE、ABC（如：优化算法-人工蜂群算法）等。
主要方法：递归特征消除算法。
优点：对特征进行搜索时围绕学习算法展开的，对特征选择的标准规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，真正关注的是学习问题本身。由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。
缺点：运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
3.Embedded方法（嵌入式） 将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。
主要思想：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。
主要方法：用带有L1正则化的项完成特征选择（也可以结合L2惩罚项来优化）、随机森林平均不纯度减少法/平均精确度减少法。
优点：对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于Wrapper方法，比较节省时间。
缺点：运行速度慢。
03 特征选择实现方法一：去掉取值变化小的特征 （Removing features with low variance） 该方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。 考察某个特征下，样本的方差值，可以认为给定一个阈值，抛弃哪些小于某个阈值的特征。</description>
    </item>
    
    <item>
      <title>半参数回归模型</title>
      <link>https://7125messi.github.io/post/%E5%8D%8A%E5%8F%82%E6%95%B0%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 24 Jul 2019 23:42:37 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E5%8D%8A%E5%8F%82%E6%95%B0%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</guid>
      <description>[原创]
常见的处理非线性关系的方法有数据转换方法和神经转换、SVM、投影寻踪和基于树的方法等高计算强度方法。实际在应用回归问题有很大的局限性，模型的可解释性差。使用非参数和半参数回归方法来处理非线性关系一定程度可避免这些问题。
从数据本身出发来估计适合数据本身的函数形式。
 1.忽略非线性的后果  对模型所有的连续自变量都进行检验来判断是否具有非线性作用（xy），通过数据变换解决非线性问题。
 2.数据变换  幂转换：仅能够对正数取值的变量才能使用。所以才使用非参数和半参数回归方法。
 3.非参数和半参数回归方法  从数据本身出发来估计适合数据本身的函数形式。用局部估计取代全局估计。 非参数回归的局部估计是通过数据估计两个变量之间的函数形式，而全局估计通过假设来对函数形式作出规定。 半参数回归模型：利用多元模型把全局估计和局部估计结合起来。 半参数回归模型：广义可加模型（GAM）（特殊：加性模型）。 GAM指自变量为离散或连续变量的半参数回归模型，可对自变量做非参数估计，对一些自变量采取标准的方式估计。 GAM中怀疑具有非线性函数形式的连续自变量可以用非参数估计，而模型中其他变量仍以参数形式估计。 GAM依赖非参数回归，X与Y之间的全局拟合被局部拟合取代，放弃了全部拟合的假设，仍保留了加性的假设。 GAM的加性假设使得模型比神经网络、支持向量机等更容易解释，比完全的参数模型更灵活。 GAM可以适用于许多类型的因变量：连续的、计数的、二分类的，定序的和时间等等。 GAM提供了诊断非线性的框架，简单线性模型和幂转换模型是嵌套在GAM中。 GAM的局部估计可以使用F检验或似然比检验来检验线性，二次项或任何其他幂转换模型的拟合效果。如果半参数回归模型优于线性模型或幂转换模型，它就应该被采用。 半参数回归模型的检验非线性和模型比较作用给予了半参数回归模型的强大功能，对于任意的连续自变量，都应采用半参数方法进行诊断或建模。   1 非参数估计 参数回归与非参数回归的优缺点比较
&amp;gt; 参数模型 &amp;gt; &amp;gt; 优点： &amp;gt; (1)模型形式简单明确，仅由一些参数表达 &amp;gt; (2)在经济中，模型的参数具有一般都具有明确的经济含义 &amp;gt; (3)当模型参数假设成立，统计推断的精度较高，能经受实际检验 &amp;gt; (4)模型能够进行外推运算 &amp;gt; (5)模型可以用于小样本的统计推断 &amp;gt; &amp;gt; 缺点： &amp;gt; (1)回归函数的形式预先假定 &amp;gt; (2)模型限制较多：一般要求样本满足某种分布要求，随机误差满足正态假设，解释变量间独立，解释变量与随机误差不相关等 &amp;gt; (3)需要对模型的参数进行严格的检验推断，步骤较多 &amp;gt; (4)模型泛化能力弱，缺乏稳健性，当模型假设不成立，拟合效果不好，需要修正或者甚至更换模型 &amp;gt; &amp;gt; 非参数模型 &amp;gt; &amp;gt; 优点： &amp;gt; &amp;gt; (1)**回归函数形式自由，受约束少，对数据的分布一般不做任何要求** &amp;gt; (2)**适应能力强，稳健性高，回归模型完全由数据驱动** &amp;gt; (3)**模型的精度高 ;** &amp;gt; (4)**对于非线性、非齐次问题，有非常好的效果**  使用非参数回归时，利用数据来估计F的函数形式。 事先的线性假设被更弱的假设光滑总体函数所代替。这个更弱的假设的代价是两方面的： * 第一，计算方面的代价是巨大的，但考虑到现代计算技术的速度，这不再是大问题； * 第二，失去了一些可解释性，但同时也得到了一个更具代表性的估计。</description>
    </item>
    
    <item>
      <title>Feature_selector实现高效的特征选择</title>
      <link>https://7125messi.github.io/post/feature_selector%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</link>
      <pubDate>Wed, 03 Jul 2019 22:18:02 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/feature_selector%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</guid>
      <description>[参考整理]
具体可以参考我的Google云盘
https://drive.google.com/drive/folders/1cRk1d592TY3IywA6I4hWthks0QTDeTRh
下面简要介绍一下功能介绍
Introduction: Feature Selector Usage 在这个笔记本中，我们将使用FeatureSelector类来选择要从数据集中删除的要素。 此类有五种方法可用于查找要删除的功能：
 具有高missing-values百分比的特征 具有高相关性的特征 对模型预测结果无贡献的特征（即zero importance） 对模型预测结果只有很小贡献的特征（即low importance） 具有单个值的特征（即数据集中该特征取值的集合只有一个元素）  requirements：
lightgbm==2.1.1 matplotlib==2.1.2 seaborn==0.8.1 numpy==1.14.5 pandas==0.23.1 scikit-learn==0.19.1  from feature_selector.selector import FeatureSelector import pandas as pd  Example Dataset 该数据集被用作[Kaggle的[Home Credit Default Risk Competition]竞赛的一部分（https://www.kaggle.com/c/home-credit-default-risk/）。 它适用于受监督的机器学习分类任务，其目标是预测客户是否违约贷款。 整个数据集可以[这里]下载，我们将使用10,000行的小样本。
特征选择器设计用于机器学习任务，但可以应用于任何数据集。基于特征重要性的方法确实需要受监督的机器学习问题。
train = pd.read_csv(&#39;../data/credit_example.csv&#39;) train_labels = train[&#39;TARGET&#39;] train.head()  数据集中有几个分类列。 使用基于特征重要性的方法时，FeatureSelector使用独热编码处理这些。
train = train.drop(columns = [&#39;TARGET&#39;]) train.head()  Implementation FeatureSelector有五个函数用于识别要删除的列：
 identify_missing identify_single_unique identify_collinear identify_zero_importance identify_low_importance  这些方法根据指定的标准查找要删除的功能。 标识的特征存储在FeatureSelector的ops属性（Python字典）中。 我们可以手动删除已识别的功能，或使用FeatureSelector中的remove功能来实际删除功能。</description>
    </item>
    
    <item>
      <title>UER Py高质量中文BERT预训练模型</title>
      <link>https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 30 Jun 2019 21:26:33 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/uer-py%E9%AB%98%E8%B4%A8%E9%87%8F%E4%B8%AD%E6%96%87bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</guid>
      <description>[原创]
最近在github看到了一个项目:
https://github.com/dbiir/UER-py
貌似是腾讯出品，良心之作，特地去Colab撸了一下，确实可以，封装了很多模块，后续估计还要迭代，直至打成包就更好了。
有兴趣的同学可以去我的Colab上，玩一下。
https://colab.research.google.com/drive/1N81AYxPolDWPMdbZpYO1NnfV4T403Bxz</description>
    </item>
    
    <item>
      <title>基于UNet神经网络的城市人流预测</title>
      <link>https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/</link>
      <pubDate>Mon, 24 Jun 2019 21:34:14 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E5%9F%BA%E4%BA%8Eunet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%8E%E5%B8%82%E4%BA%BA%E6%B5%81%E9%A2%84%E6%B5%8B/</guid>
      <description>[原创]
1 利用手机信令数据计算人口流动数据 手机信令数据是研究人口的整体流动情况的重要数据来源。移动运营商在为手机用户提供实时通讯服务时，积累了大量的基站与设备的服务配对数据。根据配对和唤醒发生的时间以及基站的地理位置，可以很自然划定一定时间和空间范围，统计每一个时间范围内在特定空间区域内手机设备的停留，进入和离开数据，并据此估算相应的人流数据。
传统的人口网格分析过程一般只关注单个网格内某个时间点人口流动的截面数据以及城市当中不同区域的人口分布统计情况；没有将时间和空间融合考虑，不能对城市整体的人口流动形成一个完整连续的直观描述。但是，作为城市安全运营管理者和规划人员职责是需要把握好城市人口流动规律，建立有效的时空数据分析模型，从而为城市安全运行管理做好相应的人口短时预测与应急管理服务。 2 人口流动数据网格图像流处理 2.1 流处理思路 原始手机信令数据，按照一定的时间间隔（例如15分钟），划分出每个时间段的信令数据情况。主要包括：时间，格网ID，格网中心点经度，格网中心点维度，时间段内格网的停留人数，进入人数，离开人数。
根据原始数据的时空关系，将原始数据转化为4维向量空间矩阵，维度分别为时间维度、空间维度横坐标，空间维度纵坐标以及停留，进入或离开的类别：Matrix[t,i,j,k]=p意味着在t时刻，第i行第j列的空间栅格位置，k=0时则停留人数为p，k=1时则进入人数为p，k=2时则离开人数为p。
在这样的转换关系下，可以将源数据处理为3通道的时空数据。考虑到单个人员流动的时空连续性，可以认为表示人口流通的整体统计量的时空矩阵也具备一定局部关联性，换而言之，一个栅格点的人口流动数据会与该栅格附近的人口流行数据相互关联，也会与前后时间段该栅格的人口流动数据相互关联。而具体的关联形式和影响强度，则需要我们利用卷积神经，对历史数据进行学习来发现和记录相应的关联关系。
更进一步地，通过数据洞察注意到，不同栅格网络间人口流动的时间变化曲线往往倾向于若干种固定模式，直观上，商业区，住宅区，办公区域会呈现出不同的人流曲线变化模式。这种模式与地理位置，用地规划，交通路网信息等属性息息相关。本模型后续将进一步讨论不同用地类型的栅格人口流动模式的比较分析。
   TIME TAZID STAY ENTER EXIT     2017-04-05 00:00:00 1009897 460 460 52    2.2 人口栅格数据矢量化 基于一定的空间距离间隔（例如250m），将分析的目标空间划分为若干网格(141*137)。统计T时间内，属于网格M_(p,q)的手机设备停留、进入和离开的数据。按照业务需求，将手机设备数扩样为人口数量，将停留、进入和离开的数据标准化到（0,255）的空间，并将标准化后的数据作为图像的3个颜色通道，据此将T时间的整体网格数据转化为一张三通道格式的图片数据。按照时间维度将经过上述处理的图像作为视频的每一帧图像。
import pandas as pd import numpy as np import h5py # 数据转换成张量类型 data_enter_exit_sz = pd.read_csv(&#39;data/sz/data/TBL_ENTER_EXIT_SZ20170401-20170431.csv&#39;) time_list = data_enter_exit_sz[&#39;TIME&#39;].unique() N = len(time_list) string_to_ix = {string:i for i,string in enumerate(time_list)} tensor_data = np.zeros([N,141,137,3]) for _,line in data_enter_exit_sz.</description>
    </item>
    
    <item>
      <title>特征工程方法论</title>
      <link>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/</link>
      <pubDate>Sat, 22 Jun 2019 14:35:17 +0800</pubDate>
      
      <guid>https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/</guid>
      <description>[原创]
欢迎浏览我的Google云盘
https://drive.google.com/drive/folders/1XHvA5hoMNfBmfvFllD9-KUYY58KjB4v4?usp=sharing</description>
    </item>
    
  </channel>
</rss>