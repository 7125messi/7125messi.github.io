<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Nlp中的词向量  &middot; 7125messi的博客</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Nlp中的词向量  &middot; 7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2019-06-22T13:46:14&#43;08:00" />
<meta property="og:article:modified_time" content="2019-06-22T13:46:14&#43;08:00" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Nlp中的词向量" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/" />
<meta name="twitter:domain" content="https://7125messi.github.io">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Nlp中的词向量",
    "author": {
      "@type": "Person",
      "name": ""
    },
    "datePublished": "2019-06-22",
    "description": "",
    "wordCount":  383 
  }
</script>



<link rel="canonical" href="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Nlp中的词向量
</h1>

  <div class="metas">
<time datetime="2019-06-22">22 Jun, 2019</time>


  
  &middot; Read in about 2 min
  &middot; (383 Words)
  <br>
  


</div>

</header>

  <div class="container content">
  

<p>[参考整理]</p>

<p>词向量对比：word2vec/glove/fastText/elmo/GPT/bert（非常好）</p>

<p>本文以QA形式对自然语言处理中的词向量进行总结：包含word2vec/glove/fastText/elmo/bert。</p>

<h1 id="目录">目录</h1>

<p>一、文本表示和各词向量间的对比</p>

<ul>
<li><p>1、文本表示哪些方法？</p></li>

<li><p>2、怎么从语言模型理解词向量？怎么理解分布式假设？</p></li>

<li><p>3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？</p></li>

<li><p>4、word2vec和NNLM对比有什么区别？（word2vec vs NNLM）</p></li>

<li><p>5、word2vec和fastText对比有什么区别？（word2vec vs fastText）</p></li>

<li><p>6、glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）</p></li>

<li><p>7、elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）</p></li>
</ul>

<p>二、深入解剖word2vec</p>

<ul>
<li><p>1、word2vec的两种模型分别是什么？</p></li>

<li><p>2、word2vec的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？</p></li>
</ul>

<p>三、深入解剖Glove详解</p>

<ul>
<li><p>1、GloVe构建过程是怎样的？</p></li>

<li><p>2、GloVe的训练过程是怎样的？</p></li>

<li><p>3、Glove损失函数是如何确定的？</p></li>
</ul>

<p>四、深入解剖bert（与elmo和GPT比较）</p>

<ul>
<li><p>1、为什么bert采取的是双向Transformer Encoder，而不叫decoder？</p></li>

<li><p>2、elmo、GPT和bert在单双向语言模型处理上的不同之处？</p></li>

<li><p>3、bert构建双向语言模型不是很简单吗？不也可以直接像elmo拼接Transformer decoder吗？</p></li>

<li><p>4、为什么要采取Marked LM，而不直接应用Transformer Encoder？</p></li>

<li><p>5、bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？</p></li>
</ul>

<h1 id="一-文本表示和各词向量间的对比">一、文本表示和各词向量间的对比</h1>

<p><a name="Bu9WM"></a></p>

<h2 id="1-文本表示哪些方法">1、文本表示哪些方法？</h2>

<p>下面对文本表示进行一个归纳，也就是对于一篇文本可以如何用数学语言表示呢？</p>

<ul>
<li><strong>基于one-hot、tf-idf、textrank等的bag-of-words；</strong></li>
<li>主题模型：LSA（SVD）、pLSA、LDA；</li>
<li><strong>基于词向量的固定表征：word2vec、fastText、glove</strong></li>
<li><strong>基于词向量的动态表征：elmo、GPT、bert</strong></li>
</ul>

<p><a name="3vpiR"></a></p>

<h2 id="2-怎么从语言模型理解词向量-怎么理解分布式假设">2、怎么从语言模型理解词向量？怎么理解分布式假设？</h2>

<p>上面给出的4个类型也是nlp领域最为常用的文本表示了，文本是由每个单词构成的，而谈起词向量，one-hot是可认为是最为简单的词向量，<strong>但存在维度灾难和语义鸿沟等问题</strong>；通过构建共现矩阵并利用SVD求解构建词向量，<strong>则计算复杂度高</strong>；而早期词向量的研究通常来源于<strong>语言模型，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363640-7e5aeb39-8de7-4b0e-8ad1-4d012ac607ab.jpeg#align=left&amp;display=inline&amp;height=385&amp;originHeight=385&amp;originWidth=433&amp;size=0&amp;status=done&amp;width=433" alt="" />NNLM</p>

<p>所谓分布式假设，用一句话可以表达：<strong>相同上下文语境的词有似含义</strong>。而由此引申出了<strong>word2vec、fastText</strong>，在此类词向量中，虽然其本质仍然是语言模型，但是<strong>它的目标并不是语言模型本身，而是词向量，其所作的一系列优化，都是为了更快更好的得到词向量</strong>。<strong>glove则是基于全局语料库、并结合上下文语境构建词向量，结合了LSA和word2vec的优点。</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363643-1a700053-0888-423b-b211-d2d2e16a7ef7.jpeg#align=left&amp;display=inline&amp;height=189&amp;originHeight=189&amp;originWidth=602&amp;size=0&amp;status=done&amp;width=602" alt="" /></p>

<p><a name="3TvXC"></a></p>

<h2 id="3-传统的词向量有什么问题-怎么解决-各种词向量的特点是什么">3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？</h2>

<p><strong>上述方法得到的</strong><strong>词向量是固定表征的，无法解决一词多义等问题</strong>，如“川普”。为此引入<strong>基于语言模型的动态表征方法：elmo、GPT、bert。</strong></p>

<p><strong>各种词向量的特点：</strong><br />（1）One-hot 表示 ：维度灾难、语义鸿沟；<br />（2）分布式表示 (distributed representation) ：</p>

<ul>
<li><strong>矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；</strong></li>
<li><strong>基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；</strong></li>
<li><strong>word2vec、fastText：优化效率高，但是基于局部语料；</strong></li>
<li><strong>glove：基于全局预料，结合了LSA和word2vec的优点；</strong></li>
<li><strong>elmo、GPT、bert：动态特征；</strong></li>
</ul>

<p><a name="fAlJs"></a></p>

<h2 id="4-word2vec和nnlm对比有什么区别-word2vec-vs-nnlm">4、word2vec和NNLM对比有什么区别？（word2vec vs NNLM）</h2>

<p>1）其本质都可以看作是<strong>语言模型</strong>；<br />2）词向量只不过NNLM一个产物，word2vec虽然其本质也是语言模型，但是其专注于<strong>词向量本身</strong>，因此做了许多<strong>优化来提高计算效率</strong>：</p>

<ul>
<li>与NNLM相比，词<strong>向量直接sum，不再拼接，并舍弃隐层；</strong></li>
<li><strong>考虑到sofmax归一化需要遍历整个词汇表</strong>，采用<strong>hierarchical softmax 和negative sampling</strong>进行优化，<strong>hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路径变小；negative sampling更为直接，实质上对每一个样本中每一个词都进行负例采样；</strong></li>
</ul>

<p><a name="sKrv0"></a></p>

<h2 id="5-word2vec和fasttext对比有什么区别-word2vec-vs-fasttext">5、word2vec和fastText对比有什么区别？（word2vec vs fastText）</h2>

<p>1）都可以<strong>无监督学习词向量</strong>， <strong>fastText训练词向量时会考虑subword</strong>；<br />2） fastText还可以进行<strong>有监督学习进行文本分类</strong>，其主要特点：</p>

<ul>
<li>结构与CBOW类似<strong>，但学习目标是人工标注的分类结果；</strong></li>
<li>采用hierarchical softmax对输出的分类标签建立哈夫曼树，<strong>样本中标签多的类别被分配短的搜寻路径；</strong></li>
<li>引入<strong>N-gram，考虑词序特征</strong>；</li>
<li>引入<strong>subword来处理长词，处理未登陆词问题</strong>；</li>
</ul>

<p><a name="gUiej"></a></p>

<h2 id="6-glove和word2vec-lsa对比有什么区别-word2vec-vs-glove-vs-lsa">6、glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）</h2>

<p>1）<strong>glove vs LSA</strong></p>

<ul>
<li>LSA（Latent Semantic Analysis）可以<strong>基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高；</strong></li>
<li>glove可看作是对<strong>LSA一种优化的高效矩阵分解算法，采用Adagrad对最小平方损失进行优化；</strong></li>
</ul>

<p>2）<strong>word2vec vs glove</strong></p>

<ul>
<li>word2vec是<strong>局部语料库训练的，其特征提取是基于滑窗的</strong>；而<strong>glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率</strong>；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。</li>
<li>word2vec是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数<img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363672-37071e33-1934-4617-8b3a-0329c4c8d63b.svg#align=left&amp;display=inline&amp;height=24&amp;originHeight=27&amp;originWidth=73&amp;size=0&amp;status=done&amp;width=65" alt="" />。</li>
<li>word2vec损失函数实质上是<strong>带权重的交叉熵，权重固定</strong>；glove的损<strong>失函数是最小平方损失函数，权重可以做映射变换。</strong></li>
<li>总体来看，<strong>glove可以被看作是更换了目标函数和权重函数的全局word2vec</strong>。</li>
</ul>

<p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363653-722b72ee-8604-411f-9dbc-ad3d55dbb0f8.jpeg#align=left&amp;display=inline&amp;height=366&amp;originHeight=191&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=1380" alt="" />
<a name="G2Kmh"></a></p>

<h2 id="7-elmo-gpt-bert三者之间有什么区别-elmo-vs-gpt-vs-bert">7、 elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）</h2>

<p>之前介绍词向量均是<strong>静态的词向量，无法解决一次多义等问题</strong>。下面介绍三种elmo、GPT、bert词向量，它们都是<strong>基于语言模型的动态词向量</strong>。下面从几个方面对这三者进行对比：<br />（1）<strong>特征提取器</strong>：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。<br />很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。<br />（2）<strong>单/双向语言模型</strong>：</p>

<ul>
<li>GPT采用单向语言模型，elmo和bert采用双向语言模型。<strong>但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</strong></li>

<li><p><strong>GPT和bert都采用Transformer，Transformer是encoder-decoder结构</strong>，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；<strong>bert的双向语言模型则采用encoder部分，采用了完整句子。</strong>
<a name="fCwGA"></a></p>

<h1 id="二-深入解剖word2vec">二、深入解剖word2vec</h1>

<p><a name="Okf2d"></a></p>

<h2 id="1-word2vec的两种模型分别是什么">1、word2vec的两种模型分别是什么？</h2>

<p>word2Vec 有两种模型：<strong>CBOW</strong> 和 <strong>Skip-Gram：</strong></p></li>

<li><p>CBOW 在已知 <code>context(w)</code> 的情况下，预测 <code>w</code>；</p></li>

<li><p>Skip-Gram在已知 <code>w</code> 的情况下预测 <code>context(w)</code> ；</p></li>
</ul>

<p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363643-55acfce6-95ff-47e7-ab6c-598f69f88930.jpeg#align=left&amp;display=inline&amp;height=352&amp;originHeight=352&amp;originWidth=612&amp;size=0&amp;status=done&amp;width=612" alt="" />word2vec<br />与NNLM相比，word2vec的主要目的是生成词向量而不是语言模型，在CBOW中，投射层将词向量直接相加而不是拼接起来，并舍弃了隐层，这些牺牲都是为了减少计算量，使训练更加</p>

<p><a name="RTxY4"></a></p>

<h2 id="2-word2vec的两种优化方法是什么-它们的目标函数怎样确定的-训练过程又是怎样的">2、word2vec的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？</h2>

<p>不经过优化的CBOW和Skip-gram中 ,在每个样本中每个词的训练过程都要遍历整个词汇表，也就是都需要经过softmax归一化，计算误差向量和梯度以更新两个词向量矩阵（这两个词向量矩阵实际上就是最终的词向量，可认为初始化不一样），当语料库规模变大、词汇表增长时，训练变得不切实际。为了解决这个问题，word2vec支持两种优化方法：<strong>hierarchical softmax </strong>和<strong>negative sampling</strong>。此部分仅做关键介绍，数学推导请仔细阅读《<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/itplus/article/details/37969519">word2vec 中的数学原理详解</a>》。</p>

<p><strong>（1）基于hierarchical softmax 的 CBOW 和 Skip-gram</strong><br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363664-a6333afd-c5c1-44a4-922a-d05f7eb1d302.jpeg#align=left&amp;display=inline&amp;height=393&amp;originHeight=328&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=862" alt="" />基于hierarchical softmax 的 CBOW 和 Skip-gram</p>

<p>hierarchical softmax 使用一颗二叉树表示词汇表中的单词，每个单词都作为二叉树的叶子节点。对于一个大小为V的词汇表，其对应的二叉树包含V-1非叶子节点。假如每个非叶子节点向左转标记为1，向右转标记为0，那么每个单词都具有唯一的从根节点到达该叶子节点的由｛0 1｝组成的代号（<strong>实际上为哈夫曼编码，为哈夫曼树，是带权路径长度最短的树，哈夫曼树保证了词频高的单词的路径短，词频相对低的单词的路径长，这种编码方式很大程度减少了计算量</strong>）。</p>

<p>CBOW中的目标函数是使条件概率 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363708-0581d1e5-924e-4208-92bd-2e82df2c4a9f.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=153&amp;size=0&amp;status=done&amp;width=136" alt="" /> 最大化，其等价于：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363679-aaa1f9c7-d6cb-44dd-9ecd-63ff85d10281.jpeg#align=left&amp;display=inline&amp;height=121&amp;originHeight=121&amp;originWidth=511&amp;size=0&amp;status=done&amp;width=511" alt="" /></p>

<p>Skip-gram中的目标函数是使条件概率 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363779-5eda187a-4dbe-4aeb-b365-569b277a9b21.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=153&amp;size=0&amp;status=done&amp;width=136" alt="" /> 最大化，其等价于：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363691-c51f80ac-1914-473f-8287-474599aa4908.jpeg#align=left&amp;display=inline&amp;height=127&amp;originHeight=127&amp;originWidth=660&amp;size=0&amp;status=done&amp;width=660" alt="" /></p>

<p><strong>（2）基于negative sampling的 CBOW 和 Skip-gram</strong><br />negative sampling是一种不同于hierarchical softmax的优化策略，相比于hierarchical softmax，negative sampling的想法更直接——<strong>为每个训练实例都提供负例。</strong></p>

<p>对于<strong>CBOW</strong>，其目标函数是最大化：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363711-67828f57-7836-4f78-8d1b-62e66e848c36.jpeg#align=left&amp;display=inline&amp;height=58&amp;originHeight=58&amp;originWidth=320&amp;size=0&amp;status=done&amp;width=320" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1559659363705-fc78883a-356c-4ab3-993f-1a27cf7ee433.png#align=left&amp;display=inline&amp;height=49&amp;originHeight=49&amp;originWidth=437&amp;size=0&amp;status=done&amp;width=437" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363775-783485d6-b9cf-47e2-9ef6-16bd7a057072.jpeg#align=left&amp;display=inline&amp;height=158&amp;originHeight=158&amp;originWidth=619&amp;size=0&amp;status=done&amp;width=619" alt="" /></p>

<p>对于<strong>Skip-gram</strong>，同样也可以得到其目标函数是最大化：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363758-b83e3f22-36bb-4a6e-a1b8-d8c3861b757f.jpeg#align=left&amp;display=inline&amp;height=61&amp;originHeight=61&amp;originWidth=338&amp;size=0&amp;status=done&amp;width=338" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1559659363719-bdcc6198-2ef3-4b1b-a6dd-d0ea05342cf6.png#align=left&amp;display=inline&amp;height=39&amp;originHeight=39&amp;originWidth=424&amp;size=0&amp;status=done&amp;width=424" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363720-80601554-1c08-41ff-a145-eea92054665c.jpeg#align=left&amp;display=inline&amp;height=191&amp;originHeight=191&amp;originWidth=693&amp;size=0&amp;status=done&amp;width=693" alt="" /></p>

<p>负采样算法实际上就是一个<strong>带权采样</strong>过程，负例的选择机制是和单词词频联系起来的。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1559659363728-e11e1044-d1f2-4d72-a5b5-a3122c4a2f5e.png#align=left&amp;display=inline&amp;height=56&amp;originHeight=56&amp;originWidth=359&amp;size=0&amp;status=done&amp;width=359" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1559659363742-3aafa5f4-19c7-48a1-8d46-b57b6035b088.png#align=left&amp;display=inline&amp;height=18&amp;originHeight=18&amp;originWidth=425&amp;size=0&amp;status=done&amp;width=425" alt="" /></p>

<p>具体做法是以 <code>N+1</code> 个点对区间 <code>[0,1]</code> 做非等距切分，并引入的一个在区间 <code>[0,1]</code> 上的 <code>M</code> 等距切分，其中 <code>M &gt;&gt; N。</code>源码中取 M = 10^8。然后对两个切分做投影，得到映射关系：采样时，每次生成一个 [1, M-1] 之间的整数 i，则 Table(i) 就对应一个样本；当采样到正例时，跳过（<strong>拒绝采样</strong>）。<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363724-f3e5f13b-61a3-40d8-b174-5fa45d59ceb4.jpeg#align=left&amp;display=inline&amp;height=174&amp;originHeight=174&amp;originWidth=528&amp;size=0&amp;status=done&amp;width=528" alt="" /></p>

<p><a name="3Tn5y"></a></p>

<h1 id="三-深入解剖glove详解">三、深入解剖Glove详解</h1>

<p>GloVe的全称叫Global Vectors for Word Representation，它是一个基于&gt; <strong>全局词频统计</strong>（count-based &amp; overall statistics）的词表征（word representation）工具。</p>

<p>官网在此 <a href="http://nlp.stanford.edu/projects/glove/">http://nlp.stanford.edu/projects/glove/</a><br />官方的代码的GitHub在此 ： <a href="https://github.com/stanfordnlp/GloVe">https://github.com/stanfordnlp/GloVe</a></p>

<p>正如论文的标题而言，GloVe的全称叫Global Vectors for Word Representation，它是一个基于全局词频统计（count-based &amp; overall statistics）的<strong>词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。</strong></p>

<p><a name="NOq11"></a></p>

<h2 id="1-glove构建过程是怎样的">1、GloVe构建过程是怎样的？</h2>

<p>（1）根据<strong>语料库构建一个共现矩阵</strong>，矩阵中的每一个元素 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363783-09bb55d6-b0d8-4754-86b8-301c3292afd2.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=31&amp;size=0&amp;status=done&amp;width=27" alt="" /> 代表单词 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363801-c1c578a1-ae93-420f-82cb-2377db48e4d7.svg#align=left&amp;display=inline&amp;height=17&amp;originHeight=20&amp;originWidth=7&amp;size=0&amp;status=done&amp;width=6" alt="" /> 和上下文单词 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363778-b0d9fd06-8fe4-45a9-9326-b34ca5e799b3.svg#align=left&amp;display=inline&amp;height=20&amp;originHeight=23&amp;originWidth=9&amp;size=0&amp;status=done&amp;width=8" alt="" /> 在特定大小的上下文窗口内共同出现的次数。</p>

<p>（2）<strong>构建词向量（Word Vector）</strong><strong>和</strong><strong>共现矩阵（Co-ocurrence Matrix）之间的近似关系。</strong></p>

<p><img src="https://cdn.nlark.com/yuque/__latex/cecd3daebc73aca5cf4eec9b484bf584.svg#card=math&amp;code=w_%7Bi%7D%5E%7BT%7D%20%5Coverline%7Bw%7D_%7Bj%7D%2Bb_%7Bi%7D%2B%5Ctilde%7Bb%7D_%7Bj%7D%3D%5Clog%20%5Cleft%28X_%7Bi%20j%7D%5Cright%29&amp;height=24&amp;width=186" alt="" /></p>

<p>其中，<img src="https://cdn.nlark.com/yuque/__latex/02929915b6f3e4584ec2a19f933d42cb.svg#card=math&amp;code=w_%7Bi%7D%5E%7BT%7D&amp;height=24&amp;width=21" alt="" /> <strong>和<img src="https://cdn.nlark.com/yuque/__latex/c856e72b5c14540910d8ab67da9c0c1e.svg#card=math&amp;code=%5Ctilde%7Bw%7D_%7Bj%7D&amp;height=24&amp;width=18" alt="" />是我们最终要求解的词向量；</strong><img src="https://cdn.nlark.com/yuque/__latex/d0d8bfa8a8f9d6dc902d34751807a05e.svg#card=math&amp;code=b_%7Bi%7D%5E%7BT%7D&amp;height=24&amp;width=17" alt="" /> <strong>和<img src="https://cdn.nlark.com/yuque/__latex/45d4892b2623694cafd20829964f78e2.svg#card=math&amp;code=%5Ctilde%7Bb%7D_%7Bj%7D&amp;height=24&amp;width=15" alt="" /></strong>分别是两个词向量的bias term。</p>

<p>目标函数为： </p>

<p><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363820-c79f59b2-22dc-47f4-80ed-6384101d99a5.svg#align=left&amp;display=inline&amp;height=61&amp;originHeight=69&amp;originWidth=402&amp;size=0&amp;status=done&amp;width=357" alt="" /> </p>

<p>这个loss function的基本形式就是最简单的<strong>mean square loss</strong>，只不过在此基础上加了一个权重函数 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363816-d1ab79c1-1c6a-4a11-8632-3360352c4564.svg#align=left&amp;display=inline&amp;height=24&amp;originHeight=27&amp;originWidth=53&amp;size=0&amp;status=done&amp;width=47" alt="" /> ，那么这个函数起了什么作用，为什么要添加这个函数呢？我们知道<strong>在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的（frequent co-occurrences）</strong>，那么我们希望：</p>

<p>1.这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是<strong>非递减函数（non-decreasing）</strong>；</p>

<p>2.但我们<strong>也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加；</strong></p>

<p>3.<strong>如果两个单词没有在一起出现</strong>，也就是<img src="https://cdn.nlark.com/yuque/__latex/935a651a3ddaf601ecda7b685e4ab0de.svg#card=math&amp;code=X_%7Bi%20j%7D%3D0&amp;height=24&amp;width=54" alt="" />，那么他们应该不参与到loss function的计算当中去，也就是f(x)要满足f(0)=0</p>

<p>满足以上两个条件的函数有很多，<strong>作者采用了如下形式的分段函数</strong>：</p>

<p><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363820-c01c60d6-556f-47c8-bfdc-f734886988a5.svg#align=left&amp;display=inline&amp;height=50&amp;originHeight=56&amp;originWidth=302&amp;size=0&amp;status=done&amp;width=268" alt="" /></p>

<p>根据实验发现 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363798-635e4766-6269-4e3d-8861-48f1e9af0239.svg#align=left&amp;display=inline&amp;height=16&amp;originHeight=18&amp;originWidth=42&amp;size=0&amp;status=done&amp;width=37" alt="" /> 的值对结果的影响并不是很大，原作者采用了 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363821-329b1f85-a1d3-4a3a-b374-08eadc445272.svg#align=left&amp;display=inline&amp;height=20&amp;originHeight=23&amp;originWidth=101&amp;size=0&amp;status=done&amp;width=90" alt="" /> 。而 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363871-221461d2-ac88-41c5-ac3f-a313baf62fae.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=73&amp;size=0&amp;status=done&amp;width=65" alt="" />时的结果要比 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363826-011d8f2a-86d1-429a-a22b-2ccbcefc362a.svg#align=left&amp;display=inline&amp;height=18&amp;originHeight=20&amp;originWidth=52&amp;size=0&amp;status=done&amp;width=46" alt="" /> 时要更好。下面是 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363820-dcf0bc32-1ce7-4d3a-8d9f-b49fd3e58a34.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=73&amp;size=0&amp;status=done&amp;width=65" alt="" /> 时 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363840-d843c0be-fb10-4fef-906d-0bf0cf4d5bfc.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=40&amp;size=0&amp;status=done&amp;width=35" alt="" /> 的函数图象，可以看出对于较小的 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363833-141277df-d797-425e-8beb-d40e53467371.svg#align=left&amp;display=inline&amp;height=23&amp;originHeight=26&amp;originWidth=31&amp;size=0&amp;status=done&amp;width=27" alt="" /> ，权值也较小。这个函数图像如下所示：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363803-ae9e67de-9c7b-429d-a7de-f466b3a9f5f2.jpeg#align=left&amp;display=inline&amp;height=296&amp;originHeight=296&amp;originWidth=469&amp;size=0&amp;status=done&amp;width=469" alt="" /></p>

<p><a name="5Ybq1"></a></p>

<h2 id="2-glove的训练过程是怎样的">2、GloVe的训练过程是怎样的？</h2>

<ol>
<li><strong>实质上还是监督学习：虽然glove不需要人工标注为无监督学习，但实质还是有label就是 </strong><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363841-b8ca677a-2c5c-4d60-a110-9decbeca164f.svg#align=left&amp;display=inline&amp;height=24&amp;originHeight=27&amp;originWidth=73&amp;size=0&amp;status=done&amp;width=65" alt="" /><strong> </strong>。</li>
<li>向量 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363860-3799ae9f-ed65-4d95-9513-1bfc26bdd57a.svg#align=left&amp;display=inline&amp;height=13&amp;originHeight=15&amp;originWidth=15&amp;size=0&amp;status=done&amp;width=13" alt="" /> 和 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363884-31a8dea5-b503-43ea-b671-f670b49416ea.svg#align=left&amp;display=inline&amp;height=17&amp;originHeight=20&amp;originWidth=15&amp;size=0&amp;status=done&amp;width=13" alt="" />为学习参数，本质上与监督学习的训练方法一样，采用了AdaGrad的梯度下降算法，对矩阵 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363864-b0028a1d-795c-4cb4-8466-82829e7473e6.svg#align=left&amp;display=inline&amp;height=18&amp;originHeight=20&amp;originWidth=18&amp;size=0&amp;status=done&amp;width=16" alt="" /> 中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。</li>
<li>最终学习得到的是<strong>两个词向量是 </strong><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363850-e6a9429a-2836-4661-8eff-353185604226.svg#align=left&amp;display=inline&amp;height=17&amp;originHeight=20&amp;originWidth=15&amp;size=0&amp;status=done&amp;width=13" alt="" /><strong> 和 </strong><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363859-60c984b7-9f87-4020-86c3-f781590ca6bc.svg#align=left&amp;display=inline&amp;height=13&amp;originHeight=15&amp;originWidth=15&amp;size=0&amp;status=done&amp;width=13" alt="" /><strong> ，因为 </strong><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363852-f4929fc8-cbb0-4277-a245-d8dfde623311.svg#align=left&amp;display=inline&amp;height=18&amp;originHeight=20&amp;originWidth=18&amp;size=0&amp;status=done&amp;width=16" alt="" /><strong> 是对称的（symmetric）</strong>，所以从原理上讲<img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363890-d46f896e-d57d-46cb-b947-fcdebf31d222.svg#align=left&amp;display=inline&amp;height=17&amp;originHeight=20&amp;originWidth=15&amp;size=0&amp;status=done&amp;width=13" alt="" /> 和 <img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363878-59712a4e-4c6b-4fea-b7f0-737f5308ff68.svg#align=left&amp;display=inline&amp;height=13&amp;originHeight=15&amp;originWidth=15&amp;size=0&amp;status=done&amp;width=13" alt="" /> ，是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们<strong>最终会选择两者之和 </strong><img src="https://cdn.nlark.com/yuque/0/2019/svg/200056/1559659363867-6da8da44-7f04-4f1b-b162-df6ee49bcfd9.svg#align=left&amp;display=inline&amp;height=18&amp;originHeight=21&amp;originWidth=56&amp;size=0&amp;status=done&amp;width=49" alt="" /><strong> 作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。</strong></li>
</ol>

<p><a name="4vCf8"></a></p>

<h2 id="3-glove损失函数是如何确定的-来自-glove详解-https-link-zhihu-com-target-http-3a-www-fanyeong-com-2018-02-19-glove-in-detail-23comment-1462">3、Glove损失函数是如何确定的？（来自<a href="https://link.zhihu.com/?target=http%3A//www.fanyeong.com/2018/02/19/glove-in-detail/%23comment-1462">GloVe详解</a>）</h2>

<p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363845-99248d46-b921-4992-a8ce-6b1ffaf6ef55.jpeg#align=left&amp;display=inline&amp;height=854&amp;originHeight=479&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=1284" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363879-1ed98137-2e67-40fc-8e49-912a05507797.jpeg#align=left&amp;display=inline&amp;height=1070&amp;originHeight=586&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=1315" alt="" /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363877-68242cc5-7f75-44b3-9eae-3f96bfc43d1b.jpeg#align=left&amp;display=inline&amp;height=876&amp;originHeight=491&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=1285" alt="" /><br />但是在实际应用中，尤其是DL中，<br />关于word2vec和GloVe的比较，请移步这里：Making sense of word2vec<br /><a href="https://link.zhihu.com/?target=http%3A//rare-technologies.com/making-sense-of-word2vec/">https://link.zhihu.com/?target=http%3A//rare-technologies.com/making-sense-of-word2vec/</a><br />word2vec仍然是state-of-the-art的，相比之下GloVe略逊一筹，但也有可圈可点之处。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/200056/1559705436906-334ba331-02f1-418d-a5bb-348311dfa2a8.png#align=left&amp;display=inline&amp;height=374&amp;name=image.png&amp;originHeight=374&amp;originWidth=665&amp;size=76518&amp;status=done&amp;width=665" alt="image.png" /></p>

<p><a name="0m5px"></a></p>

<h1 id="四-深入解剖bert-与elmo和gpt比较">四、深入解剖bert（与elmo和GPT比较）</h1>

<p>bert的全称是Bidirectional Encoder Representation from Transformers，bert的核心是双向<strong>Transformer Encoder</strong>，提出以下问题并进行解答：</p>

<p><a name="bJigS"></a></p>

<h2 id="1-为什么bert采取的是双向transformer-encoder-而不叫decoder">1、为什么bert采取的是双向Transformer Encoder，而不叫decoder？</h2>

<p>BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获要预测的信息的。</p>

<p><a name="BPGQj"></a></p>

<h2 id="2-elmo-gpt和bert在单双向语言模型处理上的不同之处">2、elmo、GPT和bert在单双向语言模型处理上的不同之处？</h2>

<p>在上述3个模型中，只有bert共同依赖于左右上下文。那elmo不是双向吗？实际上elmo使用的是经过独立训练的从左到右和从右到左LSTM的串联拼接起来的。而GPT使用从左到右的Transformer，实际就是“Transformer decoder”。</p>

<p><a name="F8NMQ"></a></p>

<h2 id="3-bert构建双向语言模型不是很简单吗-不也可以直接像elmo拼接transformer-decoder吗">3、bert构建双向语言模型不是很简单吗？不也可以直接像elmo拼接Transformer decoder吗？</h2>

<p>BERT 的作者认为，这种拼接式的bi-directional 仍然不能完整地理解整个语句的语义。更好的办法是用上下文全向来预测[mask]，也就是用 “能/实现/语言/表征/../的/模型”，来预测[mask]。BERT 作者把上下文全向的预测方法，称之为 deep bi-directional。</p>

<p><a name="Xa4wy"></a></p>

<h2 id="4-bert为什么要采取marked-lm-而不直接应用transformer-encoder">4、bert为什么要采取Marked LM，而不直接应用Transformer Encoder？</h2>

<p>我们知道向Transformer这样深度越深，学习效果会越好。可是为什么不直接应用双向模型呢？因为随着网络深度增加会导致标签泄露。如下图：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363878-f7eb43df-558f-428e-88b7-b8a049c94d26.jpeg#align=left&amp;display=inline&amp;height=556&amp;originHeight=467&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=857" alt="" />双向编码与网络深度的冲突</p>

<p>深度双向模型比left-to-right 模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。</p>

<p>为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。</p>

<p><a name="aLgKW"></a></p>

<h2 id="5-bert为什么并不总是用实际的-mask-token替换被-masked-的词汇">5、bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？</h2>

<blockquote>
<p><a href="https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/4dbdb5ab959b">NLP必读 | 十分钟读懂谷歌BERT模型</a>： 虽然这确实能让团队获得双向预训练模型，但这种方法有两个缺点。首先，预训练和finetuning之间不匹配，因为在finetuning期间从未看到[MASK]token。为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：数据生成器将执行以下操作，而不是始终用[MASK]替换所选单词：80％的时间：用[MASK]标记替换单词，例如，my dog is hairy → my dog is [MASK]10％的时间：用一个随机的单词替换该单词，例如，my dog is hairy → my dog is apple10％的时间：保持单词不变，例如，my dog is hairy → my dog is hairy. 这样做的目的是将表示偏向于实际观察到的单词。Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。使用MLM的第二个缺点是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。</p>
</blockquote>

<p>bert模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363864-4bcc3ade-d25b-4d41-aa39-e2282c012052.jpeg#align=left&amp;display=inline&amp;height=311&amp;originHeight=311&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=720" alt="" /></p>

<p>下面给出了Transformer Encoder模型的整体结构：<br /><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363850-99143b36-2b23-45a2-911d-d63d09c783f9.jpeg#align=left&amp;display=inline&amp;height=914&amp;originHeight=914&amp;originWidth=602&amp;size=0&amp;status=done&amp;width=602" alt="" />Transformer Encoder<img src="https://cdn.nlark.com/yuque/0/2019/jpeg/200056/1559659363857-ca4dd426-f4e6-47fd-883f-16e5e85ada08.jpeg#align=left&amp;display=inline&amp;height=962&amp;originHeight=365&amp;originWidth=720&amp;size=0&amp;status=done&amp;width=1898" alt="" />multi-head attention</p>

<p><strong>Reference</strong></p>

<ol>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/itplus/article/details/37969519">word2vec 中的数学原理详解</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//www.fanyeong.com/2018/02/19/glove-in-detail/%23comment-1462">GloVe详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/49271699">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/4dbdb5ab959b">NLP必读 | 十分钟读懂谷歌BERT模型</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//www.chinahadoop.cn/course/1253">谷歌BERT解析&mdash;-2小时上手最强NLP训练模型</a></li>
</ol>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    

    
    <a class="next" href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/" title="特征工程方法论">
      Next
    </a>
    

  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

