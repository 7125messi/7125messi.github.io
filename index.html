<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>7125messi的博客 </title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="website" />



<link href="https://7125messi.github.io/index.xml" rel="alternate" type="application/rss+xml" title="7125messi的博客" />

<link rel="canonical" href="https://7125messi.github.io/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<section id="main-content" class="container main_content homepage">
  <header class="container header">
    <h1>7125messi的博客
</h1>

    <span>last update: <time datetime="2021-10-16T14:34:09&#43;08:00">16 October at 2:34pm</time>
</span>

  </header>
  
  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/mmlspark%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B/">mmlspark订单量预测案例
</a>
</h2>

  <time datetime="2021-05-19">19 May, 2021</time>

</div>

  <p class="container content">
  
  
    本文是前文 Spark预测算法端到端案例的姊妹篇，那一篇用的是普通的lightgbm做的订单量预测，后面随着门店的增加，运行效率不高。
本文主要采用了微软开源的 Microsoft Machine Learning for Apache Spark
https://github.com/Azure/mmlspark
具体用法在上文 Lightgbm在spark实战介绍过，可参考学习。。。
1 门店分类 由于门店数据量巨大，所有门店数据在一起做模型训练，即使在Spark环境下也非常吃性能，所以这里先将门店进行聚类分类，然后在针对同一类型的门店一起训练。
根据 wfm_dw1.wfm_order_channel_half_hour 渠道半小时订单量数据 生成 7天每半小时特征数据 24*2*7
训练 数据:20201130--20210131 预测未来28天数据：20210201--20210228
tstart=&ldquo;20201130&rdquo; tend=&ldquo;20210131&rdquo;
原数据pdf +-------------------+--------+---------+---------+ |global_store_number|sale_day|half_hour|order_qty| +-------------------+--------+---------+---------+ | 28710|20201210| 20| 17.0| | 28710|20201230| 19| 15.0| | 28710|20201211| 13| 19.0| | 28710|20201203| 31| 1.0| | 28710|20201205| 11| 3.0| | 28710|20210111| 19| 14.0| | 28710|20210119| 16| 22.0| | 28710|20210130| 23| 5.0| | 28710|20210107| 16| 30.0| | 28710|20210117| 23| 8.
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/mmlspark%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/lightgbm%E5%9C%A8spark%E5%AE%9E%E6%88%98/">Lightgbm在spark实战
</a>
</h2>

  <time datetime="2021-04-26">26 Apr, 2021</time>

</div>

  <p class="container content">
  
  
    通常业务中对计算性能有要求时，通常不使用GPU跑tf，会使用xgboost/lightgbm on Spark来解决，既保证速度，准确率也能接受。
LightGBM是使用基于树的学习算法的梯度增强框架。它被设计为分布式且高效的，具有以下优点：
根据官网的介绍 * LigthGBM训练速度更快，效率更高。LightGBM比XGBoost快将近10倍。 * 降低内存使用率。内存占用率大约为XGBoost的1/6。 * 准确性有相应提升。 * 支持并行和GPU学习。 * 能够处理大规模数据。
大部分使用和分析LigthGBM的都是在python单机版本上。要在spark上使用LigthGBM，需要安装微软的MMLSpark包。
MMLSpark可以通过&ndash;packages安装。
spark * &ndash;packages参数
根据jar包的maven地址，使用该包，该参数不常用，因为公司内部的数据平台的集群不一定能联网。 如下示例：
$ bin/spark-shell --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 http://maven.aliyun.com/nexus/content/groups/public/   &ndash;repositories 为该包的maven地址，建议给定，不写则使用默认源。 若依赖多个包，则中间以逗号分隔，类似&ndash;jars 默认下载的包位于当前用户根目录下的.ivy/jars文件夹中 应用场景：本地没有编译好的jar包，集群中服务需要该包的的时候，都是从给定的maven地址，直接下载  MMLSpark用法 1 .MMLSpark可以通&ndash;packages选项方便地安装在现有的Spark集群上，例如:
spark-shell --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 pyspark --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 spark-submit --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1  这也可以在其他Spark contexts中使用，例如，可以通过将MMLSpark添加到.aztk/spark-default.conf文件中来在AZTK中使用MMLSpark。
2 .要在Python(或Conda)安装上尝试MMLSpark，首先通过pip安装PySpark, pip安装PySpark。接下来，使用&ndash;package或在运行时添加包来获取scala源代码
import pyspark spark = pyspark.sql.SparkSession.builder.appName(&quot;MyApp&quot;).\ config(&quot;spark.jars.packages&quot;,&quot;com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1&quot;).\ getOrCreate() import mmlspark  3.python建模使用
# 分类 from mmlspark.lightgbm import LightGBMClassifier model = LightGBMClassifier(learningRate=0.
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/lightgbm%E5%9C%A8spark%E5%AE%9E%E6%88%98/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/spark%E9%A2%84%E6%B5%8B%E7%AE%97%E6%B3%95%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A1%88%E4%BE%8B/">Spark预测算法端到端案例
</a>
</h2>

  <time datetime="2021-03-27">27 Mar, 2021</time>

</div>

  <p class="container content">
  
  
    最近做完了一个预测算法，数据量巨大，需要分布式环境跑数据，算法本身用的是LightGBM，没什么好说的，主要是怎么用Spark每个driver端跑模型。
1.基础数据  订单数据：主数据，包括渠道订单量和品类销量（day和hour）
wfm_order_quantity_day wfm_order_quantity_half_hour
wfm_sale_quantity_day wfm_sale_quantity_half_hour
渠道：instore、mop(手机下单，店里取)、mod(外卖,打包) （店内、啡快、专星送） 销量预测：品类+销量 单量预测：渠道+订单量
 天气数据
 促销数据
 门店数据：商圈类型 城市级别
 商品数据：商品品类
 预测目标
 算法中除了考虑内部因子（如历史销售、市场促销、周中周末等）以外，还需纳入外部因子（如天气、节假日、季节、偶发事件等） 每家店的每日平均半小时预测准确率应超过75%（1-MAPE） 每家店的每月平均每日预测准确率应超过92%（1-MAPE） 每家店的每日预测准确率应超过80%（1-WMAPE）   2.特征处理  时间类：年月日，星期，小时等 历史统计类：最大，最小，均值，方差，中位数等 促销类：促销类型，粒度等 节假日：工作日，节假日【传统节日，法定节日等】  3.模型加工 一般预测未来N天有三种方式，本项目预测28天：
 循环发 每次预测一天 带入历史特征 滚动预测下一天 性能低下 gap 1-28 28个模型 效率低 分段 直接预测 比较合理  分7 14 21 28四个shift， 组成28天预测模型
预测1-7 采用 shift 7
预测8-14 采用 shift 14
预测15-21 采用 shift 21
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/spark%E9%A2%84%E6%B5%8B%E7%AE%97%E6%B3%95%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A1%88%E4%BE%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/xgboost_lightgbm_catboost_gridsearch_randomsearch_bayes_opt/">Xgboost_lightgbm_catboost_gridsearch_randomsearch_bayes_opt
</a>
</h2>

  <time datetime="2020-11-26">26 Nov, 2020</time>

</div>

  <p class="container content">
  
  
    本文主要是提高xgboost/lightgbm/catboost等模型在参数调优方面的工程化实现以及在stacking模型融合使用 * xgboost调优 * lightgbm调优 * catboost调优 * stacking融合 * 网格搜索、随机搜索和贝叶斯优化 * LightGBM各种操作
0 工具类函数 from __future__ import print_function from __future__ import division import numpy as np import pandas as pd import matplotlib matplotlib.use(&quot;Agg&quot;) import matplotlib.pyplot as plt from sklearn.metrics import roc_curve, auc from sklearn.metrics import confusion_matrix from sklearn.metrics import f1_score from itertools import chain import time import os def timer(func): &quot;&quot;&quot;计时器&quot;&quot;&quot; def wrapper(*args, **kwargs): t1 = time.time() func(*args, **kwargs) t2 = time.
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/xgboost_lightgbm_catboost_gridsearch_randomsearch_bayes_opt/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/">Spark_Hive_RDBMS读写操作
</a>
</h2>

  <time datetime="2020-09-10">10 Sep, 2020</time>

</div>

  <p class="container content">
  
  
    [项目总结提炼]
前面我们在做数据工程化过程中会大量用到数据的读写操作,现总结如下！！！
主要有以下几个文件：
config.ini 配置文件 func_forecast.sh 工程执行文件 mysql-connector-java-8.0.11.jar MySQL连接jdbc驱动 predict_pred.py 执行主代码 utils.py 工具项代码  1 config.ini 主要定义参数值
[spark] executor_memory = 2g driver_memory = 2g sql_execution_arrow_enabled = true executor_cores = 2 executor_instances = 2 jar = /root/spark-rdbms/mysql-connector-java-8.0.11.jar [mysql] host = 11.23.32.16 port = 3306 db = test user = test password = 123456 [postgresql] host = 11.23.32.16 port = 3306 db = test user = test password = 123456 [dbms_parameters] mysql_conn_info_passwd @@@ =4bI=prodha mysql_conn_info_database = test_db  2 utils.
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/spark_hive_rdbms%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/">数据工程化案例介绍
</a>
</h2>

  <time datetime="2020-08-03">3 Aug, 2020</time>

</div>

  <p class="container content">
  
  
    [原创]
数据工程化案例介绍 好久没写博客了😃😃😃😃😃😃，最近做完了一个偏数据工程的项目，系统的使用了大数据相关组件，学习了Hadoop生态圈技术以及数据仓库相关知识。下面将一些体会写下。。。
1 项目背景和业务特点 XXX医药业务场景：以终端消费者为中心的服务，以门店、连锁加盟、批发模式触达，当前核心竞争力是品牌加盟和供应链采购能力。随着加盟业务快速成长，致力于成为中国最大的零售药房加盟商，需要配套成长的供应链物流能力和信息化建设。“高库存、高退货、高效期” 等环节精细化运营的薄弱是主要原因，具体表现在以下几点：
 (1) 门店补货靠经验，造成了“高退货”; (2) 加盟店和批发商等物流能力尚未触达、物流信息尚未线上化; (3) 与供应商信息沟通均为线下,补货频次较为传统; (4) 采购计划依赖采购员个人经验采用公式计算，未考虑复杂因素;  项目目标是构建以智能补货为智慧大脑的需求驱动的全局、动态、平衡的数字化供应链运营体系，提供安全、高效、精准的供应链能力。主要包括以下部分：
 (1) 数据清洗 (2) 特种工程 (3) 模型训练 (4) 模型融合 (5) 数据工程化  其中前4个部分是机器学习的常见方法和步骤,考虑到线上生产环境要能正常执行,第5部分数据工程化部分启动非常重要的地位,下面对这个部分进行详细的叙述。
2 数据工程化流程架构 这里我们的数据源主要Oracle业务数据以及一些客户提供的人工数据,利用sqoop每日凌晨00:40定时同步数据至Hadoop集群src层。按照经典的Kappa数据仓库分层架构分为:src-&gt;ods-&gt;dw1-&gt;dw2-&gt;app.
与传统的数仓建模不同的是我们主要的目的是利用机器学习方法进行预测补货,数据仓库ods/dw1都是数据清洗和复杂业务逻辑处理,dw2是特征工程处理生成宽表用于训练模型。在数据清洗的过程中会有一些指标用于KPI展示以及app模型预测补货结果我们会同步至MySQL,这些都是作为数据应用服务。
整个数据工程基于Hadoop生态圈技术为载体,数据存储主要是HDFS,数据计算主要是Hive/Spark,元数据管理是Apache Atlas,数据质量分析用的是Apache Griffin,数据任务流调度系统用的是Azkaban,数据OLAP数据库是Presto,数据分析可视化Dashboard用的是Superset。这些大数据组件采用Cloudera Manager(CM)进行统一安装配置,为了保证服务的高可用(HA)采用Zookeeper进行资源调度和管理。
3 数据工程生产环境部署 3.1 可配置项 配置项对于数据工程的重要性不言而喻,可灵活调整部署代码,方便控制管理
├─conf │ │ config.ini 所有可配置参数 │ │ env_name.conf 生产环境和测试环境的标识符 │ │ ini.sh 读写配置文件的函数  例如:
[replenish_parameters] start_date=2020-07-18 end_date=2020-08-31 rolling_day=7 rolling_num=50 [env_parameters] data_base_dir=/data base_dir_jar=/root  这样我们对于每张表的计算添加统一的配置项
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%8C%96%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">SVM调优详解
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲，RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们就需要使用RBF，在享受RBF对非线性数据的良好分类效果前，我们需要对主要的超参数进行选取。本文我们就对scikit-learn中 SVM RBF的调参做一个小结。

1 SVM RBF 主要超参数概述 如果是SVM分类模型，这两个超参数分别是惩罚系数和RBF核函数的系数。当然如果是nu-SVC的话，惩罚系数C代替为分类错误率上限nu, 由于惩罚系数C和分类错误率上限nu起的作用等价，因此本文只讨论带惩罚系数C的分类SVM**

1.1 SVM分类模型 ###（1） 惩罚系数
 惩罚系数C即上一篇里讲到的松弛变量ξ的系数。它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数。
 当惩罚系数C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合。
 当惩罚系数C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单。scikit-learn中默认值是1。
  
（2）RBF核函数的系数 另一个超参数是RBF核函数的参数。回忆下RBF 核函数
γ主要定义了单个样本对整个分类超平面的影响。
 当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量
 当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量**，或者说整个模型的支持向量也会多。scikit-learn中默认值是1/n_features**
  
（3）惩罚系数和RBF核函数的系数 如果把惩罚系数和RBF核函数的系数一起看：
 当C比较大、 γ比较大时，会有更多的支持向量，模型会比较复杂，较容易过拟合 当C比较小、γ比较小时，模型会变得简单，支持向量的个数会少  
1.2 SVM回归模型 SVM回归模型的RBF核比分类模型要复杂一点，因为此时除了惩罚系数C和RBF核函数的系数γ之外，还多了一个损失距离度量ϵ。如果是nu-SVR的话，损失距离度量ϵ代替为分类错误率上限nu，由于损失距离度量ϵ和分类错误率上限nu起的作用等价，因此本文只讨论带距离度量ϵ的回归SVM。
 对于惩罚系数C和RBF核函数的系数γ，回归模型和分类模型的作用基本相同。
 对于损失距离度量ϵ，它决定了样本点到超平面的距离损失.当ϵ比较大时，损失较小，更多的点在损失距离范围之内，模型较简单;当ϵ比较小时，损失函数会较大，模型也会变得复杂；scikit-learn中默认值是0.1
  惩罚系数C、RBF核函数的系数γ和损失距离度量ϵ一起看
 当C比较大、 γ比较大、ϵ比较小时，会有更多的支持向量，模型会比较复杂，容易过拟合一些;
 当C比较小、γ比较小、ϵ比较大时**，模型会变得简单，支持向量的个数会少
  
2 SVM RBF 主要调参方法 对于SVM的RBF核，主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类。
当然也可以使用cross_val_score类来调参，但是个人觉得没有GridSearchCV方便。本文只讨论用GridSearchCV**来进行SVM的RBF核的调参。
将GridSearchCV类用于SVM RBF调参时要注意的参数有：
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">RandomForest调优详解
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    原文来自：http://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/

为什么要调整机器学习算法？ 一个月以前，我在kaggle上参加了一个名为TFI的比赛。 我第一次提交的结果在50%。 我不懈努力在特征工程上花了超过2周的时间，勉强达到20%。 出乎我意料的事是，在调整机器学习算法参数之后，我能够达到前10%。
这就是机器学习算法参数调优的重要性。 随机森林是在工业界中使用的最简单的机器学习工具之一。 在我们以前的文章中，我们已经向您介绍了随机森林和和CART模型进行了对比 。 机器学习工具包正由于这些算法的表现而被人所熟知。
随机森林是什么？ 随机森林是一个集成工具，它使用观测数据的子集（BootStraping）和特征变量的子集（随机选择特征变量）来建立一个决策树。 它建立多个这样的决策树，然后将他们合并在一起以获得更准确和稳定的预测。 这样做最直接的事实是，在这一组独立的预测结果中，用投票方式得到一个最高投票结果，这个比单独使用最好模型预测的结果要好。
我们通常将随机森林作为一个黑盒子，输入数据然后给出了预测结果，无需担心模型是如何计算的。这个黑盒子本身有几个我们可以摆弄的杠杆。 每个杠杆都能在一定程度上影响模型的性能或资源时间平衡。 在这篇文章中，我们将更多地讨论我们可以调整的杠杆，同时建立一个随机森林模型。
调整随机森林的参数杠杆 随机森林的参数即可以增加模型的预测能力，又可以使训练模型更加容易。 以下我们将更详细地谈论各个参数（请注意，这些参数，我使用的是Python常规的命名法）：
1.使模型预测更好的特征 主要有3类特征可以被调整，以改善该模型的预测能力

A. max_features： 随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：
 Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。
 sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。
 0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。
   If &ldquo;auto&rdquo;, then max_features=sqrt(n_features).
 If &ldquo;sqrt&rdquo;, then max_features=sqrt(n_features) (same as &ldquo;auto&rdquo;).
 If &ldquo;log2&rdquo;, then max_features=log2(n_features).
 If None, then max_features=n_features.
  max_features如何影响性能和速度？
增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它 同时也降低了单个树的多样性 ，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">GBDT调优详解
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    原文地址：Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python by Aarshay Jain
1.前言 如果一直以来你只把GBM当作黑匣子，只知调用却不明就里，是时候来打开这个黑匣子一探究竟了！
不像bagging算法只能改善模型高方差（high variance）情况，Boosting算法对同时控制偏差（bias）和方差（variance）都有非常好的效果，而且更加高效。
如果你需要同时处理模型中的方差和偏差，认真理解这篇文章一定会对你大有帮助，
本文会用Python阐明GBM算法，更重要的是会介绍如何对GBM调参，而恰当的参数往往能令结果大不相同。

2.目录  Boosing是怎么工作的？
 理解GBM模型中的参数
 学会调参（附详例）
  
3.Boosting是如何工作的？ Boosting可以将一系列弱学习因子（weak learners）相结合来提升总体模型的预测准确度。在任意时间t，根据t-1时刻得到的结果我们给当前结果赋予一个权重。之前正确预测的结果获得较小权重，错误分类的结果得到较大权重。回归问题的处理方法也是相似的。
让我们用图像帮助理解：
 图一： 第一个弱学习因子的预测结果（从左至右）
 一开始所有的点具有相同的权重（以点的尺寸表示）。
 分类线正确地分类了两个正极和五个负极的点。
  图二： 第二个弱学习因子的预测结果
 在图一中被正确预测的点有较小的权重（尺寸较小），而被预测错误的点则有较大的权重。
 这时候模型就会更加注重具有大权重的点的预测结果，即上一轮分类错误的点，现在这些点被正确归类了，但其他点中的一些点却归类错误。
   对图3的输出结果的理解也是类似的。这个算法一直如此持续进行直到所有的学习模型根据它们的预测结果都被赋予了一个权重，这样我们就得到了一个总体上更为准确的预测模型。
现在你是否对Boosting更感兴趣了？不妨看看下面这些文章（主要讨论GBM）：
 Learn Gradient Boosting Algorithm for better predictions (with codes in R)
 Quick Introduction to Boosting Algorithms in Machine Learning
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/">XGBoost调优指南
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    原文地址：Complete Guide to Parameter Tuning in XGBoost by Aarshay Jain

1. 简介 如果你的预测模型表现得有些不尽如人意，那就用XGBoost吧。XGBoost算法现在已经成为很多数据工程师的重要武器。它是一种十分精致的算法，可以处理各种不规则的数据。
构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难(至少我觉得十分纠结)。这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。在解决实际问题的时候，有些问题是很难回答的——你需要调整哪些参数？这些参数要调到什么值，才能达到理想的输出？
这篇文章最适合刚刚接触XGBoost的人阅读。在这篇文章中，我们会学到参数调优的技巧，以及XGboost相关的一些有用的知识。以及，我们会用Python在一个数据集上实践一下这个算法。

2. 你需要知道的 XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。在前文章中，基于Python的Gradient Boosting算法参数调整完全指南，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。

3. 内容列表 1、XGBoost的优势 2、理解XGBoost的参数 3、调参示例

4. XGBoost的优势 XGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势：

4.1 正则化  标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。
 实际上，XGBoost以正则化提升(regularized boosting)技术而闻名。  
4.2 并行处理  XGBoost可以实现并行处理，相比GBM有了速度的飞跃。 不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢?每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。(构造决策树的结构时，样本分割点位置，可以使用并行计算)
 XGBoost 也支持Hadoop实现。  
4.3 高度的灵活性  XGBoost 允许用户定义自定义优化目标函数和评价标准 它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。  
4.4 缺失值处理  XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/">Read more &rarr;</a>

</div>


</article>

  
  
<div class="container pagination">
  


<a aria-label="First" href="https://7125messi.github.io/">
  <span aria-hidden="true">««</span>
</a>

<a class="disabled" aria-label="Previous" href="#">
  <span aria-hidden="true">«</span>
</a>


<a class="active" href="https://7125messi.github.io/">
  1
</a>

<a href="https://7125messi.github.io/page/2/">
  2
</a>


<a aria-label="Next" href="https://7125messi.github.io/page/2/">
  <span aria-hidden="true">»</span>
</a>

<a aria-label="Last" href="https://7125messi.github.io/page/2/">
  <span aria-hidden="true">»»</span>
</a>


</div>


</section>

      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

