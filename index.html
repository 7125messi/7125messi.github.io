<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>7125messi的博客 </title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="website" />



<link href="https://7125messi.github.io/index.xml" rel="alternate" type="application/rss+xml" title="7125messi的博客" />

<link rel="canonical" href="https://7125messi.github.io/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<section id="main-content" class="container main_content homepage">
  <header class="container header">
    <h1>7125messi的博客
</h1>

    <span>last update: <time datetime="2019-08-02T21:14:54&#43;08:00">2 August at 9:14pm</time>
</span>

  </header>
  
  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">RandomForest调优详解
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    原文来自：http://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/

为什么要调整机器学习算法？ 一个月以前，我在kaggle上参加了一个名为TFI的比赛。 我第一次提交的结果在50%。 我不懈努力在特征工程上花了超过2周的时间，勉强达到20%。 出乎我意料的事是，在调整机器学习算法参数之后，我能够达到前10%。
这就是机器学习算法参数调优的重要性。 随机森林是在工业界中使用的最简单的机器学习工具之一。 在我们以前的文章中，我们已经向您介绍了随机森林和和CART模型进行了对比 。 机器学习工具包正由于这些算法的表现而被人所熟知。
随机森林是什么？ 随机森林是一个集成工具，它使用观测数据的子集（BootStraping）和特征变量的子集（随机选择特征变量）来建立一个决策树。 它建立多个这样的决策树，然后将他们合并在一起以获得更准确和稳定的预测。 这样做最直接的事实是，在这一组独立的预测结果中，用投票方式得到一个最高投票结果，这个比单独使用最好模型预测的结果要好。
我们通常将随机森林作为一个黑盒子，输入数据然后给出了预测结果，无需担心模型是如何计算的。这个黑盒子本身有几个我们可以摆弄的杠杆。 每个杠杆都能在一定程度上影响模型的性能或资源时间平衡。 在这篇文章中，我们将更多地讨论我们可以调整的杠杆，同时建立一个随机森林模型。
调整随机森林的参数杠杆 随机森林的参数即可以增加模型的预测能力，又可以使训练模型更加容易。 以下我们将更详细地谈论各个参数（请注意，这些参数，我使用的是Python常规的命名法）：
1.使模型预测更好的特征 主要有3类特征可以被调整，以改善该模型的预测能力

A. max_features： 随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：
 Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。
 sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。
 0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。
   If &ldquo;auto&rdquo;, then max_features=sqrt(n_features).
 If &ldquo;sqrt&rdquo;, then max_features=sqrt(n_features) (same as &ldquo;auto&rdquo;).
 If &ldquo;log2&rdquo;, then max_features=log2(n_features).
 If None, then max_features=n_features.
  max_features如何影响性能和速度？
增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它 同时也降低了单个树的多样性 ，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/randomforest%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">GBDT调优详解
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    原文地址：Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python by Aarshay Jain
1.前言 如果一直以来你只把GBM当作黑匣子，只知调用却不明就里，是时候来打开这个黑匣子一探究竟了！
不像bagging算法只能改善模型高方差（high variance）情况，Boosting算法对同时控制偏差（bias）和方差（variance）都有非常好的效果，而且更加高效。
如果你需要同时处理模型中的方差和偏差，认真理解这篇文章一定会对你大有帮助，
本文会用Python阐明GBM算法，更重要的是会介绍如何对GBM调参，而恰当的参数往往能令结果大不相同。

2.目录  Boosing是怎么工作的？
 理解GBM模型中的参数
 学会调参（附详例）
  
3.Boosting是如何工作的？ Boosting可以将一系列弱学习因子（weak learners）相结合来提升总体模型的预测准确度。在任意时间t，根据t-1时刻得到的结果我们给当前结果赋予一个权重。之前正确预测的结果获得较小权重，错误分类的结果得到较大权重。回归问题的处理方法也是相似的。
让我们用图像帮助理解：
 图一： 第一个弱学习因子的预测结果（从左至右）
 一开始所有的点具有相同的权重（以点的尺寸表示）。
 分类线正确地分类了两个正极和五个负极的点。
  图二： 第二个弱学习因子的预测结果
 在图一中被正确预测的点有较小的权重（尺寸较小），而被预测错误的点则有较大的权重。
 这时候模型就会更加注重具有大权重的点的预测结果，即上一轮分类错误的点，现在这些点被正确归类了，但其他点中的一些点却归类错误。
   对图3的输出结果的理解也是类似的。这个算法一直如此持续进行直到所有的学习模型根据它们的预测结果都被赋予了一个权重，这样我们就得到了一个总体上更为准确的预测模型。
现在你是否对Boosting更感兴趣了？不妨看看下面这些文章（主要讨论GBM）：
 Learn Gradient Boosting Algorithm for better predictions (with codes in R)
 Quick Introduction to Boosting Algorithms in Machine Learning
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/">XGBoost调优指南
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    原文地址：Complete Guide to Parameter Tuning in XGBoost by Aarshay Jain

1. 简介 如果你的预测模型表现得有些不尽如人意，那就用XGBoost吧。XGBoost算法现在已经成为很多数据工程师的重要武器。它是一种十分精致的算法，可以处理各种不规则的数据。
构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难(至少我觉得十分纠结)。这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。在解决实际问题的时候，有些问题是很难回答的——你需要调整哪些参数？这些参数要调到什么值，才能达到理想的输出？
这篇文章最适合刚刚接触XGBoost的人阅读。在这篇文章中，我们会学到参数调优的技巧，以及XGboost相关的一些有用的知识。以及，我们会用Python在一个数据集上实践一下这个算法。

2. 你需要知道的 XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。在前文章中，基于Python的Gradient Boosting算法参数调整完全指南，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。

3. 内容列表 1、XGBoost的优势 2、理解XGBoost的参数 3、调参示例

4. XGBoost的优势 XGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势：

4.1 正则化  标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。
 实际上，XGBoost以正则化提升(regularized boosting)技术而闻名。  
4.2 并行处理  XGBoost可以实现并行处理，相比GBM有了速度的飞跃。 不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢?每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。(构造决策树的结构时，样本分割点位置，可以使用并行计算)
 XGBoost 也支持Hadoop实现。  
4.3 高度的灵活性  XGBoost 允许用户定义自定义优化目标函数和评价标准 它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。  
4.4 缺失值处理  XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/xgboost%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/lightgbm%E5%AD%A6%E4%B9%A0/">LightGBM学习
</a>
</h2>

  <time datetime="2019-08-02">2 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
1 LightGBM原理 
1.1 GBDT和 LightGBM对比 GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT 也是各种数据挖掘竞赛的致命武器，据统计 Kaggle 上的比赛有一半以上的冠军方案都是基于 GBDT。
LightGBM（Light Gradient Boosting Machine）同样是一款基于决策树算法的分布式梯度提升框架。为了满足工业界缩短模型计算时间的需求，LightGBM的设计思路主要是两点：
 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据； 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。
由此可见，LightGBM的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具。  XGBoost和LightGBM都是基于决策树提升(Tree Boosting)的工具，都拥有对输入要求不敏感、计算复杂度不高和效果好的特点，适合在工业界中进行大量的应用。
主页地址：http://lightgbm.apachecn.org
LightGBM （Light Gradient Boosting Machine）是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有以下优点：
 更快的训练速度 更低的内存消耗 更好的准确率 分布式支持，可以快速处理海量数据  如下图，在 Higgs 数据集上 LightGBM 比 XGBoost 快将近 10 倍，内存占用率大约为 XGBoost 的1/6，并且准确率也有提升。

1.2 LightGBM 的动机 常用的机器学习算法，例如神经网络等算法，都可以以 mini-batch 的方式训练，训练数据的大小不会受到内存限制。
而 GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的。
 LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/lightgbm%E5%AD%A6%E4%B9%A0/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/">特征工程———特征选择的原理和实现
</a>
</h2>

  <time datetime="2019-08-01">1 Aug, 2019</time>

</div>

  <p class="container content">
  
  
    [参考总结提炼]
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。
那特征工程是什么？
 特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。
 特征工程又包含了Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等子问题，本文主要讨论特征选择相关的方法及实现。在实际项目中，我们可能会有大量的特征可使用，有的特征携带的信息丰富，有的特征携带的信息有重叠，有的特征则属于无关特征，如果所有特征不经筛选地全部作为训练特征，经常会出现维度灾难问题，甚至会降低模型的准确性。因此，我们需要进行特征筛选，排除无效/冗余的特征，把有用的特征挑选出来作为模型的训练数据。
01 特征选择介绍 1.特征按重要性分类  相关特征：
对于学习任务（例如分类问题）有帮助，可以提升学习算法的效果；
 无关特征：
对于我们的算法没有任何帮助，不会给算法的效果带来任何提升；
 冗余特征：
不会对我们的算法带来新的信息，或者这种特征的信息可以由其他的特征推断出；
  2.特征选择的目的 对于一个特定的学习算法来说，哪一个特征是有效的是未知的。因此，需要从所有特征中选择出对于学习算法有益的相关特征。而且在实际应用中，经常会出现维度灾难问题。如果只选择所有特征中的部分特征构建模型，那么可以大大减少学习算法的运行时间，也可以增加模型的可解释性。
3.特征选择的原则 获取尽可能小的特征子集，不显著降低分类精度、不影响分类分布以及特征子集应具有稳定、适应性强等特点。 
02 特征选择的方法 
1.Filter方法（过滤式） 先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。
主要思想：对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
主要方法：
 Chi-squared test（卡方检验）
 Information gain（信息增益）
 Correlation coefficient scores（相关系数）
  优点：运行速度快，是一种非常流行的特征选择方法。
缺点：无法提供反馈，特征选择的标准规范的制定是在特征搜索算法中完成，学习算法无法向特征搜索算法传递对特征的需求。另外，可能处理某个特征时由于任意原因表示该特征不重要，但是该特征与其他特征结合起来则可能变得很重要。
2.Wrapper方法（封装式） 直接把最后要使用的分类器作为特征选择的评价函数，对于特定的分类器选择最优的特征子集。
主要思想：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA、PSO（如：优化算法-粒子群算法）、DE、ABC（如：优化算法-人工蜂群算法）等。
主要方法：递归特征消除算法。
优点：对特征进行搜索时围绕学习算法展开的，对特征选择的标准规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，真正关注的是学习问题本身。由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。
缺点：运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
3.Embedded方法（嵌入式） 将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。
主要思想：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。
主要方法：用带有L1正则化的项完成特征选择（也可以结合L2惩罚项来优化）、随机森林平均不纯度减少法/平均精确度减少法。
优点：对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于Wrapper方法，比较节省时间。
缺点：运行速度慢。
03 特征选择实现方法一：去掉取值变化小的特征 （Removing features with low variance） 该方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。 考察某个特征下，样本的方差值，可以认为给定一个阈值，抛弃哪些小于某个阈值的特征。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E5%8D%8A%E5%8F%82%E6%95%B0%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/">半参数回归模型
</a>
</h2>

  <time datetime="2019-07-24">24 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [原创]
常见的处理非线性关系的方法有数据转换方法和神经转换、SVM、投影寻踪和基于树的方法等高计算强度方法。实际在应用回归问题有很大的局限性，模型的可解释性差。使用非参数和半参数回归方法来处理非线性关系一定程度可避免这些问题。
从数据本身出发来估计适合数据本身的函数形式。
 1.忽略非线性的后果  对模型所有的连续自变量都进行检验来判断是否具有非线性作用（xy），通过数据变换解决非线性问题。
 2.数据变换  幂转换：仅能够对正数取值的变量才能使用。所以才使用非参数和半参数回归方法。
 3.非参数和半参数回归方法  从数据本身出发来估计适合数据本身的函数形式。用局部估计取代全局估计。 非参数回归的局部估计是通过数据估计两个变量之间的函数形式，而全局估计通过假设来对函数形式作出规定。 半参数回归模型：利用多元模型把全局估计和局部估计结合起来。 半参数回归模型：广义可加模型（GAM）（特殊：加性模型）。 GAM指自变量为离散或连续变量的半参数回归模型，可对自变量做非参数估计，对一些自变量采取标准的方式估计。 GAM中怀疑具有非线性函数形式的连续自变量可以用非参数估计，而模型中其他变量仍以参数形式估计。 GAM依赖非参数回归，X与Y之间的全局拟合被局部拟合取代，放弃了全部拟合的假设，仍保留了加性的假设。 GAM的加性假设使得模型比神经网络、支持向量机等更容易解释，比完全的参数模型更灵活。 GAM可以适用于许多类型的因变量：连续的、计数的、二分类的，定序的和时间等等。 GAM提供了诊断非线性的框架，简单线性模型和幂转换模型是嵌套在GAM中。 GAM的局部估计可以使用F检验或似然比检验来检验线性，二次项或任何其他幂转换模型的拟合效果。如果半参数回归模型优于线性模型或幂转换模型，它就应该被采用。 半参数回归模型的检验非线性和模型比较作用给予了半参数回归模型的强大功能，对于任意的连续自变量，都应采用半参数方法进行诊断或建模。   1 非参数估计 参数回归与非参数回归的优缺点比较
&gt; 参数模型 &gt; &gt; 优点： &gt; (1)模型形式简单明确，仅由一些参数表达 &gt; (2)在经济中，模型的参数具有一般都具有明确的经济含义 &gt; (3)当模型参数假设成立，统计推断的精度较高，能经受实际检验 &gt; (4)模型能够进行外推运算 &gt; (5)模型可以用于小样本的统计推断 &gt; &gt; 缺点： &gt; (1)回归函数的形式预先假定 &gt; (2)模型限制较多：一般要求样本满足某种分布要求，随机误差满足正态假设，解释变量间独立，解释变量与随机误差不相关等 &gt; (3)需要对模型的参数进行严格的检验推断，步骤较多 &gt; (4)模型泛化能力弱，缺乏稳健性，当模型假设不成立，拟合效果不好，需要修正或者甚至更换模型 &gt; &gt; 非参数模型 &gt; &gt; 优点： &gt; &gt; (1)**回归函数形式自由，受约束少，对数据的分布一般不做任何要求** &gt; (2)**适应能力强，稳健性高，回归模型完全由数据驱动** &gt; (3)**模型的精度高 ;** &gt; (4)**对于非线性、非齐次问题，有非常好的效果**  使用非参数回归时，利用数据来估计F的函数形式。 事先的线性假设被更弱的假设光滑总体函数所代替。这个更弱的假设的代价是两方面的： * 第一，计算方面的代价是巨大的，但考虑到现代计算技术的速度，这不再是大问题； * 第二，失去了一些可解释性，但同时也得到了一个更具代表性的估计。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E5%8D%8A%E5%8F%82%E6%95%B0%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/seq2seq%E6%A8%A1%E5%9E%8B/">Seq2Seq模型
</a>
</h2>

  <time datetime="2019-07-20">20 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
前 言 Seq2Seq，全称Sequence to Sequence。它是一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中。Seq2Seq并不是GNMT（Google Neural Machine Translation）系统的官方开源实现。框架的目的是去完成更广泛的任务，而神经机器翻译只是其中之一。在循环神经网络中我们了解到如何将一个序列转化成定长输出。在本文中，我们将探究如何将一个序列转化成一个不定长的序列输出（如机器翻译中，源语言和目标语言的句子往往并没有相同的长度）。

1 简单入门 
设计目标  通用 这个框架最初是为了机器翻译构建的，但是后来使用它完成了各种其他任务，包括文本摘要、会话建模和图像字幕。只要我们的任务，可以将输入数据以一种格式编码并将其以另一种格式解码，我们就可以使用或者扩展这个框架。
 可用性 支持多种类型的输入数据，包括标准的原始文本。
 重现性 用YAML文件来配置我们的pipelines和models，容易复现。
 可扩展性 代码以模块化的方式构建，添加一种新的attention机制或编码器体系结构只需要最小的代码更改。
 文档化:所有的代码都使用标准的Python文档字符串来记录，并且编写了使用指南来帮助我们着手执行常见的任务。
 良好的性能:为了代码的简单性，开发团队并没有试图去尽力压榨每一处可能被拓展的性能，但是对于几乎所有的生产和研究项目，当前的实现已经足够快了。此外，tf-seq2seq还支持分布式训练。
  
主要概念 Configuration 许多objects都是使用键值对来配置的。这些参数通常以YAML的形式通过配置文件传递，或者直接通过命令行传递。配置通常是嵌套的，如下例所示:
model_params: attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau attention.params: num_units: 512 embedding.dim: 1024 encoder.class: seq2seq.encoders.BidirectionalRNNEncoder encoder.params: rnn_cell: cell_class: LSTMCell cell_params: num_units: 512   Input Pipeline
  InputPipeline定义了如何读取、解析数据并将数据分隔成特征和标签。如果您想要读取新的数据格式，我们需要实现自己的输入管道。
 Encoder(编码)
 Decoder（解码）
 Model（Attention）
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/seq2seq%E6%A8%A1%E5%9E%8B/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/attention%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/">Attention那些事儿
</a>
</h2>

  <time datetime="2019-07-10">10 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
原文：遍地开花的 Attention ，你真的懂吗？
阿里机器智能
Attention PPT .pdf
Attention 自2015年被提出后，在 NLP 领域，图像领域遍地开花。Attention 赋予模型区分辨别能力，从纷繁的信息中找到应当 focus 的重点。2017年 self attention 的出现，使得 NLP 领域对词句 representation 能力有了很大的提升，整个 NLP 领域开启了全面拥抱 transformer 的年代。

1 初识Attention 
1.1 History 
Attention 的发展可以粗暴地分为两个阶段。
2015-2017年，自从 attention 提出后，基本就成为 NLP 模型的标配，各种各样的花式 attention 铺天盖地。不仅在 Machine Translation，在 Text summarization，Text Comprehend（Q&amp;A）, Text Classification 也广泛应用。奠定基础的几篇文章如下：
2015年 ICLR 《Neural machine translation by jointly learning to align and translate》首次提出 attention（基本上算是公认的首次提出），文章提出了最经典的 Attention 结构（additive attention 或者 又叫 bahdanau attention）用于机器翻译，并形象直观地展示了 attention 带来源语目标语的对齐效果，解释深度模型到底学到了什么，人类表示服气。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/attention%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E4%B8%AD%E5%8F%B0%E6%A1%88%E4%BE%8B%E7%90%86%E8%A7%A3/">中台案例理解
</a>
</h2>

  <time datetime="2019-07-10">10 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
缘起 百度指数搜索“中台”，可以发现，中台一词前几年几乎都没有搜索，反倒是今年5月21号开始蹭蹭往上涨！
百度指数
仔细搜索了一下原来5月21号腾讯召开了全球数字生态大会，会议上腾讯高级副总裁汤道生提出“开放中台能力，助力产业升级”。汤道生介绍，腾讯技术委员会正在推动“开源协同”和“自研上云”，通过技术整合实现高效的能力交付。同时，基于在即时通讯、社交、游戏等优势领域中的技术积累，腾讯将进一步开放业界领先的包括用户中台、内容中台、应用中台等在内的数据中台，以及包括通信中台、AI中台、安全中台等在内的技术中台。企业与开发者可以灵活地把这些技术应用到业务场景中。中台一词开始步入大家的视角。
其实腾讯并不是最早弄中台的，但今年中台是被腾讯带火的。国内最早弄中台的公司是阿里巴巴！说到阿里巴巴的中台就不得不说到芬兰的一家游戏公司Supercell！

芬兰游戏公司Supercell 2015年年中，马云带领阿里巴巴集团高管，拜访了位于芬兰赫尔辛基的移动游戏公司Supercell。Supercell当时号称是世界上最成功的移动游戏公司，Supercell由6名资深游戏开发者在2010年创立，旗下拥有《部落冲突》、《皇室战争》、《海岛奇兵》和《卡通农场》这四款超级现象级产品。Supercell是一家典型的以小团队模式进行游戏开发的公司，以2到5个员工、最多不超过7个员工组成独立的开发团队，称之为Cell(细胞) ，这也是公司名字Supercell （超级细胞）的由来。团队自己决定做什么样的产品，然后最快时间推出产品公测版，看看游戏是否受用户欢迎。如果用户不欢迎，迅速放弃这个产品，再进行新的尝试，期间几乎没有管理角色的介入。团队研发的产品失败后，不但不会受到惩罚，甚至还会举办庆祝仪式，以庆祝他们从失败中学到了东西。这种模式让Supercell公司成为了年税前利润15亿美金的游戏公司。
2016年6月，腾讯以86亿美元收购了员工数不超过200人的Supercell公司84.3%的股权，每一名员工人均贡献的估值超过3.54亿人民币。Supercell的成功很大原因就在于其高效的“部落”组织策略。在supercell仅有的100多人中，被分成若干个小前台组织，每个小组虽然人不多，但都包含了做一款游戏需要的所有人才。本来就不大的公司被分成若干个小组，这样做的好处是可以快速决策，快速研发，快速把产品推向市场，而游戏引擎、服务器等后台基础则不需要操心。
Supercell 的模式给参加此次拜访的阿里高管们很大的震撼，在大家反复的心得交流和讨论中，一个非常重要的问题引起了很多人的反思：信息时代的公司架构到底应该是怎样的？正是有了这次拜访才真正让阿里巴巴的领导层有了足够的决心要将组织架构进行调整，在此次拜访的半年后，阿里集团CEO逍遥子发出内部邮件，组织架构全面升级，建设整合阿里产品技术和数据能力的强大中台，组建“大中台，小前台”的组织和业务体制。

阿里中台 所谓的“中台”，并不是阿里巴巴首先提出的词语，从字面意思上理解，中台是基于前台和后台之间。阿里通过多年不懈的努力，在业务的不断催化滋养下，将自己的技术和业务能力沉淀出一套综合能力平台，具备了对于前台业务变化及创新的快速响应能力。阿里人将“中台战略”形象地比喻成陆海空三军立体化协同作战：
他们将中台分为六类，分别对应不同兵种：
业务中台，提供重用服务，例如用户中心、订单中心之类的开箱即用可重用能力，为战场提供了空军支援能力，随叫随到，威力强大；
数据中台，提供数据分析能力，帮助从数据中学习改进，调整方向，为战场提供了海军支援能力；
算法中台，提供算法能力，帮助提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡；
技术中台，提供自建系统部分的技术支撑能力，帮助解决基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备；
研发中台，提供自建系统部分的管理和技术实践支撑能力，帮助快速搭建项目、管理进度、测试、持续集成、持续交付，是前台特种兵的训练基地；
组织中台，为项目提供投资管理、风险管理、资源调度等，是战场的指挥部，战争的大脑，指挥前线，调度后方。
2018双11，阿里又一次实现了一次壮举，在2135亿的背后，在令人骄傲的战绩背后，缺少不了阿里中台铁军发挥的巨大力量。
阿里中台建设也并非一帆风顺。曾经淘宝就曾大费精力搭建了一个CRM平台，但大部分商家不买账，因为仅靠一套系统，根本无法满足不同行业、不同规模的几百万商家多样化的需求。后来淘宝采用中台战略思想，将15万家ISV的服务能力组织起来，进行组件化，搞定几百万家商家不同需求。通过开放赋能不同商家、不同业务，帮助商家实现业务创新。就此阿里中台战略在企业服务生态建设方面拉开了序幕。说完这么多中台的事情，可能你还是不知道什么是中台！再举一下董阳对阿里巴巴数据中台的理解。

白话数据中台 中台就是公共服务平台，数据中台就是将数据加工以后封装成一个公共的数据产品或服务。
家里厨房有油/盐/酱油/醋/料酒/生抽…很多种调料（数据），你（业务部门）特别喜欢吃糖醋排骨/糖醋鱼/糖醋里脊/糖醋猪蹄…（各种业务应用），你老妈（IT部门）觉得每天都按照比例调制糖醋汁很麻烦很浪费时间还每次都有偏差（每次数据有误差），于是你老妈决定按照“1料酒；2酱油；3白糖；4醋；5水”的比例（数据算法）调制好一大桶糖醋汁（数据产品），以后每天倒一点糖醋汁就可以很快做出一盘糖醋XX（业务应用）。
这个调制糖醋汁的过程就相当于构建了 一个数据中台，糖醋汁就是数据产品。数据产品往往不是直接提供给用户使用的，而是提供给业务应用使用的（类似于糖醋汁不是用来直接喝的，而是用来做糖醋XX的）。另外，为了调制更快更准确，可能还需要买一些密封大桶/漏斗/量杯（ETL/BI 等数据工具）。
当然，如果你家十天半个月才做一次糖醋XX(低频)，那就没有必要调制一大桶糖醋汁方哪儿（不需要构建这个数据产品）。类似这个逻辑，如果你家每天都做八宝粥，则可以把八种粮食（数据）混合好放一个大桶里做成八宝粥混料（数据产品）。
如果你老妈的糖醋XX做的特别好开了个餐馆，每天做给几百个人吃（需求量变大），就需要调制更多糖醋汁买个冰箱存起来（数据仓库），这也解决了随用随跳（实时取数）的效率瓶颈。所以，在做数据中台之前，先自问一下：
 有没有糖醋汁、八宝粥混料的需求？（有没有数据产品的需求？）
2. 有多少人吃？（使用这个数据产品的需求量大不大？）
3. 多久吃一次？（需要这个数据产品的频率高不高？）
如果以上都合理，就可以开始规划数据中台了。再回过头看，公司为什么要建中台？  在过去几年中，借着移动互联网的红利，许多公司都高速发展，进行大规模业务拓展，业务拓展的速度足够快，对公司自然是好事，但是随着而来的问题就是，公司内部出现了大量的重复建设和资源浪费的现象，重复造轮子。
如滴滴，有顺风车、快车、专车、代驾等多业务的垂直架构，这些业务虽然会有一些差别，但是核心系统和流程都是类似的。如果各自独立开发，也会出现各种各样的问题。开发成本过高，滴滴旗下的每个业务，其实都是可以单独支撑起一家公司的，如果每个业务都独立做到极致，那么开发成本和人力成本就会非常巨大，而如果为了控制成本，就把系统的建设放缓，则意味着，无论是核心系统本身的质量，还是对外的用户体验都不太好。在这样的背景下，滴滴也开始考虑将诸多业务，以及各个城市的系统统一规划，统一建设，提升服务前台的能力。

滴滴中台 2015年末滴滴启动了中台战略整合业务系统。决定构建业务中台主要出于四方面考虑：专业深度、人力资源、用户体验、全局打通。
· 专业深度。由于是多业务垂直化的架构，会有多个团队开发同样的架构，这就需要很多的工程师。每个团队都是用最快速的方式构建流程，所以技术很难做深。这样一来，导致客户端的流畅度不高，后端不稳定，影响可扩展性。
· 人力资源。原则上来说把每个团队加到足够的人，每个架构都能有很好的发展。但工程师的薪资都非常高，招聘大量工程师来做同样的架构，研发成本高昂。很还有些时候，愿意花钱，却招聘不到合适的人。
· 用户体验。流畅度、稳定性、扩展性、界面、交易流程等都是影响用户体验的重要因素。在当时的组织结构和研发情况下，会出现业务的颜色各异，交易流程却相同的问题，很影响用户的体验。
· 全局打通。所有业务本质都是出行，出行本质有协同效应。但在各自独立发展情况下，协同性就完全没有，在构建中台过程中，可以逐步把协同性加起来。

总结 看到这里，你估计对中台有个大概了解。中台不是凭空产生的，而是建立在业务之上，公司发展过程中一些项目有点不同，然后重新搭建架构，有点资源浪费，搭建中台系统完美解决重复造轮子问题。当然大公司才有这困惑，小公司就不用加戏了，不用盲目跟风，做好手上的事情，时间到了，风口就来了！
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E4%B8%AD%E5%8F%B0%E6%A1%88%E4%BE%8B%E7%90%86%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/">自然语言处理中的预训练技术发展史
</a>
</h2>

  <time datetime="2019-07-08">8 Jul, 2019</time>

</div>

  <p class="container content">
  
  
    [参考整理]
本文来自新浪微博资深算法工程师张俊林老师的讲解，是非常好的学习材料。
Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。
本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。

图像领域的预训练 自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。
那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。
这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。
那么新的问题来了，为什么这种预训练的思路是可行的？
目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。
一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。
听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”
嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。
没听过？那下面就把这段陈年老账讲给你听听。

Word Embedding考古史 这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。
什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。
假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？
你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。
上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词 Wt = &ldquo;Bert&rdquo; 前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：
前面任意单词 Wi 用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 C(Wi) ，每个单词的C(Wi)拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C(Wi) 是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。
2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。
Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。
为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。
使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。
我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。
假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。
上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/">Read more &rarr;</a>

</div>


</article>

  
  
<div class="container pagination">
  


<a aria-label="First" href="https://7125messi.github.io/">
  <span aria-hidden="true">««</span>
</a>

<a class="disabled" aria-label="Previous" href="#">
  <span aria-hidden="true">«</span>
</a>


<a class="active" href="https://7125messi.github.io/">
  1
</a>

<a href="https://7125messi.github.io/page/2/">
  2
</a>

<a href="https://7125messi.github.io/page/3/">
  3
</a>


<a aria-label="Next" href="https://7125messi.github.io/page/2/">
  <span aria-hidden="true">»</span>
</a>

<a aria-label="Last" href="https://7125messi.github.io/page/3/">
  <span aria-hidden="true">»»</span>
</a>


</div>


</section>

      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

