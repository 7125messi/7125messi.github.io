<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>7125messi的博客 </title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="7125messi的博客 ">
<meta property="og:site_name" content="7125messi的博客"/>
<meta property="og:url" content="https://7125messi.github.io/" />
<meta property="og:locale" content="zh-cn">


<meta property="og:type" content="website" />



<link href="https://7125messi.github.io/index.xml" rel="alternate" type="application/rss+xml" title="7125messi的博客" />

<link rel="canonical" href="https://7125messi.github.io/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://7125messi.github.io/touch-icon-144-precomposed.png">
<link href="https://7125messi.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.55.6" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://7125messi.github.io">
  箴言

</a>

</div>

  
<div class="container topline">
  
  带着爱和梦想去生活


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://7125messi.github.io">正文</a>


  
<a href="https://7125messi.github.io/about">相关</a>

<a href="https://7125messi.github.io/post" title="Show list of posts">目录</a>


</nav>

<div class="container nav secondary no-print">
  


<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/7125messi">
  <span class="fa fa-github-square"></span></a>




 


















</div>


  

</header>


<section id="main-content" class="container main_content homepage">
  <header class="container header">
    <h1>7125messi的博客
</h1>

    <span>last update: <time datetime="2019-06-23T16:49:10&#43;08:00">23 June at 4:49pm</time>
</span>

  </header>
  
  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/">Transformer技术详解
</a>
</h2>

  <time datetime="2019-06-23">23 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    10-Transformer详解 参考文章：http://jalammar.github.io/illustrated-transformer/
谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。
Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。
在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。
Attention is All You Need：https://arxiv.org/abs/1706.03762
谷歌团队近期提出的用于生成词向量的BERT算法在NLP的11项任务中取得了效果的大幅提升，堪称2018年深度学习领域最振奋人心的消息。而BERT算法的最重要的部分便是本文中提出的Transformer的概念。
正如论文的题目所说的，Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。
作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：
 间片 的计算依赖 时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。  Transformer的提出解决了上面两个问题，首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。

1. Transformer 详解 
1.1 宏观视角认识Transformer整体框架 论文中的验证Transformer的实验室基于机器翻译的，下面我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为如图1：可以看到它是由编码组件、解码组件和它们之间的连接组成。
图1：Transformer用于机器翻译
图2：Transformer的Encoder-Decoder结构
编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
图3：Transformer的Encoder和Decoder均由6个block堆叠而成
所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。
图4：Transformer由self-attention和Feed Forward neural network组成
Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/transformer%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">Paddle_hub和pytorch_hub预训练模型迁移学习
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    paddlehub预训练模型迁移学习
https://colab.research.google.com/drive/1PIRRwzCcEVVQ4jAOTm7yJnlduuxM-pLc  pytorch_hub预训练模型迁移学习
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/paddle_hub%E5%92%8Cpytorch_hub%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/">特征工程方法论
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    欢迎浏览我的Azure Notebooks
https://notebooks.azure.com/messi7125/projects/feature-engineering
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA/">Read more &rarr;</a>

</div>


</article>

  
    <article class="container content summary">
  <div class="container hat">
  <h2><a href="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/">Nlp中的词向量
</a>
</h2>

  <time datetime="2019-06-22">22 Jun, 2019</time>

</div>

  <p class="container content">
  
  
    词向量对比：word2vec/glove/fastText/elmo/GPT/bert（非常好）
本文以QA形式对自然语言处理中的词向量进行总结：包含word2vec/glove/fastText/elmo/bert。
目录 一、文本表示和各词向量间的对比
 1、文本表示哪些方法？
 2、怎么从语言模型理解词向量？怎么理解分布式假设？
 3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？
 4、word2vec和NNLM对比有什么区别？（word2vec vs NNLM）
 5、word2vec和fastText对比有什么区别？（word2vec vs fastText）
 6、glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）
 7、elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）
  二、深入解剖word2vec
 1、word2vec的两种模型分别是什么？
 2、word2vec的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？
  三、深入解剖Glove详解
 1、GloVe构建过程是怎样的？
 2、GloVe的训练过程是怎样的？
 3、Glove损失函数是如何确定的？
  四、深入解剖bert（与elmo和GPT比较）
 1、为什么bert采取的是双向Transformer Encoder，而不叫decoder？
 2、elmo、GPT和bert在单双向语言模型处理上的不同之处？
 3、bert构建双向语言模型不是很简单吗？不也可以直接像elmo拼接Transformer decoder吗？
 4、为什么要采取Marked LM，而不直接应用Transformer Encoder？
 5、bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？
  一、文本表示和各词向量间的对比 
1、文本表示哪些方法？ 下面对文本表示进行一个归纳，也就是对于一篇文本可以如何用数学语言表示呢？
  


</p>


  <div class="container readlink">
  <a href="https://7125messi.github.io/post/nlp%E4%B8%AD%E5%90%84%E7%A7%8D%E8%AF%8D%E5%90%91%E9%87%8F%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/">Read more &rarr;</a>

</div>


</article>

  
  

</section>

      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 7125messi.


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

